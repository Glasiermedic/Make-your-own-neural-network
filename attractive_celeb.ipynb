{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attractive_celeb.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Glasiermedic/Make-your-own-neural-network/blob/master/attractive_celeb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wBqGJnFObzX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "c005d710-7604-4e1a-cda6-96b32ce871b8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "!pip3 install seaborn==0.9.0\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import ensemble\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_style('white')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seaborn==0.9.0 in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0) (3.0.3)\n",
            "Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0) (0.24.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.9.3 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0) (1.16.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0) (2.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn==0.9.0) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=1.4.3->seaborn==0.9.0) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn==0.9.0) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "migx0sciQiLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "celebs = pd.read_csv(\"https://raw.githubusercontent.com/Glasiermedic/Make-your-own-neural-network/master/list_attr_celeba.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx4k2vm53FpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "celeb = celebs.sample(frac=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB6NHnyHT4Kq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 822
        },
        "outputId": "5df9a390-4f9a-4a8c-a753-c74924c73592"
      },
      "source": [
        "celeb.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 40520 entries, 105778 to 49066\n",
            "Data columns (total 41 columns):\n",
            "image_id               40520 non-null object\n",
            "5_o_Clock_Shadow       40520 non-null int64\n",
            "Arched_Eyebrows        40520 non-null int64\n",
            "Attractive             40520 non-null int64\n",
            "Bags_Under_Eyes        40520 non-null int64\n",
            "Bald                   40520 non-null int64\n",
            "Bangs                  40520 non-null int64\n",
            "Big_Lips               40520 non-null int64\n",
            "Big_Nose               40520 non-null int64\n",
            "Black_Hair             40520 non-null int64\n",
            "Blond_Hair             40520 non-null int64\n",
            "Blurry                 40520 non-null int64\n",
            "Brown_Hair             40520 non-null int64\n",
            "Bushy_Eyebrows         40520 non-null int64\n",
            "Chubby                 40520 non-null int64\n",
            "Double_Chin            40520 non-null int64\n",
            "Eyeglasses             40520 non-null int64\n",
            "Goatee                 40520 non-null int64\n",
            "Gray_Hair              40520 non-null int64\n",
            "Heavy_Makeup           40520 non-null int64\n",
            "High_Cheekbones        40520 non-null int64\n",
            "Male                   40520 non-null int64\n",
            "Mouth_Slightly_Open    40520 non-null int64\n",
            "Mustache               40520 non-null int64\n",
            "Narrow_Eyes            40520 non-null int64\n",
            "No_Beard               40520 non-null int64\n",
            "Oval_Face              40520 non-null int64\n",
            "Pale_Skin              40520 non-null int64\n",
            "Pointy_Nose            40520 non-null int64\n",
            "Receding_Hairline      40520 non-null int64\n",
            "Rosy_Cheeks            40520 non-null int64\n",
            "Sideburns              40520 non-null int64\n",
            "Smiling                40520 non-null int64\n",
            "Straight_Hair          40520 non-null int64\n",
            "Wavy_Hair              40520 non-null int64\n",
            "Wearing_Earrings       40520 non-null int64\n",
            "Wearing_Hat            40520 non-null int64\n",
            "Wearing_Lipstick       40520 non-null int64\n",
            "Wearing_Necklace       40520 non-null int64\n",
            "Wearing_Necktie        40520 non-null int64\n",
            "Young                  40520 non-null int64\n",
            "dtypes: int64(40), object(1)\n",
            "memory usage: 13.0+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBd0mv97-21x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "b73a9e57-b36c-45d5-9317-b5296e564a0d"
      },
      "source": [
        "celeb.columns\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['image_id', '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive',\n",
              "       'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose',\n",
              "       'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows',\n",
              "       'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair',\n",
              "       'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open',\n",
              "       'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin',\n",
              "       'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns',\n",
              "       'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings',\n",
              "       'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace',\n",
              "       'Wearing_Necktie', 'Young'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUpWZ6JMqVBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "celeb = celeb.drop(['image_id'], axis = 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFXnDPnrpli6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfolds = StratifiedShuffleSplit(n_splits =10, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRECcWq83vXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = MinMaxScaler(copy=True, feature_range=(-1, 1))\n",
        "celeb_scal = scaler.fit_transform(celeb)\n",
        "celeb_1 = pd.DataFrame(celeb_scal)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvnvUleomCLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "celebs = celebs.drop(['image_id'], axis = 1)\n",
        "celebs_scal = scaler.fit_transform(celebs)\n",
        "celebs1 =pd.DataFrame(celebs_scal)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYQp4DL1CsR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "celeb_1.columns = celeb.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlACp0r7Cvt4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "aba4ef1a-782f-405d-c7dd-6c116d2de01b"
      },
      "source": [
        "celeb_1.describe()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>5_o_Clock_Shadow</th>\n",
              "      <th>Arched_Eyebrows</th>\n",
              "      <th>Attractive</th>\n",
              "      <th>Bags_Under_Eyes</th>\n",
              "      <th>Bald</th>\n",
              "      <th>Bangs</th>\n",
              "      <th>Big_Lips</th>\n",
              "      <th>Big_Nose</th>\n",
              "      <th>Black_Hair</th>\n",
              "      <th>Blond_Hair</th>\n",
              "      <th>Blurry</th>\n",
              "      <th>Brown_Hair</th>\n",
              "      <th>Bushy_Eyebrows</th>\n",
              "      <th>Chubby</th>\n",
              "      <th>Double_Chin</th>\n",
              "      <th>Eyeglasses</th>\n",
              "      <th>Goatee</th>\n",
              "      <th>Gray_Hair</th>\n",
              "      <th>Heavy_Makeup</th>\n",
              "      <th>High_Cheekbones</th>\n",
              "      <th>Male</th>\n",
              "      <th>Mouth_Slightly_Open</th>\n",
              "      <th>Mustache</th>\n",
              "      <th>Narrow_Eyes</th>\n",
              "      <th>No_Beard</th>\n",
              "      <th>Oval_Face</th>\n",
              "      <th>Pale_Skin</th>\n",
              "      <th>Pointy_Nose</th>\n",
              "      <th>Receding_Hairline</th>\n",
              "      <th>Rosy_Cheeks</th>\n",
              "      <th>Sideburns</th>\n",
              "      <th>Smiling</th>\n",
              "      <th>Straight_Hair</th>\n",
              "      <th>Wavy_Hair</th>\n",
              "      <th>Wearing_Earrings</th>\n",
              "      <th>Wearing_Hat</th>\n",
              "      <th>Wearing_Lipstick</th>\n",
              "      <th>Wearing_Necklace</th>\n",
              "      <th>Wearing_Necktie</th>\n",
              "      <th>Young</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-0.776160</td>\n",
              "      <td>-0.466140</td>\n",
              "      <td>0.025420</td>\n",
              "      <td>-0.589191</td>\n",
              "      <td>-0.953011</td>\n",
              "      <td>-0.695706</td>\n",
              "      <td>-0.515005</td>\n",
              "      <td>-0.531096</td>\n",
              "      <td>-0.521964</td>\n",
              "      <td>-0.704492</td>\n",
              "      <td>-0.897285</td>\n",
              "      <td>-0.597137</td>\n",
              "      <td>-0.718559</td>\n",
              "      <td>-0.886377</td>\n",
              "      <td>-0.905133</td>\n",
              "      <td>-0.867325</td>\n",
              "      <td>-0.875469</td>\n",
              "      <td>-0.917423</td>\n",
              "      <td>-0.230355</td>\n",
              "      <td>-0.088746</td>\n",
              "      <td>-0.164561</td>\n",
              "      <td>-0.033613</td>\n",
              "      <td>-0.920434</td>\n",
              "      <td>-0.766782</td>\n",
              "      <td>0.669497</td>\n",
              "      <td>-0.436772</td>\n",
              "      <td>-0.917029</td>\n",
              "      <td>-0.446347</td>\n",
              "      <td>-0.838203</td>\n",
              "      <td>-0.871471</td>\n",
              "      <td>-0.887315</td>\n",
              "      <td>-0.035587</td>\n",
              "      <td>-0.584205</td>\n",
              "      <td>-0.362734</td>\n",
              "      <td>-0.628134</td>\n",
              "      <td>-0.903751</td>\n",
              "      <td>-0.059674</td>\n",
              "      <td>-0.751678</td>\n",
              "      <td>-0.856466</td>\n",
              "      <td>0.549408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.630544</td>\n",
              "      <td>0.884722</td>\n",
              "      <td>0.999689</td>\n",
              "      <td>0.808004</td>\n",
              "      <td>0.302940</td>\n",
              "      <td>0.718336</td>\n",
              "      <td>0.857198</td>\n",
              "      <td>0.847322</td>\n",
              "      <td>0.852978</td>\n",
              "      <td>0.709721</td>\n",
              "      <td>0.441457</td>\n",
              "      <td>0.802149</td>\n",
              "      <td>0.695475</td>\n",
              "      <td>0.462970</td>\n",
              "      <td>0.425133</td>\n",
              "      <td>0.497749</td>\n",
              "      <td>0.483280</td>\n",
              "      <td>0.397917</td>\n",
              "      <td>0.973119</td>\n",
              "      <td>0.996067</td>\n",
              "      <td>0.986379</td>\n",
              "      <td>0.999447</td>\n",
              "      <td>0.390902</td>\n",
              "      <td>0.641916</td>\n",
              "      <td>0.742824</td>\n",
              "      <td>0.899583</td>\n",
              "      <td>0.398826</td>\n",
              "      <td>0.894871</td>\n",
              "      <td>0.545365</td>\n",
              "      <td>0.490453</td>\n",
              "      <td>0.461170</td>\n",
              "      <td>0.999379</td>\n",
              "      <td>0.811616</td>\n",
              "      <td>0.931904</td>\n",
              "      <td>0.778115</td>\n",
              "      <td>0.428063</td>\n",
              "      <td>0.998230</td>\n",
              "      <td>0.659538</td>\n",
              "      <td>0.516210</td>\n",
              "      <td>0.835565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       5_o_Clock_Shadow  Arched_Eyebrows  ...  Wearing_Necktie         Young\n",
              "count      40520.000000     40520.000000  ...     40520.000000  40520.000000\n",
              "mean          -0.776160        -0.466140  ...        -0.856466      0.549408\n",
              "std            0.630544         0.884722  ...         0.516210      0.835565\n",
              "min           -1.000000        -1.000000  ...        -1.000000     -1.000000\n",
              "25%           -1.000000        -1.000000  ...        -1.000000      1.000000\n",
              "50%           -1.000000        -1.000000  ...        -1.000000      1.000000\n",
              "75%           -1.000000         1.000000  ...        -1.000000      1.000000\n",
              "max            1.000000         1.000000  ...         1.000000      1.000000\n",
              "\n",
              "[8 rows x 40 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOGkJQAw1Jf8",
        "colab_type": "text"
      },
      "source": [
        "for col in celeb_1:\n",
        "  sns_plot = sns.distplot(celeb_1[col], label = col)\n",
        "  #plt.title(col)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSSOSxWp4Mkv",
        "colab_type": "text"
      },
      "source": [
        "%%time\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "for item in celeb_1.columns:\n",
        "  plt.figure(figsize=(15,5))\n",
        "  sns.scatterplot(x =celeb_1[item], y = celeb_1['Attractive'])\n",
        "  plt.title(item)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB85HpQENaGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = celeb_1.drop(['Attractive'], axis = 1)\n",
        "Y = celeb_1['Attractive']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_3uesupEoMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMUvr3qvOqFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K7g4_lZMtvz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "outputId": "aeac7ca3-1431-41ee-f828-9731115e436a"
      },
      "source": [
        "forest = RandomForestClassifier( n_jobs=-1, class_weight='balanced', n_estimators = 500, max_features = 7, max_depth=12, random_state = 1)\n",
        "\n",
        "print(cross_val_score(forest,X_train, y_train, cv=kfolds, scoring = 'recall', verbose = 2))\n",
        "pred_y_sklearn =cross_val_predict(forest, X_test, y_test, cv=10)\n",
        "y_true = y_test\n",
        "y_pred = cross_val_predict(forest, X_test, y_test, cv=10)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(y_pred, y_true))\n",
        "print(pd.crosstab(pred_y_sklearn, y_test))\n",
        "print(\"\")\n",
        "print(classification_report(y_pred, y_true))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=  10.9s\n",
            "[CV]  ................................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.9s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ................................................. , total=   9.2s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   9.2s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   9.3s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   8.9s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   9.2s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   9.0s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   9.2s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   8.9s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   9.0s\n",
            "[0.77831325 0.78975904 0.78493976 0.79277108 0.79216867 0.78072289\n",
            " 0.77710843 0.77831325 0.78554217 0.78795181]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  1.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "[[3092  979]\n",
            " [ 838 3195]]\n",
            "Attractive  -1.0   1.0\n",
            "row_0                 \n",
            "-1.0        3092   979\n",
            " 1.0         838  3195\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        -1.0       0.79      0.76      0.77      4071\n",
            "         1.0       0.77      0.79      0.78      4033\n",
            "\n",
            "    accuracy                           0.78      8104\n",
            "   macro avg       0.78      0.78      0.78      8104\n",
            "weighted avg       0.78      0.78      0.78      8104\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1rFd3JfRkMH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "567d4a33-5366-455a-b811-0a010b50345e"
      },
      "source": [
        "forest.fit(X_train, y_train)\n",
        "importances_for = forest.feature_importances_\n",
        "\n",
        "indices = np.argsort(importances_for)[::-1]\n",
        "for_imp_feat = []\n",
        "\n",
        "for f in range(X.shape[1]):\n",
        "    for_imp_feat.append(X.columns[indices[f]])\n",
        "\n",
        "importances_df = pd.Series(importances_for, index=X.columns)\n",
        "importances_df.nlargest(10).sort_values().plot(kind='barh', figsize=(12, 6)) \n",
        "plt.title(\"Random Forest Importances\")\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAx0AAAF4CAYAAAAxP5RTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7ertb2UUFDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = celeb_1.drop(['Attractive'], axis = 1)\n",
        "X_2 = X[list(for_imp_feat)[:20]]\n",
        "Y = celeb_1['Attractive']\n",
        "\n",
        "X3 =celebs.drop(['Attractive'],axis =1)\n",
        "Y3 =celebs['Attractive']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5K9MuLxU97U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X_2, Y, test_size=0.2, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YYEzUn9pzNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, Y3, test_size=0.2, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4sqpgAaXfcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcaUWrS0T41p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = MLPClassifier(alpha=0.01, hidden_layer_sizes=(5,100,5), activation='relu', solver='sgd', \n",
        "                    learning_rate='adaptive', warm_start=True,  verbose = 1,\n",
        "                    max_iter = 50, random_state=1, tol=0.0001)\n",
        "#mlp.fit(X_train, y_train)\n",
        "#mlp.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkaIDqqHYfkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cross_val_score(mlp, X, Y, cv=kfolds, verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MfioMDUaYmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha_list = [ .01, .05]\n",
        "layer_lists = [(5,100,5),(100,)]\n",
        "parameters = {'alpha':alpha_list, 'hidden_layer_sizes': layer_lists,\n",
        "                    'solver': [ 'sgd', 'adam']}\n",
        " #if marked out once it has ran and the best estimator is identified.  The best estimator is then put into the mlp algorithm below for the final model\n",
        "#mlp_grid = GridSearchCV(mlp, parameters, cv=kfolds, scoring='neg_mean_squared_error')\n",
        "#mlp_grid.fit(X3_train,y3_train)        \n",
        "#print (mlp_grid.best_estimator_)\n",
        "#print(np.sqrt(-mlp_grid.best_score_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJQgVm_rQxYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = MLPClassifier(alpha=0.01, hidden_layer_sizes=(5,100,5), activation='relu', solver='sgd', \n",
        "                    learning_rate='adaptive', warm_start=True,  verbose = 1,\n",
        "                    max_iter = 200, random_state=1, tol=0.0001)\n",
        "#mlp.fit(X_train, y_train)\n",
        "#mlp.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9DEd05BOiiO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "46f94495-b83a-4c74-ee9e-7ed178cc8d61"
      },
      "source": [
        "print(cross_val_score(mlp,X2_train, y2_train, cv=kfolds, scoring = 'neg_mean_squared_error', verbose = 1))\n",
        "pred2_y_sklearn =cross_val_predict(mlp, X2_test, y2_test, cv=2)\n",
        "y2_true = y_test\n",
        "y2_pred = cross_val_predict(mlp, X2_test, y2_test, cv=10)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(y2_pred, y2_true))\n",
        "print(pd.crosstab(pred2_y_sklearn, y2_test))\n",
        "print(\"\")\n",
        "print(classification_report(y2_pred, y2_true))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.65424445\n",
            "Iteration 2, loss = 0.60965575\n",
            "Iteration 3, loss = 0.57767594\n",
            "Iteration 4, loss = 0.55590159\n",
            "Iteration 5, loss = 0.53986725\n",
            "Iteration 6, loss = 0.52735279\n",
            "Iteration 7, loss = 0.51755023\n",
            "Iteration 8, loss = 0.50968952\n",
            "Iteration 9, loss = 0.50330606\n",
            "Iteration 10, loss = 0.49806658\n",
            "Iteration 11, loss = 0.49373783\n",
            "Iteration 12, loss = 0.49004081\n",
            "Iteration 13, loss = 0.48722446\n",
            "Iteration 14, loss = 0.48460327\n",
            "Iteration 15, loss = 0.48243799\n",
            "Iteration 16, loss = 0.48054429\n",
            "Iteration 17, loss = 0.47891378\n",
            "Iteration 18, loss = 0.47751283\n",
            "Iteration 19, loss = 0.47617962\n",
            "Iteration 20, loss = 0.47501378\n",
            "Iteration 21, loss = 0.47392054\n",
            "Iteration 22, loss = 0.47285488\n",
            "Iteration 23, loss = 0.47182140\n",
            "Iteration 24, loss = 0.47099454\n",
            "Iteration 25, loss = 0.47003736\n",
            "Iteration 26, loss = 0.46920794\n",
            "Iteration 27, loss = 0.46836984\n",
            "Iteration 28, loss = 0.46768595\n",
            "Iteration 29, loss = 0.46693923\n",
            "Iteration 30, loss = 0.46635716\n",
            "Iteration 31, loss = 0.46568615\n",
            "Iteration 32, loss = 0.46517257\n",
            "Iteration 33, loss = 0.46458807\n",
            "Iteration 34, loss = 0.46407944\n",
            "Iteration 35, loss = 0.46350994\n",
            "Iteration 36, loss = 0.46281849\n",
            "Iteration 37, loss = 0.46237167\n",
            "Iteration 38, loss = 0.46181356\n",
            "Iteration 39, loss = 0.46143577\n",
            "Iteration 40, loss = 0.46084126\n",
            "Iteration 41, loss = 0.46046349\n",
            "Iteration 42, loss = 0.46007117\n",
            "Iteration 43, loss = 0.45966618\n",
            "Iteration 44, loss = 0.45921107\n",
            "Iteration 45, loss = 0.45886114\n",
            "Iteration 46, loss = 0.45846535\n",
            "Iteration 47, loss = 0.45819808\n",
            "Iteration 48, loss = 0.45783192\n",
            "Iteration 49, loss = 0.45748794\n",
            "Iteration 50, loss = 0.45719477\n",
            "Iteration 51, loss = 0.45698137\n",
            "Iteration 52, loss = 0.45657057\n",
            "Iteration 53, loss = 0.45633366\n",
            "Iteration 54, loss = 0.45601126\n",
            "Iteration 55, loss = 0.45576095\n",
            "Iteration 56, loss = 0.45546691\n",
            "Iteration 57, loss = 0.45516520\n",
            "Iteration 58, loss = 0.45493118\n",
            "Iteration 59, loss = 0.45468872\n",
            "Iteration 60, loss = 0.45439198\n",
            "Iteration 61, loss = 0.45419459\n",
            "Iteration 62, loss = 0.45392511\n",
            "Iteration 63, loss = 0.45374547\n",
            "Iteration 64, loss = 0.45355030\n",
            "Iteration 65, loss = 0.45324682\n",
            "Iteration 66, loss = 0.45313912\n",
            "Iteration 67, loss = 0.45291246\n",
            "Iteration 68, loss = 0.45270971\n",
            "Iteration 69, loss = 0.45253284\n",
            "Iteration 70, loss = 0.45234373\n",
            "Iteration 71, loss = 0.45219755\n",
            "Iteration 72, loss = 0.45205983\n",
            "Iteration 73, loss = 0.45188918\n",
            "Iteration 74, loss = 0.45165308\n",
            "Iteration 75, loss = 0.45155807\n",
            "Iteration 76, loss = 0.45131278\n",
            "Iteration 77, loss = 0.45116323\n",
            "Iteration 78, loss = 0.45109565\n",
            "Iteration 79, loss = 0.45087508\n",
            "Iteration 80, loss = 0.45074696\n",
            "Iteration 81, loss = 0.45056265\n",
            "Iteration 82, loss = 0.45048551\n",
            "Iteration 83, loss = 0.45041508\n",
            "Iteration 84, loss = 0.45023200\n",
            "Iteration 85, loss = 0.45015730\n",
            "Iteration 86, loss = 0.45008570\n",
            "Iteration 87, loss = 0.44987520\n",
            "Iteration 88, loss = 0.44973961\n",
            "Iteration 89, loss = 0.44961936\n",
            "Iteration 90, loss = 0.44951888\n",
            "Iteration 91, loss = 0.44939873\n",
            "Iteration 92, loss = 0.44940176\n",
            "Iteration 93, loss = 0.44923164\n",
            "Iteration 94, loss = 0.44912509\n",
            "Iteration 95, loss = 0.44907560\n",
            "Iteration 96, loss = 0.44895272\n",
            "Iteration 97, loss = 0.44896055\n",
            "Iteration 98, loss = 0.44868362\n",
            "Iteration 99, loss = 0.44867115\n",
            "Iteration 100, loss = 0.44860610\n",
            "Iteration 101, loss = 0.44854619\n",
            "Iteration 102, loss = 0.44844876\n",
            "Iteration 103, loss = 0.44833636\n",
            "Iteration 104, loss = 0.44840266\n",
            "Iteration 105, loss = 0.44820774\n",
            "Iteration 106, loss = 0.44813481\n",
            "Iteration 107, loss = 0.44806117\n",
            "Iteration 108, loss = 0.44796391\n",
            "Iteration 109, loss = 0.44779532\n",
            "Iteration 110, loss = 0.44783337\n",
            "Iteration 111, loss = 0.44780354\n",
            "Iteration 112, loss = 0.44772545\n",
            "Iteration 113, loss = 0.44756370\n",
            "Iteration 114, loss = 0.44757976\n",
            "Iteration 115, loss = 0.44749973\n",
            "Iteration 116, loss = 0.44748389\n",
            "Iteration 117, loss = 0.44741957\n",
            "Iteration 118, loss = 0.44731830\n",
            "Iteration 119, loss = 0.44729337\n",
            "Iteration 120, loss = 0.44728583\n",
            "Iteration 121, loss = 0.44726077\n",
            "Iteration 122, loss = 0.44711563\n",
            "Iteration 123, loss = 0.44705133\n",
            "Iteration 124, loss = 0.44694179\n",
            "Iteration 125, loss = 0.44688236\n",
            "Iteration 126, loss = 0.44686055\n",
            "Iteration 127, loss = 0.44679748\n",
            "Iteration 128, loss = 0.44669382\n",
            "Iteration 129, loss = 0.44665403\n",
            "Iteration 130, loss = 0.44666527\n",
            "Iteration 131, loss = 0.44659886\n",
            "Iteration 132, loss = 0.44651026\n",
            "Iteration 133, loss = 0.44643278\n",
            "Iteration 134, loss = 0.44638568\n",
            "Iteration 135, loss = 0.44637344\n",
            "Iteration 136, loss = 0.44633620\n",
            "Iteration 137, loss = 0.44627742\n",
            "Iteration 138, loss = 0.44618377\n",
            "Iteration 139, loss = 0.44615116\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 140, loss = 0.44594534\n",
            "Iteration 141, loss = 0.44591729\n",
            "Iteration 142, loss = 0.44592867\n",
            "Iteration 143, loss = 0.44591317\n",
            "Iteration 144, loss = 0.44590010\n",
            "Iteration 145, loss = 0.44588853\n",
            "Iteration 146, loss = 0.44586526\n",
            "Iteration 147, loss = 0.44586491\n",
            "Iteration 148, loss = 0.44586267\n",
            "Iteration 149, loss = 0.44585271\n",
            "Iteration 150, loss = 0.44583110\n",
            "Iteration 151, loss = 0.44584832\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 152, loss = 0.44578627\n",
            "Iteration 153, loss = 0.44578262\n",
            "Iteration 154, loss = 0.44577442\n",
            "Iteration 155, loss = 0.44577005\n",
            "Iteration 156, loss = 0.44576805\n",
            "Iteration 157, loss = 0.44576062\n",
            "Iteration 158, loss = 0.44576369\n",
            "Iteration 159, loss = 0.44575643\n",
            "Iteration 160, loss = 0.44575911\n",
            "Iteration 161, loss = 0.44575371\n",
            "Iteration 162, loss = 0.44575297\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 163, loss = 0.44574166\n",
            "Iteration 164, loss = 0.44574178\n",
            "Iteration 165, loss = 0.44574167\n",
            "Iteration 166, loss = 0.44574093\n",
            "Iteration 167, loss = 0.44574035\n",
            "Iteration 168, loss = 0.44573988\n",
            "Iteration 169, loss = 0.44573979\n",
            "Iteration 170, loss = 0.44573960\n",
            "Iteration 171, loss = 0.44573923\n",
            "Iteration 172, loss = 0.44573837\n",
            "Iteration 173, loss = 0.44573872\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 174, loss = 0.44573595\n",
            "Iteration 175, loss = 0.44573577\n",
            "Iteration 176, loss = 0.44573562\n",
            "Iteration 177, loss = 0.44573557\n",
            "Iteration 178, loss = 0.44573573\n",
            "Iteration 179, loss = 0.44573557\n",
            "Iteration 180, loss = 0.44573550\n",
            "Iteration 181, loss = 0.44573533\n",
            "Iteration 182, loss = 0.44573537\n",
            "Iteration 183, loss = 0.44573520\n",
            "Iteration 184, loss = 0.44573489\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 185, loss = 0.44573455\n",
            "Iteration 186, loss = 0.44573452\n",
            "Iteration 187, loss = 0.44573454\n",
            "Iteration 188, loss = 0.44573455\n",
            "Iteration 189, loss = 0.44573452\n",
            "Iteration 190, loss = 0.44573449\n",
            "Iteration 191, loss = 0.44573444\n",
            "Iteration 192, loss = 0.44573444\n",
            "Iteration 193, loss = 0.44573443\n",
            "Iteration 194, loss = 0.44573442\n",
            "Iteration 195, loss = 0.44573442\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.65370820\n",
            "Iteration 2, loss = 0.60896791\n",
            "Iteration 3, loss = 0.57710845\n",
            "Iteration 4, loss = 0.55530589\n",
            "Iteration 5, loss = 0.53909269\n",
            "Iteration 6, loss = 0.52667343\n",
            "Iteration 7, loss = 0.51678633\n",
            "Iteration 8, loss = 0.50897815\n",
            "Iteration 9, loss = 0.50266596\n",
            "Iteration 10, loss = 0.49747604\n",
            "Iteration 11, loss = 0.49324010\n",
            "Iteration 12, loss = 0.48975551\n",
            "Iteration 13, loss = 0.48683724\n",
            "Iteration 14, loss = 0.48445714\n",
            "Iteration 15, loss = 0.48232423\n",
            "Iteration 16, loss = 0.48045194\n",
            "Iteration 17, loss = 0.47893900\n",
            "Iteration 18, loss = 0.47753495\n",
            "Iteration 19, loss = 0.47625376\n",
            "Iteration 20, loss = 0.47510574\n",
            "Iteration 21, loss = 0.47402664\n",
            "Iteration 22, loss = 0.47303798\n",
            "Iteration 23, loss = 0.47208769\n",
            "Iteration 24, loss = 0.47122350\n",
            "Iteration 25, loss = 0.47043491\n",
            "Iteration 26, loss = 0.46961396\n",
            "Iteration 27, loss = 0.46892335\n",
            "Iteration 28, loss = 0.46822036\n",
            "Iteration 29, loss = 0.46752595\n",
            "Iteration 30, loss = 0.46689904\n",
            "Iteration 31, loss = 0.46626272\n",
            "Iteration 32, loss = 0.46574378\n",
            "Iteration 33, loss = 0.46515472\n",
            "Iteration 34, loss = 0.46461456\n",
            "Iteration 35, loss = 0.46415621\n",
            "Iteration 36, loss = 0.46366184\n",
            "Iteration 37, loss = 0.46321013\n",
            "Iteration 38, loss = 0.46266345\n",
            "Iteration 39, loss = 0.46232443\n",
            "Iteration 40, loss = 0.46192119\n",
            "Iteration 41, loss = 0.46148621\n",
            "Iteration 42, loss = 0.46106403\n",
            "Iteration 43, loss = 0.46064288\n",
            "Iteration 44, loss = 0.46029673\n",
            "Iteration 45, loss = 0.46002054\n",
            "Iteration 46, loss = 0.45971479\n",
            "Iteration 47, loss = 0.45931098\n",
            "Iteration 48, loss = 0.45896382\n",
            "Iteration 49, loss = 0.45869083\n",
            "Iteration 50, loss = 0.45832472\n",
            "Iteration 51, loss = 0.45807634\n",
            "Iteration 52, loss = 0.45772828\n",
            "Iteration 53, loss = 0.45743440\n",
            "Iteration 54, loss = 0.45718237\n",
            "Iteration 55, loss = 0.45686718\n",
            "Iteration 56, loss = 0.45667794\n",
            "Iteration 57, loss = 0.45641061\n",
            "Iteration 58, loss = 0.45620239\n",
            "Iteration 59, loss = 0.45596714\n",
            "Iteration 60, loss = 0.45573928\n",
            "Iteration 61, loss = 0.45548461\n",
            "Iteration 62, loss = 0.45529189\n",
            "Iteration 63, loss = 0.45510206\n",
            "Iteration 64, loss = 0.45479643\n",
            "Iteration 65, loss = 0.45453846\n",
            "Iteration 66, loss = 0.45431338\n",
            "Iteration 67, loss = 0.45411901\n",
            "Iteration 68, loss = 0.45387412\n",
            "Iteration 69, loss = 0.45374916\n",
            "Iteration 70, loss = 0.45352517\n",
            "Iteration 71, loss = 0.45335282\n",
            "Iteration 72, loss = 0.45305787\n",
            "Iteration 73, loss = 0.45301282\n",
            "Iteration 74, loss = 0.45284497\n",
            "Iteration 75, loss = 0.45265980\n",
            "Iteration 76, loss = 0.45248934\n",
            "Iteration 77, loss = 0.45234868\n",
            "Iteration 78, loss = 0.45211540\n",
            "Iteration 79, loss = 0.45208067\n",
            "Iteration 80, loss = 0.45193318\n",
            "Iteration 81, loss = 0.45182871\n",
            "Iteration 82, loss = 0.45161860\n",
            "Iteration 83, loss = 0.45147584\n",
            "Iteration 84, loss = 0.45139357\n",
            "Iteration 85, loss = 0.45127657\n",
            "Iteration 86, loss = 0.45113087\n",
            "Iteration 87, loss = 0.45108327\n",
            "Iteration 88, loss = 0.45092146\n",
            "Iteration 89, loss = 0.45074339\n",
            "Iteration 90, loss = 0.45066635\n",
            "Iteration 91, loss = 0.45052910\n",
            "Iteration 92, loss = 0.45049589\n",
            "Iteration 93, loss = 0.45035126\n",
            "Iteration 94, loss = 0.45033642\n",
            "Iteration 95, loss = 0.45025080\n",
            "Iteration 96, loss = 0.45012013\n",
            "Iteration 97, loss = 0.45007541\n",
            "Iteration 98, loss = 0.44991187\n",
            "Iteration 99, loss = 0.44981198\n",
            "Iteration 100, loss = 0.44978616\n",
            "Iteration 101, loss = 0.44967632\n",
            "Iteration 102, loss = 0.44962073\n",
            "Iteration 103, loss = 0.44952475\n",
            "Iteration 104, loss = 0.44946345\n",
            "Iteration 105, loss = 0.44942871\n",
            "Iteration 106, loss = 0.44926438\n",
            "Iteration 107, loss = 0.44924547\n",
            "Iteration 108, loss = 0.44921288\n",
            "Iteration 109, loss = 0.44915083\n",
            "Iteration 110, loss = 0.44901418\n",
            "Iteration 111, loss = 0.44893134\n",
            "Iteration 112, loss = 0.44881447\n",
            "Iteration 113, loss = 0.44886410\n",
            "Iteration 114, loss = 0.44869046\n",
            "Iteration 115, loss = 0.44869323\n",
            "Iteration 116, loss = 0.44863295\n",
            "Iteration 117, loss = 0.44858176\n",
            "Iteration 118, loss = 0.44847650\n",
            "Iteration 119, loss = 0.44846006\n",
            "Iteration 120, loss = 0.44841438\n",
            "Iteration 121, loss = 0.44831172\n",
            "Iteration 122, loss = 0.44843003\n",
            "Iteration 123, loss = 0.44814667\n",
            "Iteration 124, loss = 0.44815202\n",
            "Iteration 125, loss = 0.44810081\n",
            "Iteration 126, loss = 0.44798354\n",
            "Iteration 127, loss = 0.44798907\n",
            "Iteration 128, loss = 0.44784249\n",
            "Iteration 129, loss = 0.44786489\n",
            "Iteration 130, loss = 0.44772690\n",
            "Iteration 131, loss = 0.44777093\n",
            "Iteration 132, loss = 0.44763291\n",
            "Iteration 133, loss = 0.44767188\n",
            "Iteration 134, loss = 0.44755681\n",
            "Iteration 135, loss = 0.44756130\n",
            "Iteration 136, loss = 0.44737229\n",
            "Iteration 137, loss = 0.44748049\n",
            "Iteration 138, loss = 0.44736957\n",
            "Iteration 139, loss = 0.44729039\n",
            "Iteration 140, loss = 0.44736800\n",
            "Iteration 141, loss = 0.44727539\n",
            "Iteration 142, loss = 0.44726162\n",
            "Iteration 143, loss = 0.44723964\n",
            "Iteration 144, loss = 0.44716741\n",
            "Iteration 145, loss = 0.44708770\n",
            "Iteration 146, loss = 0.44708579\n",
            "Iteration 147, loss = 0.44704111\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 148, loss = 0.44677497\n",
            "Iteration 149, loss = 0.44673396\n",
            "Iteration 150, loss = 0.44672898\n",
            "Iteration 151, loss = 0.44672722\n",
            "Iteration 152, loss = 0.44673530\n",
            "Iteration 153, loss = 0.44668700\n",
            "Iteration 154, loss = 0.44670524\n",
            "Iteration 155, loss = 0.44668562\n",
            "Iteration 156, loss = 0.44668649\n",
            "Iteration 157, loss = 0.44667679\n",
            "Iteration 158, loss = 0.44666185\n",
            "Iteration 159, loss = 0.44664663\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 160, loss = 0.44662573\n",
            "Iteration 161, loss = 0.44660411\n",
            "Iteration 162, loss = 0.44659918\n",
            "Iteration 163, loss = 0.44659289\n",
            "Iteration 164, loss = 0.44659315\n",
            "Iteration 165, loss = 0.44659462\n",
            "Iteration 166, loss = 0.44658852\n",
            "Iteration 167, loss = 0.44658559\n",
            "Iteration 168, loss = 0.44658803\n",
            "Iteration 169, loss = 0.44658673\n",
            "Iteration 170, loss = 0.44658136\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 171, loss = 0.44656999\n",
            "Iteration 172, loss = 0.44657013\n",
            "Iteration 173, loss = 0.44656895\n",
            "Iteration 174, loss = 0.44656924\n",
            "Iteration 175, loss = 0.44656855\n",
            "Iteration 176, loss = 0.44656887\n",
            "Iteration 177, loss = 0.44656866\n",
            "Iteration 178, loss = 0.44656830\n",
            "Iteration 179, loss = 0.44656731\n",
            "Iteration 180, loss = 0.44656685\n",
            "Iteration 181, loss = 0.44656665\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 182, loss = 0.44656458\n",
            "Iteration 183, loss = 0.44656419\n",
            "Iteration 184, loss = 0.44656427\n",
            "Iteration 185, loss = 0.44656397\n",
            "Iteration 186, loss = 0.44656390\n",
            "Iteration 187, loss = 0.44656397\n",
            "Iteration 188, loss = 0.44656388\n",
            "Iteration 189, loss = 0.44656403\n",
            "Iteration 190, loss = 0.44656372\n",
            "Iteration 191, loss = 0.44656355\n",
            "Iteration 192, loss = 0.44656365\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 193, loss = 0.44656321\n",
            "Iteration 194, loss = 0.44656312\n",
            "Iteration 195, loss = 0.44656311\n",
            "Iteration 196, loss = 0.44656313\n",
            "Iteration 197, loss = 0.44656307\n",
            "Iteration 198, loss = 0.44656310\n",
            "Iteration 199, loss = 0.44656305\n",
            "Iteration 200, loss = 0.44656303\n",
            "Iteration 1, loss = 0.65348587\n",
            "Iteration 2, loss = 0.60839439\n",
            "Iteration 3, loss = 0.57657106\n",
            "Iteration 4, loss = 0.55488575\n",
            "Iteration 5, loss = 0.53892940\n",
            "Iteration 6, loss = 0.52636718\n",
            "Iteration 7, loss = 0.51648446\n",
            "Iteration 8, loss = 0.50860772\n",
            "Iteration 9, loss = 0.50227180\n",
            "Iteration 10, loss = 0.49693784\n",
            "Iteration 11, loss = 0.49269816\n",
            "Iteration 12, loss = 0.48911012\n",
            "Iteration 13, loss = 0.48623974\n",
            "Iteration 14, loss = 0.48374029\n",
            "Iteration 15, loss = 0.48164741\n",
            "Iteration 16, loss = 0.47981244\n",
            "Iteration 17, loss = 0.47816408\n",
            "Iteration 18, loss = 0.47678431\n",
            "Iteration 19, loss = 0.47553704\n",
            "Iteration 20, loss = 0.47439530\n",
            "Iteration 21, loss = 0.47333696\n",
            "Iteration 22, loss = 0.47236384\n",
            "Iteration 23, loss = 0.47150267\n",
            "Iteration 24, loss = 0.47066016\n",
            "Iteration 25, loss = 0.46980522\n",
            "Iteration 26, loss = 0.46912072\n",
            "Iteration 27, loss = 0.46834314\n",
            "Iteration 28, loss = 0.46767147\n",
            "Iteration 29, loss = 0.46702082\n",
            "Iteration 30, loss = 0.46642536\n",
            "Iteration 31, loss = 0.46578657\n",
            "Iteration 32, loss = 0.46519572\n",
            "Iteration 33, loss = 0.46474502\n",
            "Iteration 34, loss = 0.46422932\n",
            "Iteration 35, loss = 0.46365925\n",
            "Iteration 36, loss = 0.46321428\n",
            "Iteration 37, loss = 0.46272185\n",
            "Iteration 38, loss = 0.46221681\n",
            "Iteration 39, loss = 0.46171242\n",
            "Iteration 40, loss = 0.46122229\n",
            "Iteration 41, loss = 0.46075442\n",
            "Iteration 42, loss = 0.46025962\n",
            "Iteration 43, loss = 0.45994780\n",
            "Iteration 44, loss = 0.45946553\n",
            "Iteration 45, loss = 0.45917756\n",
            "Iteration 46, loss = 0.45875896\n",
            "Iteration 47, loss = 0.45841349\n",
            "Iteration 48, loss = 0.45800053\n",
            "Iteration 49, loss = 0.45776392\n",
            "Iteration 50, loss = 0.45735535\n",
            "Iteration 51, loss = 0.45710386\n",
            "Iteration 52, loss = 0.45677756\n",
            "Iteration 53, loss = 0.45653398\n",
            "Iteration 54, loss = 0.45620752\n",
            "Iteration 55, loss = 0.45595964\n",
            "Iteration 56, loss = 0.45567224\n",
            "Iteration 57, loss = 0.45542399\n",
            "Iteration 58, loss = 0.45514714\n",
            "Iteration 59, loss = 0.45492067\n",
            "Iteration 60, loss = 0.45467113\n",
            "Iteration 61, loss = 0.45435554\n",
            "Iteration 62, loss = 0.45411488\n",
            "Iteration 63, loss = 0.45384850\n",
            "Iteration 64, loss = 0.45363313\n",
            "Iteration 65, loss = 0.45335392\n",
            "Iteration 66, loss = 0.45323174\n",
            "Iteration 67, loss = 0.45289764\n",
            "Iteration 68, loss = 0.45279346\n",
            "Iteration 69, loss = 0.45255614\n",
            "Iteration 70, loss = 0.45239359\n",
            "Iteration 71, loss = 0.45226018\n",
            "Iteration 72, loss = 0.45198830\n",
            "Iteration 73, loss = 0.45186589\n",
            "Iteration 74, loss = 0.45156407\n",
            "Iteration 75, loss = 0.45155603\n",
            "Iteration 76, loss = 0.45133814\n",
            "Iteration 77, loss = 0.45111687\n",
            "Iteration 78, loss = 0.45101568\n",
            "Iteration 79, loss = 0.45086218\n",
            "Iteration 80, loss = 0.45075509\n",
            "Iteration 81, loss = 0.45058917\n",
            "Iteration 82, loss = 0.45051260\n",
            "Iteration 83, loss = 0.45031608\n",
            "Iteration 84, loss = 0.45015505\n",
            "Iteration 85, loss = 0.45007909\n",
            "Iteration 86, loss = 0.44992991\n",
            "Iteration 87, loss = 0.44977228\n",
            "Iteration 88, loss = 0.44957471\n",
            "Iteration 89, loss = 0.44953760\n",
            "Iteration 90, loss = 0.44942576\n",
            "Iteration 91, loss = 0.44931609\n",
            "Iteration 92, loss = 0.44925803\n",
            "Iteration 93, loss = 0.44909828\n",
            "Iteration 94, loss = 0.44907325\n",
            "Iteration 95, loss = 0.44889185\n",
            "Iteration 96, loss = 0.44885807\n",
            "Iteration 97, loss = 0.44875612\n",
            "Iteration 98, loss = 0.44868402\n",
            "Iteration 99, loss = 0.44849385\n",
            "Iteration 100, loss = 0.44845133\n",
            "Iteration 101, loss = 0.44836633\n",
            "Iteration 102, loss = 0.44822043\n",
            "Iteration 103, loss = 0.44815225\n",
            "Iteration 104, loss = 0.44813635\n",
            "Iteration 105, loss = 0.44794709\n",
            "Iteration 106, loss = 0.44801025\n",
            "Iteration 107, loss = 0.44788522\n",
            "Iteration 108, loss = 0.44785383\n",
            "Iteration 109, loss = 0.44773458\n",
            "Iteration 110, loss = 0.44771506\n",
            "Iteration 111, loss = 0.44762342\n",
            "Iteration 112, loss = 0.44757872\n",
            "Iteration 113, loss = 0.44753951\n",
            "Iteration 114, loss = 0.44738517\n",
            "Iteration 115, loss = 0.44733551\n",
            "Iteration 116, loss = 0.44724772\n",
            "Iteration 117, loss = 0.44716078\n",
            "Iteration 118, loss = 0.44710088\n",
            "Iteration 119, loss = 0.44707158\n",
            "Iteration 120, loss = 0.44696981\n",
            "Iteration 121, loss = 0.44701422\n",
            "Iteration 122, loss = 0.44685797\n",
            "Iteration 123, loss = 0.44683730\n",
            "Iteration 124, loss = 0.44670382\n",
            "Iteration 125, loss = 0.44666246\n",
            "Iteration 126, loss = 0.44658181\n",
            "Iteration 127, loss = 0.44652440\n",
            "Iteration 128, loss = 0.44646333\n",
            "Iteration 129, loss = 0.44632360\n",
            "Iteration 130, loss = 0.44637090\n",
            "Iteration 131, loss = 0.44629179\n",
            "Iteration 132, loss = 0.44634468\n",
            "Iteration 133, loss = 0.44624841\n",
            "Iteration 134, loss = 0.44621025\n",
            "Iteration 135, loss = 0.44616605\n",
            "Iteration 136, loss = 0.44606757\n",
            "Iteration 137, loss = 0.44600703\n",
            "Iteration 138, loss = 0.44595443\n",
            "Iteration 139, loss = 0.44591007\n",
            "Iteration 140, loss = 0.44592544\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 141, loss = 0.44565710\n",
            "Iteration 142, loss = 0.44564601\n",
            "Iteration 143, loss = 0.44562057\n",
            "Iteration 144, loss = 0.44561292\n",
            "Iteration 145, loss = 0.44561825\n",
            "Iteration 146, loss = 0.44561016\n",
            "Iteration 147, loss = 0.44558041\n",
            "Iteration 148, loss = 0.44556643\n",
            "Iteration 149, loss = 0.44559079\n",
            "Iteration 150, loss = 0.44558057\n",
            "Iteration 151, loss = 0.44554158\n",
            "Iteration 152, loss = 0.44554294\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 153, loss = 0.44550526\n",
            "Iteration 154, loss = 0.44549834\n",
            "Iteration 155, loss = 0.44549025\n",
            "Iteration 156, loss = 0.44549184\n",
            "Iteration 157, loss = 0.44548195\n",
            "Iteration 158, loss = 0.44548095\n",
            "Iteration 159, loss = 0.44548094\n",
            "Iteration 160, loss = 0.44548635\n",
            "Iteration 161, loss = 0.44547568\n",
            "Iteration 162, loss = 0.44547741\n",
            "Iteration 163, loss = 0.44547397\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 164, loss = 0.44546205\n",
            "Iteration 165, loss = 0.44546156\n",
            "Iteration 166, loss = 0.44546104\n",
            "Iteration 167, loss = 0.44546174\n",
            "Iteration 168, loss = 0.44545979\n",
            "Iteration 169, loss = 0.44545952\n",
            "Iteration 170, loss = 0.44545901\n",
            "Iteration 171, loss = 0.44545857\n",
            "Iteration 172, loss = 0.44545839\n",
            "Iteration 173, loss = 0.44545874\n",
            "Iteration 174, loss = 0.44545766\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 175, loss = 0.44545565\n",
            "Iteration 176, loss = 0.44545538\n",
            "Iteration 177, loss = 0.44545552\n",
            "Iteration 178, loss = 0.44545532\n",
            "Iteration 179, loss = 0.44545538\n",
            "Iteration 180, loss = 0.44545522\n",
            "Iteration 181, loss = 0.44545508\n",
            "Iteration 182, loss = 0.44545490\n",
            "Iteration 183, loss = 0.44545500\n",
            "Iteration 184, loss = 0.44545471\n",
            "Iteration 185, loss = 0.44545491\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 186, loss = 0.44545433\n",
            "Iteration 187, loss = 0.44545425\n",
            "Iteration 188, loss = 0.44545427\n",
            "Iteration 189, loss = 0.44545424\n",
            "Iteration 190, loss = 0.44545424\n",
            "Iteration 191, loss = 0.44545421\n",
            "Iteration 192, loss = 0.44545420\n",
            "Iteration 193, loss = 0.44545422\n",
            "Iteration 194, loss = 0.44545420\n",
            "Iteration 195, loss = 0.44545416\n",
            "Iteration 196, loss = 0.44545416\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.65417174\n",
            "Iteration 2, loss = 0.60981089\n",
            "Iteration 3, loss = 0.57799566\n",
            "Iteration 4, loss = 0.55635527\n",
            "Iteration 5, loss = 0.54027993\n",
            "Iteration 6, loss = 0.52788476\n",
            "Iteration 7, loss = 0.51798849\n",
            "Iteration 8, loss = 0.51012047\n",
            "Iteration 9, loss = 0.50376978\n",
            "Iteration 10, loss = 0.49845708\n",
            "Iteration 11, loss = 0.49404661\n",
            "Iteration 12, loss = 0.49051477\n",
            "Iteration 13, loss = 0.48756200\n",
            "Iteration 14, loss = 0.48497966\n",
            "Iteration 15, loss = 0.48289826\n",
            "Iteration 16, loss = 0.48097081\n",
            "Iteration 17, loss = 0.47948577\n",
            "Iteration 18, loss = 0.47788383\n",
            "Iteration 19, loss = 0.47673231\n",
            "Iteration 20, loss = 0.47552133\n",
            "Iteration 21, loss = 0.47442105\n",
            "Iteration 22, loss = 0.47342649\n",
            "Iteration 23, loss = 0.47242015\n",
            "Iteration 24, loss = 0.47153190\n",
            "Iteration 25, loss = 0.47077489\n",
            "Iteration 26, loss = 0.46999539\n",
            "Iteration 27, loss = 0.46917516\n",
            "Iteration 28, loss = 0.46847835\n",
            "Iteration 29, loss = 0.46780254\n",
            "Iteration 30, loss = 0.46719773\n",
            "Iteration 31, loss = 0.46658705\n",
            "Iteration 32, loss = 0.46609396\n",
            "Iteration 33, loss = 0.46548402\n",
            "Iteration 34, loss = 0.46490900\n",
            "Iteration 35, loss = 0.46441207\n",
            "Iteration 36, loss = 0.46383965\n",
            "Iteration 37, loss = 0.46332558\n",
            "Iteration 38, loss = 0.46287740\n",
            "Iteration 39, loss = 0.46240178\n",
            "Iteration 40, loss = 0.46191839\n",
            "Iteration 41, loss = 0.46153067\n",
            "Iteration 42, loss = 0.46112985\n",
            "Iteration 43, loss = 0.46070745\n",
            "Iteration 44, loss = 0.46035082\n",
            "Iteration 45, loss = 0.45993428\n",
            "Iteration 46, loss = 0.45958420\n",
            "Iteration 47, loss = 0.45927053\n",
            "Iteration 48, loss = 0.45890357\n",
            "Iteration 49, loss = 0.45859168\n",
            "Iteration 50, loss = 0.45830438\n",
            "Iteration 51, loss = 0.45797513\n",
            "Iteration 52, loss = 0.45772067\n",
            "Iteration 53, loss = 0.45736754\n",
            "Iteration 54, loss = 0.45703884\n",
            "Iteration 55, loss = 0.45680583\n",
            "Iteration 56, loss = 0.45661967\n",
            "Iteration 57, loss = 0.45632546\n",
            "Iteration 58, loss = 0.45600406\n",
            "Iteration 59, loss = 0.45581013\n",
            "Iteration 60, loss = 0.45552582\n",
            "Iteration 61, loss = 0.45530471\n",
            "Iteration 62, loss = 0.45498138\n",
            "Iteration 63, loss = 0.45492951\n",
            "Iteration 64, loss = 0.45472822\n",
            "Iteration 65, loss = 0.45444329\n",
            "Iteration 66, loss = 0.45424537\n",
            "Iteration 67, loss = 0.45407169\n",
            "Iteration 68, loss = 0.45381458\n",
            "Iteration 69, loss = 0.45371244\n",
            "Iteration 70, loss = 0.45351949\n",
            "Iteration 71, loss = 0.45322657\n",
            "Iteration 72, loss = 0.45310232\n",
            "Iteration 73, loss = 0.45297847\n",
            "Iteration 74, loss = 0.45277466\n",
            "Iteration 75, loss = 0.45258819\n",
            "Iteration 76, loss = 0.45246738\n",
            "Iteration 77, loss = 0.45233324\n",
            "Iteration 78, loss = 0.45211163\n",
            "Iteration 79, loss = 0.45195627\n",
            "Iteration 80, loss = 0.45178401\n",
            "Iteration 81, loss = 0.45163514\n",
            "Iteration 82, loss = 0.45157994\n",
            "Iteration 83, loss = 0.45140604\n",
            "Iteration 84, loss = 0.45124451\n",
            "Iteration 85, loss = 0.45117303\n",
            "Iteration 86, loss = 0.45100602\n",
            "Iteration 87, loss = 0.45078263\n",
            "Iteration 88, loss = 0.45069878\n",
            "Iteration 89, loss = 0.45066102\n",
            "Iteration 90, loss = 0.45049856\n",
            "Iteration 91, loss = 0.45032333\n",
            "Iteration 92, loss = 0.45023085\n",
            "Iteration 93, loss = 0.45011576\n",
            "Iteration 94, loss = 0.45002586\n",
            "Iteration 95, loss = 0.45000216\n",
            "Iteration 96, loss = 0.44991322\n",
            "Iteration 97, loss = 0.44978060\n",
            "Iteration 98, loss = 0.44965145\n",
            "Iteration 99, loss = 0.44963371\n",
            "Iteration 100, loss = 0.44951460\n",
            "Iteration 101, loss = 0.44941753\n",
            "Iteration 102, loss = 0.44931371\n",
            "Iteration 103, loss = 0.44919235\n",
            "Iteration 104, loss = 0.44920884\n",
            "Iteration 105, loss = 0.44905752\n",
            "Iteration 106, loss = 0.44898086\n",
            "Iteration 107, loss = 0.44890923\n",
            "Iteration 108, loss = 0.44881227\n",
            "Iteration 109, loss = 0.44870728\n",
            "Iteration 110, loss = 0.44873103\n",
            "Iteration 111, loss = 0.44859029\n",
            "Iteration 112, loss = 0.44850332\n",
            "Iteration 113, loss = 0.44844756\n",
            "Iteration 114, loss = 0.44842351\n",
            "Iteration 115, loss = 0.44844474\n",
            "Iteration 116, loss = 0.44829162\n",
            "Iteration 117, loss = 0.44818798\n",
            "Iteration 118, loss = 0.44815032\n",
            "Iteration 119, loss = 0.44807693\n",
            "Iteration 120, loss = 0.44793494\n",
            "Iteration 121, loss = 0.44790997\n",
            "Iteration 122, loss = 0.44788783\n",
            "Iteration 123, loss = 0.44785870\n",
            "Iteration 124, loss = 0.44783257\n",
            "Iteration 125, loss = 0.44773610\n",
            "Iteration 126, loss = 0.44754973\n",
            "Iteration 127, loss = 0.44752587\n",
            "Iteration 128, loss = 0.44744323\n",
            "Iteration 129, loss = 0.44742291\n",
            "Iteration 130, loss = 0.44731524\n",
            "Iteration 131, loss = 0.44722170\n",
            "Iteration 132, loss = 0.44739658\n",
            "Iteration 133, loss = 0.44722379\n",
            "Iteration 134, loss = 0.44716095\n",
            "Iteration 135, loss = 0.44700739\n",
            "Iteration 136, loss = 0.44700246\n",
            "Iteration 137, loss = 0.44701419\n",
            "Iteration 138, loss = 0.44697076\n",
            "Iteration 139, loss = 0.44689204\n",
            "Iteration 140, loss = 0.44684800\n",
            "Iteration 141, loss = 0.44677404\n",
            "Iteration 142, loss = 0.44681882\n",
            "Iteration 143, loss = 0.44668267\n",
            "Iteration 144, loss = 0.44667706\n",
            "Iteration 145, loss = 0.44662526\n",
            "Iteration 146, loss = 0.44661313\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 147, loss = 0.44633163\n",
            "Iteration 148, loss = 0.44632604\n",
            "Iteration 149, loss = 0.44629235\n",
            "Iteration 150, loss = 0.44627634\n",
            "Iteration 151, loss = 0.44631184\n",
            "Iteration 152, loss = 0.44629551\n",
            "Iteration 153, loss = 0.44629557\n",
            "Iteration 154, loss = 0.44625917\n",
            "Iteration 155, loss = 0.44624592\n",
            "Iteration 156, loss = 0.44624718\n",
            "Iteration 157, loss = 0.44623039\n",
            "Iteration 158, loss = 0.44620979\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 159, loss = 0.44617177\n",
            "Iteration 160, loss = 0.44616935\n",
            "Iteration 161, loss = 0.44616507\n",
            "Iteration 162, loss = 0.44615756\n",
            "Iteration 163, loss = 0.44615497\n",
            "Iteration 164, loss = 0.44615731\n",
            "Iteration 165, loss = 0.44616088\n",
            "Iteration 166, loss = 0.44615961\n",
            "Iteration 167, loss = 0.44615273\n",
            "Iteration 168, loss = 0.44615103\n",
            "Iteration 169, loss = 0.44615240\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 170, loss = 0.44613422\n",
            "Iteration 171, loss = 0.44613549\n",
            "Iteration 172, loss = 0.44613463\n",
            "Iteration 173, loss = 0.44613429\n",
            "Iteration 174, loss = 0.44613402\n",
            "Iteration 175, loss = 0.44613291\n",
            "Iteration 176, loss = 0.44613325\n",
            "Iteration 177, loss = 0.44613367\n",
            "Iteration 178, loss = 0.44613297\n",
            "Iteration 179, loss = 0.44613210\n",
            "Iteration 180, loss = 0.44613142\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 181, loss = 0.44612931\n",
            "Iteration 182, loss = 0.44612960\n",
            "Iteration 183, loss = 0.44612935\n",
            "Iteration 184, loss = 0.44612928\n",
            "Iteration 185, loss = 0.44612919\n",
            "Iteration 186, loss = 0.44612921\n",
            "Iteration 187, loss = 0.44612907\n",
            "Iteration 188, loss = 0.44612910\n",
            "Iteration 189, loss = 0.44612910\n",
            "Iteration 190, loss = 0.44612877\n",
            "Iteration 191, loss = 0.44612877\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 192, loss = 0.44612830\n",
            "Iteration 193, loss = 0.44612828\n",
            "Iteration 194, loss = 0.44612826\n",
            "Iteration 195, loss = 0.44612827\n",
            "Iteration 196, loss = 0.44612826\n",
            "Iteration 197, loss = 0.44612823\n",
            "Iteration 198, loss = 0.44612822\n",
            "Iteration 199, loss = 0.44612819\n",
            "Iteration 200, loss = 0.44612821\n",
            "Iteration 1, loss = 0.65430684\n",
            "Iteration 2, loss = 0.61002039\n",
            "Iteration 3, loss = 0.57830371\n",
            "Iteration 4, loss = 0.55684662\n",
            "Iteration 5, loss = 0.54098174\n",
            "Iteration 6, loss = 0.52867806\n",
            "Iteration 7, loss = 0.51902878\n",
            "Iteration 8, loss = 0.51123696\n",
            "Iteration 9, loss = 0.50496296\n",
            "Iteration 10, loss = 0.49982152\n",
            "Iteration 11, loss = 0.49558350\n",
            "Iteration 12, loss = 0.49207524\n",
            "Iteration 13, loss = 0.48908638\n",
            "Iteration 14, loss = 0.48669719\n",
            "Iteration 15, loss = 0.48450462\n",
            "Iteration 16, loss = 0.48268185\n",
            "Iteration 17, loss = 0.48114862\n",
            "Iteration 18, loss = 0.47968187\n",
            "Iteration 19, loss = 0.47839514\n",
            "Iteration 20, loss = 0.47730235\n",
            "Iteration 21, loss = 0.47622963\n",
            "Iteration 22, loss = 0.47520463\n",
            "Iteration 23, loss = 0.47428155\n",
            "Iteration 24, loss = 0.47347740\n",
            "Iteration 25, loss = 0.47257248\n",
            "Iteration 26, loss = 0.47182485\n",
            "Iteration 27, loss = 0.47106284\n",
            "Iteration 28, loss = 0.47031007\n",
            "Iteration 29, loss = 0.46961406\n",
            "Iteration 30, loss = 0.46899336\n",
            "Iteration 31, loss = 0.46836207\n",
            "Iteration 32, loss = 0.46782697\n",
            "Iteration 33, loss = 0.46727670\n",
            "Iteration 34, loss = 0.46674511\n",
            "Iteration 35, loss = 0.46621443\n",
            "Iteration 36, loss = 0.46558172\n",
            "Iteration 37, loss = 0.46501835\n",
            "Iteration 38, loss = 0.46452323\n",
            "Iteration 39, loss = 0.46399115\n",
            "Iteration 40, loss = 0.46355488\n",
            "Iteration 41, loss = 0.46306271\n",
            "Iteration 42, loss = 0.46265711\n",
            "Iteration 43, loss = 0.46237136\n",
            "Iteration 44, loss = 0.46189518\n",
            "Iteration 45, loss = 0.46152510\n",
            "Iteration 46, loss = 0.46112920\n",
            "Iteration 47, loss = 0.46082949\n",
            "Iteration 48, loss = 0.46046579\n",
            "Iteration 49, loss = 0.46013149\n",
            "Iteration 50, loss = 0.45982606\n",
            "Iteration 51, loss = 0.45963164\n",
            "Iteration 52, loss = 0.45929147\n",
            "Iteration 53, loss = 0.45909191\n",
            "Iteration 54, loss = 0.45872655\n",
            "Iteration 55, loss = 0.45851566\n",
            "Iteration 56, loss = 0.45813257\n",
            "Iteration 57, loss = 0.45789381\n",
            "Iteration 58, loss = 0.45764282\n",
            "Iteration 59, loss = 0.45735779\n",
            "Iteration 60, loss = 0.45712257\n",
            "Iteration 61, loss = 0.45695156\n",
            "Iteration 62, loss = 0.45660250\n",
            "Iteration 63, loss = 0.45641254\n",
            "Iteration 64, loss = 0.45615193\n",
            "Iteration 65, loss = 0.45599485\n",
            "Iteration 66, loss = 0.45576587\n",
            "Iteration 67, loss = 0.45554462\n",
            "Iteration 68, loss = 0.45545957\n",
            "Iteration 69, loss = 0.45519864\n",
            "Iteration 70, loss = 0.45499285\n",
            "Iteration 71, loss = 0.45484548\n",
            "Iteration 72, loss = 0.45469136\n",
            "Iteration 73, loss = 0.45445528\n",
            "Iteration 74, loss = 0.45436610\n",
            "Iteration 75, loss = 0.45418015\n",
            "Iteration 76, loss = 0.45408554\n",
            "Iteration 77, loss = 0.45391404\n",
            "Iteration 78, loss = 0.45381314\n",
            "Iteration 79, loss = 0.45364119\n",
            "Iteration 80, loss = 0.45352442\n",
            "Iteration 81, loss = 0.45337343\n",
            "Iteration 82, loss = 0.45323723\n",
            "Iteration 83, loss = 0.45313323\n",
            "Iteration 84, loss = 0.45302990\n",
            "Iteration 85, loss = 0.45295507\n",
            "Iteration 86, loss = 0.45279622\n",
            "Iteration 87, loss = 0.45272717\n",
            "Iteration 88, loss = 0.45256623\n",
            "Iteration 89, loss = 0.45256201\n",
            "Iteration 90, loss = 0.45239182\n",
            "Iteration 91, loss = 0.45232737\n",
            "Iteration 92, loss = 0.45225708\n",
            "Iteration 93, loss = 0.45211960\n",
            "Iteration 94, loss = 0.45202932\n",
            "Iteration 95, loss = 0.45193975\n",
            "Iteration 96, loss = 0.45181223\n",
            "Iteration 97, loss = 0.45183785\n",
            "Iteration 98, loss = 0.45169282\n",
            "Iteration 99, loss = 0.45163518\n",
            "Iteration 100, loss = 0.45148063\n",
            "Iteration 101, loss = 0.45155078\n",
            "Iteration 102, loss = 0.45139893\n",
            "Iteration 103, loss = 0.45127364\n",
            "Iteration 104, loss = 0.45115067\n",
            "Iteration 105, loss = 0.45114814\n",
            "Iteration 106, loss = 0.45105931\n",
            "Iteration 107, loss = 0.45099766\n",
            "Iteration 108, loss = 0.45093826\n",
            "Iteration 109, loss = 0.45090815\n",
            "Iteration 110, loss = 0.45067886\n",
            "Iteration 111, loss = 0.45067464\n",
            "Iteration 112, loss = 0.45070899\n",
            "Iteration 113, loss = 0.45066437\n",
            "Iteration 114, loss = 0.45059413\n",
            "Iteration 115, loss = 0.45039466\n",
            "Iteration 116, loss = 0.45037834\n",
            "Iteration 117, loss = 0.45039032\n",
            "Iteration 118, loss = 0.45026944\n",
            "Iteration 119, loss = 0.45022423\n",
            "Iteration 120, loss = 0.45021793\n",
            "Iteration 121, loss = 0.45020611\n",
            "Iteration 122, loss = 0.45000181\n",
            "Iteration 123, loss = 0.44996009\n",
            "Iteration 124, loss = 0.44993125\n",
            "Iteration 125, loss = 0.44989866\n",
            "Iteration 126, loss = 0.44983011\n",
            "Iteration 127, loss = 0.44969052\n",
            "Iteration 128, loss = 0.44972426\n",
            "Iteration 129, loss = 0.44979765\n",
            "Iteration 130, loss = 0.44963345\n",
            "Iteration 131, loss = 0.44960775\n",
            "Iteration 132, loss = 0.44957306\n",
            "Iteration 133, loss = 0.44950830\n",
            "Iteration 134, loss = 0.44941547\n",
            "Iteration 135, loss = 0.44948409\n",
            "Iteration 136, loss = 0.44933452\n",
            "Iteration 137, loss = 0.44935703\n",
            "Iteration 138, loss = 0.44926470\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 139, loss = 0.44909406\n",
            "Iteration 140, loss = 0.44905057\n",
            "Iteration 141, loss = 0.44904557\n",
            "Iteration 142, loss = 0.44902888\n",
            "Iteration 143, loss = 0.44902123\n",
            "Iteration 144, loss = 0.44903644\n",
            "Iteration 145, loss = 0.44900759\n",
            "Iteration 146, loss = 0.44899150\n",
            "Iteration 147, loss = 0.44897786\n",
            "Iteration 148, loss = 0.44897130\n",
            "Iteration 149, loss = 0.44895366\n",
            "Iteration 150, loss = 0.44894718\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 151, loss = 0.44890479\n",
            "Iteration 152, loss = 0.44889751\n",
            "Iteration 153, loss = 0.44889770\n",
            "Iteration 154, loss = 0.44889279\n",
            "Iteration 155, loss = 0.44889894\n",
            "Iteration 156, loss = 0.44888961\n",
            "Iteration 157, loss = 0.44888904\n",
            "Iteration 158, loss = 0.44888522\n",
            "Iteration 159, loss = 0.44888666\n",
            "Iteration 160, loss = 0.44888372\n",
            "Iteration 161, loss = 0.44887986\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 162, loss = 0.44886953\n",
            "Iteration 163, loss = 0.44886962\n",
            "Iteration 164, loss = 0.44886959\n",
            "Iteration 165, loss = 0.44886946\n",
            "Iteration 166, loss = 0.44886907\n",
            "Iteration 167, loss = 0.44886896\n",
            "Iteration 168, loss = 0.44886982\n",
            "Iteration 169, loss = 0.44886725\n",
            "Iteration 170, loss = 0.44886710\n",
            "Iteration 171, loss = 0.44886850\n",
            "Iteration 172, loss = 0.44886706\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 173, loss = 0.44886439\n",
            "Iteration 174, loss = 0.44886451\n",
            "Iteration 175, loss = 0.44886443\n",
            "Iteration 176, loss = 0.44886425\n",
            "Iteration 177, loss = 0.44886421\n",
            "Iteration 178, loss = 0.44886408\n",
            "Iteration 179, loss = 0.44886416\n",
            "Iteration 180, loss = 0.44886393\n",
            "Iteration 181, loss = 0.44886409\n",
            "Iteration 182, loss = 0.44886388\n",
            "Iteration 183, loss = 0.44886383\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 184, loss = 0.44886337\n",
            "Iteration 185, loss = 0.44886329\n",
            "Iteration 186, loss = 0.44886332\n",
            "Iteration 187, loss = 0.44886326\n",
            "Iteration 188, loss = 0.44886325\n",
            "Iteration 189, loss = 0.44886322\n",
            "Iteration 190, loss = 0.44886322\n",
            "Iteration 191, loss = 0.44886328\n",
            "Iteration 192, loss = 0.44886329\n",
            "Iteration 193, loss = 0.44886322\n",
            "Iteration 194, loss = 0.44886319\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.65407875\n",
            "Iteration 2, loss = 0.60966353\n",
            "Iteration 3, loss = 0.57795159\n",
            "Iteration 4, loss = 0.55650175\n",
            "Iteration 5, loss = 0.54061132\n",
            "Iteration 6, loss = 0.52822541\n",
            "Iteration 7, loss = 0.51841004\n",
            "Iteration 8, loss = 0.51065608\n",
            "Iteration 9, loss = 0.50428269\n",
            "Iteration 10, loss = 0.49911126\n",
            "Iteration 11, loss = 0.49478839\n",
            "Iteration 12, loss = 0.49114752\n",
            "Iteration 13, loss = 0.48827335\n",
            "Iteration 14, loss = 0.48573109\n",
            "Iteration 15, loss = 0.48350501\n",
            "Iteration 16, loss = 0.48158826\n",
            "Iteration 17, loss = 0.47993965\n",
            "Iteration 18, loss = 0.47852720\n",
            "Iteration 19, loss = 0.47722199\n",
            "Iteration 20, loss = 0.47597630\n",
            "Iteration 21, loss = 0.47482383\n",
            "Iteration 22, loss = 0.47384164\n",
            "Iteration 23, loss = 0.47282804\n",
            "Iteration 24, loss = 0.47192695\n",
            "Iteration 25, loss = 0.47101550\n",
            "Iteration 26, loss = 0.47032101\n",
            "Iteration 27, loss = 0.46947024\n",
            "Iteration 28, loss = 0.46873529\n",
            "Iteration 29, loss = 0.46800276\n",
            "Iteration 30, loss = 0.46739486\n",
            "Iteration 31, loss = 0.46684510\n",
            "Iteration 32, loss = 0.46621309\n",
            "Iteration 33, loss = 0.46561153\n",
            "Iteration 34, loss = 0.46501622\n",
            "Iteration 35, loss = 0.46444133\n",
            "Iteration 36, loss = 0.46400660\n",
            "Iteration 37, loss = 0.46336828\n",
            "Iteration 38, loss = 0.46297215\n",
            "Iteration 39, loss = 0.46241056\n",
            "Iteration 40, loss = 0.46202805\n",
            "Iteration 41, loss = 0.46160604\n",
            "Iteration 42, loss = 0.46113439\n",
            "Iteration 43, loss = 0.46074695\n",
            "Iteration 44, loss = 0.46036547\n",
            "Iteration 45, loss = 0.46009255\n",
            "Iteration 46, loss = 0.45967423\n",
            "Iteration 47, loss = 0.45930185\n",
            "Iteration 48, loss = 0.45903546\n",
            "Iteration 49, loss = 0.45865870\n",
            "Iteration 50, loss = 0.45839196\n",
            "Iteration 51, loss = 0.45807203\n",
            "Iteration 52, loss = 0.45778327\n",
            "Iteration 53, loss = 0.45746040\n",
            "Iteration 54, loss = 0.45721966\n",
            "Iteration 55, loss = 0.45692971\n",
            "Iteration 56, loss = 0.45663035\n",
            "Iteration 57, loss = 0.45635313\n",
            "Iteration 58, loss = 0.45599397\n",
            "Iteration 59, loss = 0.45586646\n",
            "Iteration 60, loss = 0.45568069\n",
            "Iteration 61, loss = 0.45535722\n",
            "Iteration 62, loss = 0.45501502\n",
            "Iteration 63, loss = 0.45497039\n",
            "Iteration 64, loss = 0.45472197\n",
            "Iteration 65, loss = 0.45455134\n",
            "Iteration 66, loss = 0.45433470\n",
            "Iteration 67, loss = 0.45408516\n",
            "Iteration 68, loss = 0.45376732\n",
            "Iteration 69, loss = 0.45368791\n",
            "Iteration 70, loss = 0.45350983\n",
            "Iteration 71, loss = 0.45330801\n",
            "Iteration 72, loss = 0.45309948\n",
            "Iteration 73, loss = 0.45296483\n",
            "Iteration 74, loss = 0.45278873\n",
            "Iteration 75, loss = 0.45263420\n",
            "Iteration 76, loss = 0.45247349\n",
            "Iteration 77, loss = 0.45227031\n",
            "Iteration 78, loss = 0.45215971\n",
            "Iteration 79, loss = 0.45209553\n",
            "Iteration 80, loss = 0.45192729\n",
            "Iteration 81, loss = 0.45182589\n",
            "Iteration 82, loss = 0.45166980\n",
            "Iteration 83, loss = 0.45153700\n",
            "Iteration 84, loss = 0.45134044\n",
            "Iteration 85, loss = 0.45131259\n",
            "Iteration 86, loss = 0.45117021\n",
            "Iteration 87, loss = 0.45106057\n",
            "Iteration 88, loss = 0.45096691\n",
            "Iteration 89, loss = 0.45087675\n",
            "Iteration 90, loss = 0.45078189\n",
            "Iteration 91, loss = 0.45068208\n",
            "Iteration 92, loss = 0.45056367\n",
            "Iteration 93, loss = 0.45038993\n",
            "Iteration 94, loss = 0.45032663\n",
            "Iteration 95, loss = 0.45021119\n",
            "Iteration 96, loss = 0.45021141\n",
            "Iteration 97, loss = 0.45009261\n",
            "Iteration 98, loss = 0.45000789\n",
            "Iteration 99, loss = 0.44989403\n",
            "Iteration 100, loss = 0.44982703\n",
            "Iteration 101, loss = 0.44971856\n",
            "Iteration 102, loss = 0.44971629\n",
            "Iteration 103, loss = 0.44953356\n",
            "Iteration 104, loss = 0.44948935\n",
            "Iteration 105, loss = 0.44946197\n",
            "Iteration 106, loss = 0.44937370\n",
            "Iteration 107, loss = 0.44937661\n",
            "Iteration 108, loss = 0.44929909\n",
            "Iteration 109, loss = 0.44916411\n",
            "Iteration 110, loss = 0.44908511\n",
            "Iteration 111, loss = 0.44901263\n",
            "Iteration 112, loss = 0.44893043\n",
            "Iteration 113, loss = 0.44895228\n",
            "Iteration 114, loss = 0.44882646\n",
            "Iteration 115, loss = 0.44873663\n",
            "Iteration 116, loss = 0.44872829\n",
            "Iteration 117, loss = 0.44857804\n",
            "Iteration 118, loss = 0.44856369\n",
            "Iteration 119, loss = 0.44848059\n",
            "Iteration 120, loss = 0.44844896\n",
            "Iteration 121, loss = 0.44832232\n",
            "Iteration 122, loss = 0.44828561\n",
            "Iteration 123, loss = 0.44831359\n",
            "Iteration 124, loss = 0.44825207\n",
            "Iteration 125, loss = 0.44818760\n",
            "Iteration 126, loss = 0.44814995\n",
            "Iteration 127, loss = 0.44804724\n",
            "Iteration 128, loss = 0.44805295\n",
            "Iteration 129, loss = 0.44801531\n",
            "Iteration 130, loss = 0.44791107\n",
            "Iteration 131, loss = 0.44785446\n",
            "Iteration 132, loss = 0.44778379\n",
            "Iteration 133, loss = 0.44773231\n",
            "Iteration 134, loss = 0.44768162\n",
            "Iteration 135, loss = 0.44771475\n",
            "Iteration 136, loss = 0.44787539\n",
            "Iteration 137, loss = 0.44759918\n",
            "Iteration 138, loss = 0.44753052\n",
            "Iteration 139, loss = 0.44755052\n",
            "Iteration 140, loss = 0.44734842\n",
            "Iteration 141, loss = 0.44740068\n",
            "Iteration 142, loss = 0.44734017\n",
            "Iteration 143, loss = 0.44724570\n",
            "Iteration 144, loss = 0.44732552\n",
            "Iteration 145, loss = 0.44711419\n",
            "Iteration 146, loss = 0.44716649\n",
            "Iteration 147, loss = 0.44708671\n",
            "Iteration 148, loss = 0.44717551\n",
            "Iteration 149, loss = 0.44705926\n",
            "Iteration 150, loss = 0.44702155\n",
            "Iteration 151, loss = 0.44717854\n",
            "Iteration 152, loss = 0.44693517\n",
            "Iteration 153, loss = 0.44693206\n",
            "Iteration 154, loss = 0.44687328\n",
            "Iteration 155, loss = 0.44677994\n",
            "Iteration 156, loss = 0.44685353\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 157, loss = 0.44661782\n",
            "Iteration 158, loss = 0.44656416\n",
            "Iteration 159, loss = 0.44654427\n",
            "Iteration 160, loss = 0.44654740\n",
            "Iteration 161, loss = 0.44653443\n",
            "Iteration 162, loss = 0.44651948\n",
            "Iteration 163, loss = 0.44651022\n",
            "Iteration 164, loss = 0.44649965\n",
            "Iteration 165, loss = 0.44649203\n",
            "Iteration 166, loss = 0.44649019\n",
            "Iteration 167, loss = 0.44647989\n",
            "Iteration 168, loss = 0.44648882\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 169, loss = 0.44642310\n",
            "Iteration 170, loss = 0.44642267\n",
            "Iteration 171, loss = 0.44641962\n",
            "Iteration 172, loss = 0.44641701\n",
            "Iteration 173, loss = 0.44641482\n",
            "Iteration 174, loss = 0.44641435\n",
            "Iteration 175, loss = 0.44640962\n",
            "Iteration 176, loss = 0.44641059\n",
            "Iteration 177, loss = 0.44640969\n",
            "Iteration 178, loss = 0.44640618\n",
            "Iteration 179, loss = 0.44640575\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 180, loss = 0.44639283\n",
            "Iteration 181, loss = 0.44639300\n",
            "Iteration 182, loss = 0.44639340\n",
            "Iteration 183, loss = 0.44639280\n",
            "Iteration 184, loss = 0.44639251\n",
            "Iteration 185, loss = 0.44639250\n",
            "Iteration 186, loss = 0.44639281\n",
            "Iteration 187, loss = 0.44639176\n",
            "Iteration 188, loss = 0.44639128\n",
            "Iteration 189, loss = 0.44639019\n",
            "Iteration 190, loss = 0.44639066\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 191, loss = 0.44638808\n",
            "Iteration 192, loss = 0.44638783\n",
            "Iteration 193, loss = 0.44638778\n",
            "Iteration 194, loss = 0.44638777\n",
            "Iteration 195, loss = 0.44638770\n",
            "Iteration 196, loss = 0.44638757\n",
            "Iteration 197, loss = 0.44638764\n",
            "Iteration 198, loss = 0.44638753\n",
            "Iteration 199, loss = 0.44638737\n",
            "Iteration 200, loss = 0.44638747\n",
            "Iteration 1, loss = 0.65376305\n",
            "Iteration 2, loss = 0.60892856\n",
            "Iteration 3, loss = 0.57707234\n",
            "Iteration 4, loss = 0.55552645\n",
            "Iteration 5, loss = 0.53951972\n",
            "Iteration 6, loss = 0.52703465\n",
            "Iteration 7, loss = 0.51722366\n",
            "Iteration 8, loss = 0.50938490\n",
            "Iteration 9, loss = 0.50303318\n",
            "Iteration 10, loss = 0.49782506\n",
            "Iteration 11, loss = 0.49351154\n",
            "Iteration 12, loss = 0.49000378\n",
            "Iteration 13, loss = 0.48703021\n",
            "Iteration 14, loss = 0.48460732\n",
            "Iteration 15, loss = 0.48240862\n",
            "Iteration 16, loss = 0.48043689\n",
            "Iteration 17, loss = 0.47895560\n",
            "Iteration 18, loss = 0.47749644\n",
            "Iteration 19, loss = 0.47627582\n",
            "Iteration 20, loss = 0.47514090\n",
            "Iteration 21, loss = 0.47410147\n",
            "Iteration 22, loss = 0.47305797\n",
            "Iteration 23, loss = 0.47210500\n",
            "Iteration 24, loss = 0.47131033\n",
            "Iteration 25, loss = 0.47047544\n",
            "Iteration 26, loss = 0.46965881\n",
            "Iteration 27, loss = 0.46891958\n",
            "Iteration 28, loss = 0.46834023\n",
            "Iteration 29, loss = 0.46761753\n",
            "Iteration 30, loss = 0.46689618\n",
            "Iteration 31, loss = 0.46631200\n",
            "Iteration 32, loss = 0.46565687\n",
            "Iteration 33, loss = 0.46520068\n",
            "Iteration 34, loss = 0.46455952\n",
            "Iteration 35, loss = 0.46403026\n",
            "Iteration 36, loss = 0.46340811\n",
            "Iteration 37, loss = 0.46293887\n",
            "Iteration 38, loss = 0.46243887\n",
            "Iteration 39, loss = 0.46194133\n",
            "Iteration 40, loss = 0.46149241\n",
            "Iteration 41, loss = 0.46108350\n",
            "Iteration 42, loss = 0.46062975\n",
            "Iteration 43, loss = 0.46020172\n",
            "Iteration 44, loss = 0.45981271\n",
            "Iteration 45, loss = 0.45947075\n",
            "Iteration 46, loss = 0.45911923\n",
            "Iteration 47, loss = 0.45869783\n",
            "Iteration 48, loss = 0.45837796\n",
            "Iteration 49, loss = 0.45804695\n",
            "Iteration 50, loss = 0.45776916\n",
            "Iteration 51, loss = 0.45741148\n",
            "Iteration 52, loss = 0.45718810\n",
            "Iteration 53, loss = 0.45675243\n",
            "Iteration 54, loss = 0.45660777\n",
            "Iteration 55, loss = 0.45625294\n",
            "Iteration 56, loss = 0.45601806\n",
            "Iteration 57, loss = 0.45570828\n",
            "Iteration 58, loss = 0.45544122\n",
            "Iteration 59, loss = 0.45520810\n",
            "Iteration 60, loss = 0.45491581\n",
            "Iteration 61, loss = 0.45472164\n",
            "Iteration 62, loss = 0.45448620\n",
            "Iteration 63, loss = 0.45437692\n",
            "Iteration 64, loss = 0.45396351\n",
            "Iteration 65, loss = 0.45381712\n",
            "Iteration 66, loss = 0.45353280\n",
            "Iteration 67, loss = 0.45333262\n",
            "Iteration 68, loss = 0.45311606\n",
            "Iteration 69, loss = 0.45291545\n",
            "Iteration 70, loss = 0.45274073\n",
            "Iteration 71, loss = 0.45254125\n",
            "Iteration 72, loss = 0.45249864\n",
            "Iteration 73, loss = 0.45226179\n",
            "Iteration 74, loss = 0.45204251\n",
            "Iteration 75, loss = 0.45187831\n",
            "Iteration 76, loss = 0.45168764\n",
            "Iteration 77, loss = 0.45161948\n",
            "Iteration 78, loss = 0.45138325\n",
            "Iteration 79, loss = 0.45131682\n",
            "Iteration 80, loss = 0.45118294\n",
            "Iteration 81, loss = 0.45093921\n",
            "Iteration 82, loss = 0.45089349\n",
            "Iteration 83, loss = 0.45083906\n",
            "Iteration 84, loss = 0.45061346\n",
            "Iteration 85, loss = 0.45058728\n",
            "Iteration 86, loss = 0.45038089\n",
            "Iteration 87, loss = 0.45021953\n",
            "Iteration 88, loss = 0.45012681\n",
            "Iteration 89, loss = 0.45008351\n",
            "Iteration 90, loss = 0.44991051\n",
            "Iteration 91, loss = 0.44985444\n",
            "Iteration 92, loss = 0.44967566\n",
            "Iteration 93, loss = 0.44959163\n",
            "Iteration 94, loss = 0.44950439\n",
            "Iteration 95, loss = 0.44932463\n",
            "Iteration 96, loss = 0.44931179\n",
            "Iteration 97, loss = 0.44921185\n",
            "Iteration 98, loss = 0.44904037\n",
            "Iteration 99, loss = 0.44893411\n",
            "Iteration 100, loss = 0.44892042\n",
            "Iteration 101, loss = 0.44878623\n",
            "Iteration 102, loss = 0.44875585\n",
            "Iteration 103, loss = 0.44873576\n",
            "Iteration 104, loss = 0.44851220\n",
            "Iteration 105, loss = 0.44848368\n",
            "Iteration 106, loss = 0.44842192\n",
            "Iteration 107, loss = 0.44833336\n",
            "Iteration 108, loss = 0.44819182\n",
            "Iteration 109, loss = 0.44812749\n",
            "Iteration 110, loss = 0.44813201\n",
            "Iteration 111, loss = 0.44792734\n",
            "Iteration 112, loss = 0.44793153\n",
            "Iteration 113, loss = 0.44784173\n",
            "Iteration 114, loss = 0.44775545\n",
            "Iteration 115, loss = 0.44775694\n",
            "Iteration 116, loss = 0.44757368\n",
            "Iteration 117, loss = 0.44753849\n",
            "Iteration 118, loss = 0.44756817\n",
            "Iteration 119, loss = 0.44744491\n",
            "Iteration 120, loss = 0.44735222\n",
            "Iteration 121, loss = 0.44729980\n",
            "Iteration 122, loss = 0.44726074\n",
            "Iteration 123, loss = 0.44716150\n",
            "Iteration 124, loss = 0.44712328\n",
            "Iteration 125, loss = 0.44701283\n",
            "Iteration 126, loss = 0.44705924\n",
            "Iteration 127, loss = 0.44689472\n",
            "Iteration 128, loss = 0.44689029\n",
            "Iteration 129, loss = 0.44690263\n",
            "Iteration 130, loss = 0.44672803\n",
            "Iteration 131, loss = 0.44672700\n",
            "Iteration 132, loss = 0.44663980\n",
            "Iteration 133, loss = 0.44659395\n",
            "Iteration 134, loss = 0.44658117\n",
            "Iteration 135, loss = 0.44646176\n",
            "Iteration 136, loss = 0.44644832\n",
            "Iteration 137, loss = 0.44633561\n",
            "Iteration 138, loss = 0.44632301\n",
            "Iteration 139, loss = 0.44628894\n",
            "Iteration 140, loss = 0.44629615\n",
            "Iteration 141, loss = 0.44624886\n",
            "Iteration 142, loss = 0.44617904\n",
            "Iteration 143, loss = 0.44610593\n",
            "Iteration 144, loss = 0.44605584\n",
            "Iteration 145, loss = 0.44605435\n",
            "Iteration 146, loss = 0.44605970\n",
            "Iteration 147, loss = 0.44601279\n",
            "Iteration 148, loss = 0.44591156\n",
            "Iteration 149, loss = 0.44598760\n",
            "Iteration 150, loss = 0.44587265\n",
            "Iteration 151, loss = 0.44584021\n",
            "Iteration 152, loss = 0.44566138\n",
            "Iteration 153, loss = 0.44570431\n",
            "Iteration 154, loss = 0.44567420\n",
            "Iteration 155, loss = 0.44570024\n",
            "Iteration 156, loss = 0.44559231\n",
            "Iteration 157, loss = 0.44553731\n",
            "Iteration 158, loss = 0.44547785\n",
            "Iteration 159, loss = 0.44552817\n",
            "Iteration 160, loss = 0.44555542\n",
            "Iteration 161, loss = 0.44542368\n",
            "Iteration 162, loss = 0.44542249\n",
            "Iteration 163, loss = 0.44531300\n",
            "Iteration 164, loss = 0.44526796\n",
            "Iteration 165, loss = 0.44524613\n",
            "Iteration 166, loss = 0.44516821\n",
            "Iteration 167, loss = 0.44518518\n",
            "Iteration 168, loss = 0.44522794\n",
            "Iteration 169, loss = 0.44520077\n",
            "Iteration 170, loss = 0.44515039\n",
            "Iteration 171, loss = 0.44502230\n",
            "Iteration 172, loss = 0.44508986\n",
            "Iteration 173, loss = 0.44500821\n",
            "Iteration 174, loss = 0.44493814\n",
            "Iteration 175, loss = 0.44497953\n",
            "Iteration 176, loss = 0.44485609\n",
            "Iteration 177, loss = 0.44484644\n",
            "Iteration 178, loss = 0.44489847\n",
            "Iteration 179, loss = 0.44484960\n",
            "Iteration 180, loss = 0.44478799\n",
            "Iteration 181, loss = 0.44472371\n",
            "Iteration 182, loss = 0.44471741\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 183, loss = 0.44459542\n",
            "Iteration 184, loss = 0.44450135\n",
            "Iteration 185, loss = 0.44450662\n",
            "Iteration 186, loss = 0.44448924\n",
            "Iteration 187, loss = 0.44449505\n",
            "Iteration 188, loss = 0.44446356\n",
            "Iteration 189, loss = 0.44448344\n",
            "Iteration 190, loss = 0.44446191\n",
            "Iteration 191, loss = 0.44448430\n",
            "Iteration 192, loss = 0.44447421\n",
            "Iteration 193, loss = 0.44443894\n",
            "Iteration 194, loss = 0.44444580\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 195, loss = 0.44440169\n",
            "Iteration 196, loss = 0.44438437\n",
            "Iteration 197, loss = 0.44438778\n",
            "Iteration 198, loss = 0.44437859\n",
            "Iteration 199, loss = 0.44438219\n",
            "Iteration 200, loss = 0.44437853\n",
            "Iteration 1, loss = 0.65365009\n",
            "Iteration 2, loss = 0.60897526\n",
            "Iteration 3, loss = 0.57715329\n",
            "Iteration 4, loss = 0.55560967\n",
            "Iteration 5, loss = 0.53968759\n",
            "Iteration 6, loss = 0.52726704\n",
            "Iteration 7, loss = 0.51751355\n",
            "Iteration 8, loss = 0.50973723\n",
            "Iteration 9, loss = 0.50347572\n",
            "Iteration 10, loss = 0.49828976\n",
            "Iteration 11, loss = 0.49401556\n",
            "Iteration 12, loss = 0.49045847\n",
            "Iteration 13, loss = 0.48759070\n",
            "Iteration 14, loss = 0.48507294\n",
            "Iteration 15, loss = 0.48290537\n",
            "Iteration 16, loss = 0.48106347\n",
            "Iteration 17, loss = 0.47946846\n",
            "Iteration 18, loss = 0.47805586\n",
            "Iteration 19, loss = 0.47683411\n",
            "Iteration 20, loss = 0.47561551\n",
            "Iteration 21, loss = 0.47449223\n",
            "Iteration 22, loss = 0.47350994\n",
            "Iteration 23, loss = 0.47260636\n",
            "Iteration 24, loss = 0.47169045\n",
            "Iteration 25, loss = 0.47093148\n",
            "Iteration 26, loss = 0.47013735\n",
            "Iteration 27, loss = 0.46939017\n",
            "Iteration 28, loss = 0.46868997\n",
            "Iteration 29, loss = 0.46798607\n",
            "Iteration 30, loss = 0.46745307\n",
            "Iteration 31, loss = 0.46679724\n",
            "Iteration 32, loss = 0.46616308\n",
            "Iteration 33, loss = 0.46568519\n",
            "Iteration 34, loss = 0.46509667\n",
            "Iteration 35, loss = 0.46455424\n",
            "Iteration 36, loss = 0.46415214\n",
            "Iteration 37, loss = 0.46363997\n",
            "Iteration 38, loss = 0.46309119\n",
            "Iteration 39, loss = 0.46268420\n",
            "Iteration 40, loss = 0.46225068\n",
            "Iteration 41, loss = 0.46187352\n",
            "Iteration 42, loss = 0.46138354\n",
            "Iteration 43, loss = 0.46098953\n",
            "Iteration 44, loss = 0.46060707\n",
            "Iteration 45, loss = 0.46018893\n",
            "Iteration 46, loss = 0.45987417\n",
            "Iteration 47, loss = 0.45949615\n",
            "Iteration 48, loss = 0.45913253\n",
            "Iteration 49, loss = 0.45885368\n",
            "Iteration 50, loss = 0.45850224\n",
            "Iteration 51, loss = 0.45822235\n",
            "Iteration 52, loss = 0.45783204\n",
            "Iteration 53, loss = 0.45768553\n",
            "Iteration 54, loss = 0.45733541\n",
            "Iteration 55, loss = 0.45708246\n",
            "Iteration 56, loss = 0.45675509\n",
            "Iteration 57, loss = 0.45672881\n",
            "Iteration 58, loss = 0.45632546\n",
            "Iteration 59, loss = 0.45608632\n",
            "Iteration 60, loss = 0.45581240\n",
            "Iteration 61, loss = 0.45555838\n",
            "Iteration 62, loss = 0.45531991\n",
            "Iteration 63, loss = 0.45508992\n",
            "Iteration 64, loss = 0.45484779\n",
            "Iteration 65, loss = 0.45467958\n",
            "Iteration 66, loss = 0.45446514\n",
            "Iteration 67, loss = 0.45431170\n",
            "Iteration 68, loss = 0.45408612\n",
            "Iteration 69, loss = 0.45387822\n",
            "Iteration 70, loss = 0.45375893\n",
            "Iteration 71, loss = 0.45355073\n",
            "Iteration 72, loss = 0.45335661\n",
            "Iteration 73, loss = 0.45323215\n",
            "Iteration 74, loss = 0.45302594\n",
            "Iteration 75, loss = 0.45290245\n",
            "Iteration 76, loss = 0.45278928\n",
            "Iteration 77, loss = 0.45256663\n",
            "Iteration 78, loss = 0.45248910\n",
            "Iteration 79, loss = 0.45229184\n",
            "Iteration 80, loss = 0.45215990\n",
            "Iteration 81, loss = 0.45202268\n",
            "Iteration 82, loss = 0.45189697\n",
            "Iteration 83, loss = 0.45187870\n",
            "Iteration 84, loss = 0.45165481\n",
            "Iteration 85, loss = 0.45156527\n",
            "Iteration 86, loss = 0.45144529\n",
            "Iteration 87, loss = 0.45138091\n",
            "Iteration 88, loss = 0.45128232\n",
            "Iteration 89, loss = 0.45122026\n",
            "Iteration 90, loss = 0.45103568\n",
            "Iteration 91, loss = 0.45092772\n",
            "Iteration 92, loss = 0.45076974\n",
            "Iteration 93, loss = 0.45071303\n",
            "Iteration 94, loss = 0.45059245\n",
            "Iteration 95, loss = 0.45045719\n",
            "Iteration 96, loss = 0.45039539\n",
            "Iteration 97, loss = 0.45028750\n",
            "Iteration 98, loss = 0.45019686\n",
            "Iteration 99, loss = 0.45006248\n",
            "Iteration 100, loss = 0.45006974\n",
            "Iteration 101, loss = 0.44992860\n",
            "Iteration 102, loss = 0.44990350\n",
            "Iteration 103, loss = 0.44977896\n",
            "Iteration 104, loss = 0.44974882\n",
            "Iteration 105, loss = 0.44964501\n",
            "Iteration 106, loss = 0.44952561\n",
            "Iteration 107, loss = 0.44953041\n",
            "Iteration 108, loss = 0.44946592\n",
            "Iteration 109, loss = 0.44939896\n",
            "Iteration 110, loss = 0.44933028\n",
            "Iteration 111, loss = 0.44927203\n",
            "Iteration 112, loss = 0.44921201\n",
            "Iteration 113, loss = 0.44911398\n",
            "Iteration 114, loss = 0.44907117\n",
            "Iteration 115, loss = 0.44894185\n",
            "Iteration 116, loss = 0.44891224\n",
            "Iteration 117, loss = 0.44889184\n",
            "Iteration 118, loss = 0.44884357\n",
            "Iteration 119, loss = 0.44871584\n",
            "Iteration 120, loss = 0.44867861\n",
            "Iteration 121, loss = 0.44862527\n",
            "Iteration 122, loss = 0.44850614\n",
            "Iteration 123, loss = 0.44852495\n",
            "Iteration 124, loss = 0.44844520\n",
            "Iteration 125, loss = 0.44845718\n",
            "Iteration 126, loss = 0.44831089\n",
            "Iteration 127, loss = 0.44829072\n",
            "Iteration 128, loss = 0.44819269\n",
            "Iteration 129, loss = 0.44810954\n",
            "Iteration 130, loss = 0.44808355\n",
            "Iteration 131, loss = 0.44803601\n",
            "Iteration 132, loss = 0.44799092\n",
            "Iteration 133, loss = 0.44791761\n",
            "Iteration 134, loss = 0.44789985\n",
            "Iteration 135, loss = 0.44785500\n",
            "Iteration 136, loss = 0.44776276\n",
            "Iteration 137, loss = 0.44776002\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 138, loss = 0.44747891\n",
            "Iteration 139, loss = 0.44749431\n",
            "Iteration 140, loss = 0.44747088\n",
            "Iteration 141, loss = 0.44746379\n",
            "Iteration 142, loss = 0.44744287\n",
            "Iteration 143, loss = 0.44744167\n",
            "Iteration 144, loss = 0.44742783\n",
            "Iteration 145, loss = 0.44743438\n",
            "Iteration 146, loss = 0.44741093\n",
            "Iteration 147, loss = 0.44739734\n",
            "Iteration 148, loss = 0.44739388\n",
            "Iteration 149, loss = 0.44739053\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 150, loss = 0.44736152\n",
            "Iteration 151, loss = 0.44733594\n",
            "Iteration 152, loss = 0.44732748\n",
            "Iteration 153, loss = 0.44732292\n",
            "Iteration 154, loss = 0.44731869\n",
            "Iteration 155, loss = 0.44731694\n",
            "Iteration 156, loss = 0.44731432\n",
            "Iteration 157, loss = 0.44731983\n",
            "Iteration 158, loss = 0.44731299\n",
            "Iteration 159, loss = 0.44730938\n",
            "Iteration 160, loss = 0.44730533\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 161, loss = 0.44729615\n",
            "Iteration 162, loss = 0.44729574\n",
            "Iteration 163, loss = 0.44729631\n",
            "Iteration 164, loss = 0.44729676\n",
            "Iteration 165, loss = 0.44729513\n",
            "Iteration 166, loss = 0.44729512\n",
            "Iteration 167, loss = 0.44729468\n",
            "Iteration 168, loss = 0.44729520\n",
            "Iteration 169, loss = 0.44729400\n",
            "Iteration 170, loss = 0.44729313\n",
            "Iteration 171, loss = 0.44729388\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 172, loss = 0.44729049\n",
            "Iteration 173, loss = 0.44729054\n",
            "Iteration 174, loss = 0.44729040\n",
            "Iteration 175, loss = 0.44729041\n",
            "Iteration 176, loss = 0.44729017\n",
            "Iteration 177, loss = 0.44728994\n",
            "Iteration 178, loss = 0.44728998\n",
            "Iteration 179, loss = 0.44728999\n",
            "Iteration 180, loss = 0.44728980\n",
            "Iteration 181, loss = 0.44728963\n",
            "Iteration 182, loss = 0.44728963\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 183, loss = 0.44728918\n",
            "Iteration 184, loss = 0.44728914\n",
            "Iteration 185, loss = 0.44728916\n",
            "Iteration 186, loss = 0.44728910\n",
            "Iteration 187, loss = 0.44728916\n",
            "Iteration 188, loss = 0.44728911\n",
            "Iteration 189, loss = 0.44728908\n",
            "Iteration 190, loss = 0.44728905\n",
            "Iteration 191, loss = 0.44728904\n",
            "Iteration 192, loss = 0.44728903\n",
            "Iteration 193, loss = 0.44728899\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.65414298\n",
            "Iteration 2, loss = 0.60958903\n",
            "Iteration 3, loss = 0.57775089\n",
            "Iteration 4, loss = 0.55614568\n",
            "Iteration 5, loss = 0.54007825\n",
            "Iteration 6, loss = 0.52762443\n",
            "Iteration 7, loss = 0.51786350\n",
            "Iteration 8, loss = 0.51016449\n",
            "Iteration 9, loss = 0.50386343\n",
            "Iteration 10, loss = 0.49861931\n",
            "Iteration 11, loss = 0.49438135\n",
            "Iteration 12, loss = 0.49085272\n",
            "Iteration 13, loss = 0.48803545\n",
            "Iteration 14, loss = 0.48556543\n",
            "Iteration 15, loss = 0.48344642\n",
            "Iteration 16, loss = 0.48155654\n",
            "Iteration 17, loss = 0.47992816\n",
            "Iteration 18, loss = 0.47849512\n",
            "Iteration 19, loss = 0.47725920\n",
            "Iteration 20, loss = 0.47605384\n",
            "Iteration 21, loss = 0.47494450\n",
            "Iteration 22, loss = 0.47396255\n",
            "Iteration 23, loss = 0.47302393\n",
            "Iteration 24, loss = 0.47211874\n",
            "Iteration 25, loss = 0.47128493\n",
            "Iteration 26, loss = 0.47043928\n",
            "Iteration 27, loss = 0.46971140\n",
            "Iteration 28, loss = 0.46894656\n",
            "Iteration 29, loss = 0.46826368\n",
            "Iteration 30, loss = 0.46754839\n",
            "Iteration 31, loss = 0.46695258\n",
            "Iteration 32, loss = 0.46630463\n",
            "Iteration 33, loss = 0.46573734\n",
            "Iteration 34, loss = 0.46516488\n",
            "Iteration 35, loss = 0.46460656\n",
            "Iteration 36, loss = 0.46413489\n",
            "Iteration 37, loss = 0.46354608\n",
            "Iteration 38, loss = 0.46303341\n",
            "Iteration 39, loss = 0.46257371\n",
            "Iteration 40, loss = 0.46210760\n",
            "Iteration 41, loss = 0.46171054\n",
            "Iteration 42, loss = 0.46130260\n",
            "Iteration 43, loss = 0.46088282\n",
            "Iteration 44, loss = 0.46046090\n",
            "Iteration 45, loss = 0.46007351\n",
            "Iteration 46, loss = 0.45977665\n",
            "Iteration 47, loss = 0.45939562\n",
            "Iteration 48, loss = 0.45905919\n",
            "Iteration 49, loss = 0.45874099\n",
            "Iteration 50, loss = 0.45836419\n",
            "Iteration 51, loss = 0.45808973\n",
            "Iteration 52, loss = 0.45783547\n",
            "Iteration 53, loss = 0.45747985\n",
            "Iteration 54, loss = 0.45722628\n",
            "Iteration 55, loss = 0.45695885\n",
            "Iteration 56, loss = 0.45669970\n",
            "Iteration 57, loss = 0.45638300\n",
            "Iteration 58, loss = 0.45611646\n",
            "Iteration 59, loss = 0.45588983\n",
            "Iteration 60, loss = 0.45573537\n",
            "Iteration 61, loss = 0.45537478\n",
            "Iteration 62, loss = 0.45526904\n",
            "Iteration 63, loss = 0.45493977\n",
            "Iteration 64, loss = 0.45475620\n",
            "Iteration 65, loss = 0.45446328\n",
            "Iteration 66, loss = 0.45430359\n",
            "Iteration 67, loss = 0.45412333\n",
            "Iteration 68, loss = 0.45395925\n",
            "Iteration 69, loss = 0.45377099\n",
            "Iteration 70, loss = 0.45371077\n",
            "Iteration 71, loss = 0.45335591\n",
            "Iteration 72, loss = 0.45331307\n",
            "Iteration 73, loss = 0.45300693\n",
            "Iteration 74, loss = 0.45286592\n",
            "Iteration 75, loss = 0.45270296\n",
            "Iteration 76, loss = 0.45254105\n",
            "Iteration 77, loss = 0.45241420\n",
            "Iteration 78, loss = 0.45222379\n",
            "Iteration 79, loss = 0.45206272\n",
            "Iteration 80, loss = 0.45200490\n",
            "Iteration 81, loss = 0.45187811\n",
            "Iteration 82, loss = 0.45170646\n",
            "Iteration 83, loss = 0.45157803\n",
            "Iteration 84, loss = 0.45148616\n",
            "Iteration 85, loss = 0.45132621\n",
            "Iteration 86, loss = 0.45114796\n",
            "Iteration 87, loss = 0.45110623\n",
            "Iteration 88, loss = 0.45105246\n",
            "Iteration 89, loss = 0.45090108\n",
            "Iteration 90, loss = 0.45068390\n",
            "Iteration 91, loss = 0.45070123\n",
            "Iteration 92, loss = 0.45052492\n",
            "Iteration 93, loss = 0.45043929\n",
            "Iteration 94, loss = 0.45028051\n",
            "Iteration 95, loss = 0.45027596\n",
            "Iteration 96, loss = 0.45026218\n",
            "Iteration 97, loss = 0.45006123\n",
            "Iteration 98, loss = 0.44995174\n",
            "Iteration 99, loss = 0.44992157\n",
            "Iteration 100, loss = 0.44973361\n",
            "Iteration 101, loss = 0.44967659\n",
            "Iteration 102, loss = 0.44971471\n",
            "Iteration 103, loss = 0.44956669\n",
            "Iteration 104, loss = 0.44947007\n",
            "Iteration 105, loss = 0.44945999\n",
            "Iteration 106, loss = 0.44928147\n",
            "Iteration 107, loss = 0.44926238\n",
            "Iteration 108, loss = 0.44912458\n",
            "Iteration 109, loss = 0.44903644\n",
            "Iteration 110, loss = 0.44905088\n",
            "Iteration 111, loss = 0.44890671\n",
            "Iteration 112, loss = 0.44883062\n",
            "Iteration 113, loss = 0.44885361\n",
            "Iteration 114, loss = 0.44867294\n",
            "Iteration 115, loss = 0.44860213\n",
            "Iteration 116, loss = 0.44849757\n",
            "Iteration 117, loss = 0.44849617\n",
            "Iteration 118, loss = 0.44837525\n",
            "Iteration 119, loss = 0.44838003\n",
            "Iteration 120, loss = 0.44827554\n",
            "Iteration 121, loss = 0.44812584\n",
            "Iteration 122, loss = 0.44811246\n",
            "Iteration 123, loss = 0.44807721\n",
            "Iteration 124, loss = 0.44800508\n",
            "Iteration 125, loss = 0.44791158\n",
            "Iteration 126, loss = 0.44789479\n",
            "Iteration 127, loss = 0.44783277\n",
            "Iteration 128, loss = 0.44775600\n",
            "Iteration 129, loss = 0.44776832\n",
            "Iteration 130, loss = 0.44768390\n",
            "Iteration 131, loss = 0.44773272\n",
            "Iteration 132, loss = 0.44758060\n",
            "Iteration 133, loss = 0.44749295\n",
            "Iteration 134, loss = 0.44755329\n",
            "Iteration 135, loss = 0.44741497\n",
            "Iteration 136, loss = 0.44727688\n",
            "Iteration 137, loss = 0.44741370\n",
            "Iteration 138, loss = 0.44725734\n",
            "Iteration 139, loss = 0.44716491\n",
            "Iteration 140, loss = 0.44711486\n",
            "Iteration 141, loss = 0.44707453\n",
            "Iteration 142, loss = 0.44703842\n",
            "Iteration 143, loss = 0.44697224\n",
            "Iteration 144, loss = 0.44688373\n",
            "Iteration 145, loss = 0.44678437\n",
            "Iteration 146, loss = 0.44685663\n",
            "Iteration 147, loss = 0.44676270\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 148, loss = 0.44664551\n",
            "Iteration 149, loss = 0.44653314\n",
            "Iteration 150, loss = 0.44652313\n",
            "Iteration 151, loss = 0.44651425\n",
            "Iteration 152, loss = 0.44650188\n",
            "Iteration 153, loss = 0.44651015\n",
            "Iteration 154, loss = 0.44651631\n",
            "Iteration 155, loss = 0.44648187\n",
            "Iteration 156, loss = 0.44647268\n",
            "Iteration 157, loss = 0.44647711\n",
            "Iteration 158, loss = 0.44646263\n",
            "Iteration 159, loss = 0.44645796\n",
            "Iteration 160, loss = 0.44642608\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 161, loss = 0.44638487\n",
            "Iteration 162, loss = 0.44638266\n",
            "Iteration 163, loss = 0.44638354\n",
            "Iteration 164, loss = 0.44637756\n",
            "Iteration 165, loss = 0.44637049\n",
            "Iteration 166, loss = 0.44636877\n",
            "Iteration 167, loss = 0.44636601\n",
            "Iteration 168, loss = 0.44636686\n",
            "Iteration 169, loss = 0.44636202\n",
            "Iteration 170, loss = 0.44636263\n",
            "Iteration 171, loss = 0.44636595\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 172, loss = 0.44634893\n",
            "Iteration 173, loss = 0.44634785\n",
            "Iteration 174, loss = 0.44634784\n",
            "Iteration 175, loss = 0.44634657\n",
            "Iteration 176, loss = 0.44634717\n",
            "Iteration 177, loss = 0.44634608\n",
            "Iteration 178, loss = 0.44634582\n",
            "Iteration 179, loss = 0.44634496\n",
            "Iteration 180, loss = 0.44634533\n",
            "Iteration 181, loss = 0.44634479\n",
            "Iteration 182, loss = 0.44634417\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 183, loss = 0.44634154\n",
            "Iteration 184, loss = 0.44634166\n",
            "Iteration 185, loss = 0.44634152\n",
            "Iteration 186, loss = 0.44634157\n",
            "Iteration 187, loss = 0.44634139\n",
            "Iteration 188, loss = 0.44634128\n",
            "Iteration 189, loss = 0.44634126\n",
            "Iteration 190, loss = 0.44634126\n",
            "Iteration 191, loss = 0.44634122\n",
            "Iteration 192, loss = 0.44634120\n",
            "Iteration 193, loss = 0.44634092\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 194, loss = 0.44634047\n",
            "Iteration 195, loss = 0.44634042\n",
            "Iteration 196, loss = 0.44634043\n",
            "Iteration 197, loss = 0.44634038\n",
            "Iteration 198, loss = 0.44634037\n",
            "Iteration 199, loss = 0.44634038\n",
            "Iteration 200, loss = 0.44634038\n",
            "Iteration 1, loss = 0.65388847\n",
            "Iteration 2, loss = 0.60906525\n",
            "Iteration 3, loss = 0.57717605\n",
            "Iteration 4, loss = 0.55561944\n",
            "Iteration 5, loss = 0.53955039\n",
            "Iteration 6, loss = 0.52709895\n",
            "Iteration 7, loss = 0.51718810\n",
            "Iteration 8, loss = 0.50934049\n",
            "Iteration 9, loss = 0.50297205\n",
            "Iteration 10, loss = 0.49762990\n",
            "Iteration 11, loss = 0.49333001\n",
            "Iteration 12, loss = 0.48972941\n",
            "Iteration 13, loss = 0.48684080\n",
            "Iteration 14, loss = 0.48431782\n",
            "Iteration 15, loss = 0.48222706\n",
            "Iteration 16, loss = 0.48032880\n",
            "Iteration 17, loss = 0.47882579\n",
            "Iteration 18, loss = 0.47739260\n",
            "Iteration 19, loss = 0.47612497\n",
            "Iteration 20, loss = 0.47492160\n",
            "Iteration 21, loss = 0.47393767\n",
            "Iteration 22, loss = 0.47290165\n",
            "Iteration 23, loss = 0.47190307\n",
            "Iteration 24, loss = 0.47102261\n",
            "Iteration 25, loss = 0.47021245\n",
            "Iteration 26, loss = 0.46944217\n",
            "Iteration 27, loss = 0.46860685\n",
            "Iteration 28, loss = 0.46797148\n",
            "Iteration 29, loss = 0.46721362\n",
            "Iteration 30, loss = 0.46657194\n",
            "Iteration 31, loss = 0.46596830\n",
            "Iteration 32, loss = 0.46545843\n",
            "Iteration 33, loss = 0.46486430\n",
            "Iteration 34, loss = 0.46436006\n",
            "Iteration 35, loss = 0.46381745\n",
            "Iteration 36, loss = 0.46327882\n",
            "Iteration 37, loss = 0.46292073\n",
            "Iteration 38, loss = 0.46240010\n",
            "Iteration 39, loss = 0.46186486\n",
            "Iteration 40, loss = 0.46143876\n",
            "Iteration 41, loss = 0.46093309\n",
            "Iteration 42, loss = 0.46056257\n",
            "Iteration 43, loss = 0.46018498\n",
            "Iteration 44, loss = 0.45980580\n",
            "Iteration 45, loss = 0.45936086\n",
            "Iteration 46, loss = 0.45897936\n",
            "Iteration 47, loss = 0.45862278\n",
            "Iteration 48, loss = 0.45824858\n",
            "Iteration 49, loss = 0.45804137\n",
            "Iteration 50, loss = 0.45760565\n",
            "Iteration 51, loss = 0.45733205\n",
            "Iteration 52, loss = 0.45706802\n",
            "Iteration 53, loss = 0.45677015\n",
            "Iteration 54, loss = 0.45645282\n",
            "Iteration 55, loss = 0.45628490\n",
            "Iteration 56, loss = 0.45602003\n",
            "Iteration 57, loss = 0.45569326\n",
            "Iteration 58, loss = 0.45547798\n",
            "Iteration 59, loss = 0.45522341\n",
            "Iteration 60, loss = 0.45497848\n",
            "Iteration 61, loss = 0.45469623\n",
            "Iteration 62, loss = 0.45449405\n",
            "Iteration 63, loss = 0.45425730\n",
            "Iteration 64, loss = 0.45401846\n",
            "Iteration 65, loss = 0.45378236\n",
            "Iteration 66, loss = 0.45361716\n",
            "Iteration 67, loss = 0.45336410\n",
            "Iteration 68, loss = 0.45312873\n",
            "Iteration 69, loss = 0.45294821\n",
            "Iteration 70, loss = 0.45278469\n",
            "Iteration 71, loss = 0.45256358\n",
            "Iteration 72, loss = 0.45236860\n",
            "Iteration 73, loss = 0.45224553\n",
            "Iteration 74, loss = 0.45203168\n",
            "Iteration 75, loss = 0.45196395\n",
            "Iteration 76, loss = 0.45168255\n",
            "Iteration 77, loss = 0.45158215\n",
            "Iteration 78, loss = 0.45141501\n",
            "Iteration 79, loss = 0.45128194\n",
            "Iteration 80, loss = 0.45108953\n",
            "Iteration 81, loss = 0.45097855\n",
            "Iteration 82, loss = 0.45086465\n",
            "Iteration 83, loss = 0.45070696\n",
            "Iteration 84, loss = 0.45066766\n",
            "Iteration 85, loss = 0.45046809\n",
            "Iteration 86, loss = 0.45035862\n",
            "Iteration 87, loss = 0.45026212\n",
            "Iteration 88, loss = 0.45013153\n",
            "Iteration 89, loss = 0.45005293\n",
            "Iteration 90, loss = 0.44992276\n",
            "Iteration 91, loss = 0.44982534\n",
            "Iteration 92, loss = 0.44974155\n",
            "Iteration 93, loss = 0.44969539\n",
            "Iteration 94, loss = 0.44955991\n",
            "Iteration 95, loss = 0.44942277\n",
            "Iteration 96, loss = 0.44936440\n",
            "Iteration 97, loss = 0.44927027\n",
            "Iteration 98, loss = 0.44937508\n",
            "Iteration 99, loss = 0.44908228\n",
            "Iteration 100, loss = 0.44904697\n",
            "Iteration 101, loss = 0.44894032\n",
            "Iteration 102, loss = 0.44890675\n",
            "Iteration 103, loss = 0.44879448\n",
            "Iteration 104, loss = 0.44869809\n",
            "Iteration 105, loss = 0.44865682\n",
            "Iteration 106, loss = 0.44861115\n",
            "Iteration 107, loss = 0.44852902\n",
            "Iteration 108, loss = 0.44852138\n",
            "Iteration 109, loss = 0.44826251\n",
            "Iteration 110, loss = 0.44832402\n",
            "Iteration 111, loss = 0.44823501\n",
            "Iteration 112, loss = 0.44814649\n",
            "Iteration 113, loss = 0.44793329\n",
            "Iteration 114, loss = 0.44799386\n",
            "Iteration 115, loss = 0.44792251\n",
            "Iteration 116, loss = 0.44781363\n",
            "Iteration 117, loss = 0.44777398\n",
            "Iteration 118, loss = 0.44782849\n",
            "Iteration 119, loss = 0.44763971\n",
            "Iteration 120, loss = 0.44771680\n",
            "Iteration 121, loss = 0.44761383\n",
            "Iteration 122, loss = 0.44743326\n",
            "Iteration 123, loss = 0.44748377\n",
            "Iteration 124, loss = 0.44740499\n",
            "Iteration 125, loss = 0.44730662\n",
            "Iteration 126, loss = 0.44739602\n",
            "Iteration 127, loss = 0.44725435\n",
            "Iteration 128, loss = 0.44723262\n",
            "Iteration 129, loss = 0.44694804\n",
            "Iteration 130, loss = 0.44699773\n",
            "Iteration 131, loss = 0.44703384\n",
            "Iteration 132, loss = 0.44699319\n",
            "Iteration 133, loss = 0.44688205\n",
            "Iteration 134, loss = 0.44688830\n",
            "Iteration 135, loss = 0.44687244\n",
            "Iteration 136, loss = 0.44669678\n",
            "Iteration 137, loss = 0.44666291\n",
            "Iteration 138, loss = 0.44656712\n",
            "Iteration 139, loss = 0.44663789\n",
            "Iteration 140, loss = 0.44644427\n",
            "Iteration 141, loss = 0.44649798\n",
            "Iteration 142, loss = 0.44642183\n",
            "Iteration 143, loss = 0.44638527\n",
            "Iteration 144, loss = 0.44636764\n",
            "Iteration 145, loss = 0.44630881\n",
            "Iteration 146, loss = 0.44632483\n",
            "Iteration 147, loss = 0.44624297\n",
            "Iteration 148, loss = 0.44614065\n",
            "Iteration 149, loss = 0.44619751\n",
            "Iteration 150, loss = 0.44612927\n",
            "Iteration 151, loss = 0.44603691\n",
            "Iteration 152, loss = 0.44597279\n",
            "Iteration 153, loss = 0.44595869\n",
            "Iteration 154, loss = 0.44596720\n",
            "Iteration 155, loss = 0.44597372\n",
            "Iteration 156, loss = 0.44589698\n",
            "Iteration 157, loss = 0.44586450\n",
            "Iteration 158, loss = 0.44579426\n",
            "Iteration 159, loss = 0.44579842\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 160, loss = 0.44554770\n",
            "Iteration 161, loss = 0.44551364\n",
            "Iteration 162, loss = 0.44553598\n",
            "Iteration 163, loss = 0.44550667\n",
            "Iteration 164, loss = 0.44549965\n",
            "Iteration 165, loss = 0.44547776\n",
            "Iteration 166, loss = 0.44549976\n",
            "Iteration 167, loss = 0.44547659\n",
            "Iteration 168, loss = 0.44547533\n",
            "Iteration 169, loss = 0.44548685\n",
            "Iteration 170, loss = 0.44546916\n",
            "Iteration 171, loss = 0.44544745\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 172, loss = 0.44539822\n",
            "Iteration 173, loss = 0.44539814\n",
            "Iteration 174, loss = 0.44539673\n",
            "Iteration 175, loss = 0.44539564\n",
            "Iteration 176, loss = 0.44539068\n",
            "Iteration 177, loss = 0.44539334\n",
            "Iteration 178, loss = 0.44539226\n",
            "Iteration 179, loss = 0.44538567\n",
            "Iteration 180, loss = 0.44538710\n",
            "Iteration 181, loss = 0.44538837\n",
            "Iteration 182, loss = 0.44538575\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 183, loss = 0.44537160\n",
            "Iteration 184, loss = 0.44537337\n",
            "Iteration 185, loss = 0.44537200\n",
            "Iteration 186, loss = 0.44537153\n",
            "Iteration 187, loss = 0.44537188\n",
            "Iteration 188, loss = 0.44537045\n",
            "Iteration 189, loss = 0.44537106\n",
            "Iteration 190, loss = 0.44537038\n",
            "Iteration 191, loss = 0.44536982\n",
            "Iteration 192, loss = 0.44536955\n",
            "Iteration 193, loss = 0.44536972\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 194, loss = 0.44536692\n",
            "Iteration 195, loss = 0.44536675\n",
            "Iteration 196, loss = 0.44536677\n",
            "Iteration 197, loss = 0.44536666\n",
            "Iteration 198, loss = 0.44536674\n",
            "Iteration 199, loss = 0.44536648\n",
            "Iteration 200, loss = 0.44536642\n",
            "[-0.84145589 -0.86119679 -0.87723627 -0.8426897  -0.79210364 -0.82665022\n",
            " -0.85996299 -0.80197409 -0.81554596 -0.88710672]\n",
            "Iteration 1, loss = 0.67569793\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  4.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 2, loss = 0.66804421\n",
            "Iteration 3, loss = 0.65965618\n",
            "Iteration 4, loss = 0.65169187\n",
            "Iteration 5, loss = 0.64403151\n",
            "Iteration 6, loss = 0.63716720\n",
            "Iteration 7, loss = 0.63047763\n",
            "Iteration 8, loss = 0.62428516\n",
            "Iteration 9, loss = 0.61842941\n",
            "Iteration 10, loss = 0.61290918\n",
            "Iteration 11, loss = 0.60769708\n",
            "Iteration 12, loss = 0.60251474\n",
            "Iteration 13, loss = 0.59764551\n",
            "Iteration 14, loss = 0.59298194\n",
            "Iteration 15, loss = 0.58861423\n",
            "Iteration 16, loss = 0.58430630\n",
            "Iteration 17, loss = 0.58026027\n",
            "Iteration 18, loss = 0.57660347\n",
            "Iteration 19, loss = 0.57309505\n",
            "Iteration 20, loss = 0.56987123\n",
            "Iteration 21, loss = 0.56663809\n",
            "Iteration 22, loss = 0.56373407\n",
            "Iteration 23, loss = 0.56097208\n",
            "Iteration 24, loss = 0.55837942\n",
            "Iteration 25, loss = 0.55590634\n",
            "Iteration 26, loss = 0.55351426\n",
            "Iteration 27, loss = 0.55122680\n",
            "Iteration 28, loss = 0.54905671\n",
            "Iteration 29, loss = 0.54699631\n",
            "Iteration 30, loss = 0.54493047\n",
            "Iteration 31, loss = 0.54288515\n",
            "Iteration 32, loss = 0.54087744\n",
            "Iteration 33, loss = 0.53896830\n",
            "Iteration 34, loss = 0.53722129\n",
            "Iteration 35, loss = 0.53546077\n",
            "Iteration 36, loss = 0.53369382\n",
            "Iteration 37, loss = 0.53205203\n",
            "Iteration 38, loss = 0.53040340\n",
            "Iteration 39, loss = 0.52883293\n",
            "Iteration 40, loss = 0.52737287\n",
            "Iteration 41, loss = 0.52592076\n",
            "Iteration 42, loss = 0.52439511\n",
            "Iteration 43, loss = 0.52297220\n",
            "Iteration 44, loss = 0.52160066\n",
            "Iteration 45, loss = 0.52032155\n",
            "Iteration 46, loss = 0.51904814\n",
            "Iteration 47, loss = 0.51787454\n",
            "Iteration 48, loss = 0.51689526\n",
            "Iteration 49, loss = 0.51554339\n",
            "Iteration 50, loss = 0.51426080\n",
            "Iteration 51, loss = 0.51312678\n",
            "Iteration 52, loss = 0.51202790\n",
            "Iteration 53, loss = 0.51092053\n",
            "Iteration 54, loss = 0.50988079\n",
            "Iteration 55, loss = 0.50884075\n",
            "Iteration 56, loss = 0.50786097\n",
            "Iteration 57, loss = 0.50706535\n",
            "Iteration 58, loss = 0.50582881\n",
            "Iteration 59, loss = 0.50505998\n",
            "Iteration 60, loss = 0.50400276\n",
            "Iteration 61, loss = 0.50300848\n",
            "Iteration 62, loss = 0.50209988\n",
            "Iteration 63, loss = 0.50122504\n",
            "Iteration 64, loss = 0.50023885\n",
            "Iteration 65, loss = 0.49929491\n",
            "Iteration 66, loss = 0.49844379\n",
            "Iteration 67, loss = 0.49758753\n",
            "Iteration 68, loss = 0.49674553\n",
            "Iteration 69, loss = 0.49597803\n",
            "Iteration 70, loss = 0.49528124\n",
            "Iteration 71, loss = 0.49442113\n",
            "Iteration 72, loss = 0.49371827\n",
            "Iteration 73, loss = 0.49304040\n",
            "Iteration 74, loss = 0.49230160\n",
            "Iteration 75, loss = 0.49162100\n",
            "Iteration 76, loss = 0.49091166\n",
            "Iteration 77, loss = 0.49025651\n",
            "Iteration 78, loss = 0.48959968\n",
            "Iteration 79, loss = 0.48893333\n",
            "Iteration 80, loss = 0.48827681\n",
            "Iteration 81, loss = 0.48780223\n",
            "Iteration 82, loss = 0.48704362\n",
            "Iteration 83, loss = 0.48647727\n",
            "Iteration 84, loss = 0.48594103\n",
            "Iteration 85, loss = 0.48537643\n",
            "Iteration 86, loss = 0.48492152\n",
            "Iteration 87, loss = 0.48436489\n",
            "Iteration 88, loss = 0.48389935\n",
            "Iteration 89, loss = 0.48334099\n",
            "Iteration 90, loss = 0.48284550\n",
            "Iteration 91, loss = 0.48248146\n",
            "Iteration 92, loss = 0.48196833\n",
            "Iteration 93, loss = 0.48159205\n",
            "Iteration 94, loss = 0.48112448\n",
            "Iteration 95, loss = 0.48072381\n",
            "Iteration 96, loss = 0.48027466\n",
            "Iteration 97, loss = 0.47990693\n",
            "Iteration 98, loss = 0.47952995\n",
            "Iteration 99, loss = 0.47910584\n",
            "Iteration 100, loss = 0.47883916\n",
            "Iteration 101, loss = 0.47837061\n",
            "Iteration 102, loss = 0.47816558\n",
            "Iteration 103, loss = 0.47768938\n",
            "Iteration 104, loss = 0.47734892\n",
            "Iteration 105, loss = 0.47696927\n",
            "Iteration 106, loss = 0.47668267\n",
            "Iteration 107, loss = 0.47626342\n",
            "Iteration 108, loss = 0.47599525\n",
            "Iteration 109, loss = 0.47567806\n",
            "Iteration 110, loss = 0.47538294\n",
            "Iteration 111, loss = 0.47499723\n",
            "Iteration 112, loss = 0.47478116\n",
            "Iteration 113, loss = 0.47444303\n",
            "Iteration 114, loss = 0.47422588\n",
            "Iteration 115, loss = 0.47393056\n",
            "Iteration 116, loss = 0.47374393\n",
            "Iteration 117, loss = 0.47334110\n",
            "Iteration 118, loss = 0.47301004\n",
            "Iteration 119, loss = 0.47275703\n",
            "Iteration 120, loss = 0.47245218\n",
            "Iteration 121, loss = 0.47220379\n",
            "Iteration 122, loss = 0.47204883\n",
            "Iteration 123, loss = 0.47177282\n",
            "Iteration 124, loss = 0.47143886\n",
            "Iteration 125, loss = 0.47113340\n",
            "Iteration 126, loss = 0.47092679\n",
            "Iteration 127, loss = 0.47069128\n",
            "Iteration 128, loss = 0.47041467\n",
            "Iteration 129, loss = 0.47018695\n",
            "Iteration 130, loss = 0.46995771\n",
            "Iteration 131, loss = 0.46972387\n",
            "Iteration 132, loss = 0.46950863\n",
            "Iteration 133, loss = 0.46934908\n",
            "Iteration 134, loss = 0.46911629\n",
            "Iteration 135, loss = 0.46892253\n",
            "Iteration 136, loss = 0.46861877\n",
            "Iteration 137, loss = 0.46840099\n",
            "Iteration 138, loss = 0.46815954\n",
            "Iteration 139, loss = 0.46794058\n",
            "Iteration 140, loss = 0.46779900\n",
            "Iteration 141, loss = 0.46758067\n",
            "Iteration 142, loss = 0.46736504\n",
            "Iteration 143, loss = 0.46719507\n",
            "Iteration 144, loss = 0.46697077\n",
            "Iteration 145, loss = 0.46677885\n",
            "Iteration 146, loss = 0.46663374\n",
            "Iteration 147, loss = 0.46640249\n",
            "Iteration 148, loss = 0.46625199\n",
            "Iteration 149, loss = 0.46599886\n",
            "Iteration 150, loss = 0.46590490\n",
            "Iteration 151, loss = 0.46567310\n",
            "Iteration 152, loss = 0.46545636\n",
            "Iteration 153, loss = 0.46535194\n",
            "Iteration 154, loss = 0.46515814\n",
            "Iteration 155, loss = 0.46494460\n",
            "Iteration 156, loss = 0.46473354\n",
            "Iteration 157, loss = 0.46464509\n",
            "Iteration 158, loss = 0.46446263\n",
            "Iteration 159, loss = 0.46428097\n",
            "Iteration 160, loss = 0.46409388\n",
            "Iteration 161, loss = 0.46397841\n",
            "Iteration 162, loss = 0.46378463\n",
            "Iteration 163, loss = 0.46359413\n",
            "Iteration 164, loss = 0.46352896\n",
            "Iteration 165, loss = 0.46340655\n",
            "Iteration 166, loss = 0.46330586\n",
            "Iteration 167, loss = 0.46303264\n",
            "Iteration 168, loss = 0.46288929\n",
            "Iteration 169, loss = 0.46287848\n",
            "Iteration 170, loss = 0.46268805\n",
            "Iteration 171, loss = 0.46246252\n",
            "Iteration 172, loss = 0.46231244\n",
            "Iteration 173, loss = 0.46222013\n",
            "Iteration 174, loss = 0.46207328\n",
            "Iteration 175, loss = 0.46197401\n",
            "Iteration 176, loss = 0.46186478\n",
            "Iteration 177, loss = 0.46178749\n",
            "Iteration 178, loss = 0.46153354\n",
            "Iteration 179, loss = 0.46141781\n",
            "Iteration 180, loss = 0.46121641\n",
            "Iteration 181, loss = 0.46110207\n",
            "Iteration 182, loss = 0.46106235\n",
            "Iteration 183, loss = 0.46078451\n",
            "Iteration 184, loss = 0.46074739\n",
            "Iteration 185, loss = 0.46064081\n",
            "Iteration 186, loss = 0.46042071\n",
            "Iteration 187, loss = 0.46033373\n",
            "Iteration 188, loss = 0.46018802\n",
            "Iteration 189, loss = 0.46005623\n",
            "Iteration 190, loss = 0.45994122\n",
            "Iteration 191, loss = 0.45982927\n",
            "Iteration 192, loss = 0.45977938\n",
            "Iteration 193, loss = 0.45953640\n",
            "Iteration 194, loss = 0.45952974\n",
            "Iteration 195, loss = 0.45928294\n",
            "Iteration 196, loss = 0.45919175\n",
            "Iteration 197, loss = 0.45905278\n",
            "Iteration 198, loss = 0.45890725\n",
            "Iteration 199, loss = 0.45878151\n",
            "Iteration 200, loss = 0.45870487\n",
            "Iteration 1, loss = 0.67777516\n",
            "Iteration 2, loss = 0.67125334\n",
            "Iteration 3, loss = 0.66396508\n",
            "Iteration 4, loss = 0.65682642\n",
            "Iteration 5, loss = 0.65012247\n",
            "Iteration 6, loss = 0.64346394\n",
            "Iteration 7, loss = 0.63738723\n",
            "Iteration 8, loss = 0.63135253\n",
            "Iteration 9, loss = 0.62567859\n",
            "Iteration 10, loss = 0.62017149\n",
            "Iteration 11, loss = 0.61486587\n",
            "Iteration 12, loss = 0.60984992\n",
            "Iteration 13, loss = 0.60498477\n",
            "Iteration 14, loss = 0.60025984\n",
            "Iteration 15, loss = 0.59575987\n",
            "Iteration 16, loss = 0.59142557\n",
            "Iteration 17, loss = 0.58747063\n",
            "Iteration 18, loss = 0.58362699\n",
            "Iteration 19, loss = 0.57995270\n",
            "Iteration 20, loss = 0.57651670\n",
            "Iteration 21, loss = 0.57325652\n",
            "Iteration 22, loss = 0.57023142\n",
            "Iteration 23, loss = 0.56735835\n",
            "Iteration 24, loss = 0.56484491\n",
            "Iteration 25, loss = 0.56213472\n",
            "Iteration 26, loss = 0.55961986\n",
            "Iteration 27, loss = 0.55721117\n",
            "Iteration 28, loss = 0.55501910\n",
            "Iteration 29, loss = 0.55272223\n",
            "Iteration 30, loss = 0.55061697\n",
            "Iteration 31, loss = 0.54861671\n",
            "Iteration 32, loss = 0.54661425\n",
            "Iteration 33, loss = 0.54469760\n",
            "Iteration 34, loss = 0.54284216\n",
            "Iteration 35, loss = 0.54109881\n",
            "Iteration 36, loss = 0.53936141\n",
            "Iteration 37, loss = 0.53769322\n",
            "Iteration 38, loss = 0.53619200\n",
            "Iteration 39, loss = 0.53463108\n",
            "Iteration 40, loss = 0.53325207\n",
            "Iteration 41, loss = 0.53177907\n",
            "Iteration 42, loss = 0.53041335\n",
            "Iteration 43, loss = 0.52912757\n",
            "Iteration 44, loss = 0.52795980\n",
            "Iteration 45, loss = 0.52668248\n",
            "Iteration 46, loss = 0.52561136\n",
            "Iteration 47, loss = 0.52426825\n",
            "Iteration 48, loss = 0.52321602\n",
            "Iteration 49, loss = 0.52205238\n",
            "Iteration 50, loss = 0.52104636\n",
            "Iteration 51, loss = 0.52005842\n",
            "Iteration 52, loss = 0.51905527\n",
            "Iteration 53, loss = 0.51813796\n",
            "Iteration 54, loss = 0.51729734\n",
            "Iteration 55, loss = 0.51629486\n",
            "Iteration 56, loss = 0.51550046\n",
            "Iteration 57, loss = 0.51467043\n",
            "Iteration 58, loss = 0.51399593\n",
            "Iteration 59, loss = 0.51309178\n",
            "Iteration 60, loss = 0.51225439\n",
            "Iteration 61, loss = 0.51161987\n",
            "Iteration 62, loss = 0.51083579\n",
            "Iteration 63, loss = 0.51012571\n",
            "Iteration 64, loss = 0.50941676\n",
            "Iteration 65, loss = 0.50878166\n",
            "Iteration 66, loss = 0.50809842\n",
            "Iteration 67, loss = 0.50746249\n",
            "Iteration 68, loss = 0.50681771\n",
            "Iteration 69, loss = 0.50617725\n",
            "Iteration 70, loss = 0.50557421\n",
            "Iteration 71, loss = 0.50500271\n",
            "Iteration 72, loss = 0.50439118\n",
            "Iteration 73, loss = 0.50383305\n",
            "Iteration 74, loss = 0.50331132\n",
            "Iteration 75, loss = 0.50275970\n",
            "Iteration 76, loss = 0.50215006\n",
            "Iteration 77, loss = 0.50170350\n",
            "Iteration 78, loss = 0.50113024\n",
            "Iteration 79, loss = 0.50067461\n",
            "Iteration 80, loss = 0.50023628\n",
            "Iteration 81, loss = 0.49968798\n",
            "Iteration 82, loss = 0.49928445\n",
            "Iteration 83, loss = 0.49894041\n",
            "Iteration 84, loss = 0.49840761\n",
            "Iteration 85, loss = 0.49793036\n",
            "Iteration 86, loss = 0.49757724\n",
            "Iteration 87, loss = 0.49716385\n",
            "Iteration 88, loss = 0.49677367\n",
            "Iteration 89, loss = 0.49640966\n",
            "Iteration 90, loss = 0.49608569\n",
            "Iteration 91, loss = 0.49570626\n",
            "Iteration 92, loss = 0.49528550\n",
            "Iteration 93, loss = 0.49498361\n",
            "Iteration 94, loss = 0.49466999\n",
            "Iteration 95, loss = 0.49425790\n",
            "Iteration 96, loss = 0.49413122\n",
            "Iteration 97, loss = 0.49383701\n",
            "Iteration 98, loss = 0.49343852\n",
            "Iteration 99, loss = 0.49310427\n",
            "Iteration 100, loss = 0.49280726\n",
            "Iteration 101, loss = 0.49247310\n",
            "Iteration 102, loss = 0.49222491\n",
            "Iteration 103, loss = 0.49192712\n",
            "Iteration 104, loss = 0.49170294\n",
            "Iteration 105, loss = 0.49142116\n",
            "Iteration 106, loss = 0.49120470\n",
            "Iteration 107, loss = 0.49089001\n",
            "Iteration 108, loss = 0.49064213\n",
            "Iteration 109, loss = 0.49036972\n",
            "Iteration 110, loss = 0.49016461\n",
            "Iteration 111, loss = 0.48996410\n",
            "Iteration 112, loss = 0.48972333\n",
            "Iteration 113, loss = 0.48940819\n",
            "Iteration 114, loss = 0.48921365\n",
            "Iteration 115, loss = 0.48891201\n",
            "Iteration 116, loss = 0.48875077\n",
            "Iteration 117, loss = 0.48864742\n",
            "Iteration 118, loss = 0.48831314\n",
            "Iteration 119, loss = 0.48818995\n",
            "Iteration 120, loss = 0.48796937\n",
            "Iteration 121, loss = 0.48769730\n",
            "Iteration 122, loss = 0.48754582\n",
            "Iteration 123, loss = 0.48729714\n",
            "Iteration 124, loss = 0.48707950\n",
            "Iteration 125, loss = 0.48700363\n",
            "Iteration 126, loss = 0.48671777\n",
            "Iteration 127, loss = 0.48651717\n",
            "Iteration 128, loss = 0.48641568\n",
            "Iteration 129, loss = 0.48619731\n",
            "Iteration 130, loss = 0.48603233\n",
            "Iteration 131, loss = 0.48579907\n",
            "Iteration 132, loss = 0.48562077\n",
            "Iteration 133, loss = 0.48542755\n",
            "Iteration 134, loss = 0.48529645\n",
            "Iteration 135, loss = 0.48507375\n",
            "Iteration 136, loss = 0.48492826\n",
            "Iteration 137, loss = 0.48475173\n",
            "Iteration 138, loss = 0.48458827\n",
            "Iteration 139, loss = 0.48438896\n",
            "Iteration 140, loss = 0.48419953\n",
            "Iteration 141, loss = 0.48402957\n",
            "Iteration 142, loss = 0.48384779\n",
            "Iteration 143, loss = 0.48372192\n",
            "Iteration 144, loss = 0.48355619\n",
            "Iteration 145, loss = 0.48353436\n",
            "Iteration 146, loss = 0.48334824\n",
            "Iteration 147, loss = 0.48304655\n",
            "Iteration 148, loss = 0.48291731\n",
            "Iteration 149, loss = 0.48281911\n",
            "Iteration 150, loss = 0.48262693\n",
            "Iteration 151, loss = 0.48247505\n",
            "Iteration 152, loss = 0.48236770\n",
            "Iteration 153, loss = 0.48218831\n",
            "Iteration 154, loss = 0.48204510\n",
            "Iteration 155, loss = 0.48191796\n",
            "Iteration 156, loss = 0.48174540\n",
            "Iteration 157, loss = 0.48155270\n",
            "Iteration 158, loss = 0.48140521\n",
            "Iteration 159, loss = 0.48130592\n",
            "Iteration 160, loss = 0.48111946\n",
            "Iteration 161, loss = 0.48107040\n",
            "Iteration 162, loss = 0.48086141\n",
            "Iteration 163, loss = 0.48094048\n",
            "Iteration 164, loss = 0.48067166\n",
            "Iteration 165, loss = 0.48050516\n",
            "Iteration 166, loss = 0.48038285\n",
            "Iteration 167, loss = 0.48018491\n",
            "Iteration 168, loss = 0.47999607\n",
            "Iteration 169, loss = 0.48000221\n",
            "Iteration 170, loss = 0.47974015\n",
            "Iteration 171, loss = 0.47969553\n",
            "Iteration 172, loss = 0.47949906\n",
            "Iteration 173, loss = 0.47934792\n",
            "Iteration 174, loss = 0.47916488\n",
            "Iteration 175, loss = 0.47924306\n",
            "Iteration 176, loss = 0.47898963\n",
            "Iteration 177, loss = 0.47881300\n",
            "Iteration 178, loss = 0.47870418\n",
            "Iteration 179, loss = 0.47868766\n",
            "Iteration 180, loss = 0.47843573\n",
            "Iteration 181, loss = 0.47837252\n",
            "Iteration 182, loss = 0.47827888\n",
            "Iteration 183, loss = 0.47809854\n",
            "Iteration 184, loss = 0.47795956\n",
            "Iteration 185, loss = 0.47785547\n",
            "Iteration 186, loss = 0.47775564\n",
            "Iteration 187, loss = 0.47767462\n",
            "Iteration 188, loss = 0.47754269\n",
            "Iteration 189, loss = 0.47740448\n",
            "Iteration 190, loss = 0.47729077\n",
            "Iteration 191, loss = 0.47724821\n",
            "Iteration 192, loss = 0.47705309\n",
            "Iteration 193, loss = 0.47697785\n",
            "Iteration 194, loss = 0.47686351\n",
            "Iteration 195, loss = 0.47683391\n",
            "Iteration 196, loss = 0.47668143\n",
            "Iteration 197, loss = 0.47658298\n",
            "Iteration 198, loss = 0.47648744\n",
            "Iteration 199, loss = 0.47643688\n",
            "Iteration 200, loss = 0.47625172\n",
            "Iteration 1, loss = 0.67415094\n",
            "Iteration 2, loss = 0.66088940\n",
            "Iteration 3, loss = 0.64812333\n",
            "Iteration 4, loss = 0.63620413\n",
            "Iteration 5, loss = 0.62567590\n",
            "Iteration 6, loss = 0.61589615\n",
            "Iteration 7, loss = 0.60686063\n",
            "Iteration 8, loss = 0.59836192\n",
            "Iteration 9, loss = 0.59045131\n",
            "Iteration 10, loss = 0.58325737\n",
            "Iteration 11, loss = 0.57680535\n",
            "Iteration 12, loss = 0.57112028\n",
            "Iteration 13, loss = 0.56601642\n",
            "Iteration 14, loss = 0.56129230\n",
            "Iteration 15, loss = 0.55696599\n",
            "Iteration 16, loss = 0.55290864\n",
            "Iteration 17, loss = 0.54915550\n",
            "Iteration 18, loss = 0.54561374\n",
            "Iteration 19, loss = 0.54226115\n",
            "Iteration 20, loss = 0.53913850\n",
            "Iteration 21, loss = 0.53622723\n",
            "Iteration 22, loss = 0.53342719\n",
            "Iteration 23, loss = 0.53085523\n",
            "Iteration 24, loss = 0.52838262\n",
            "Iteration 25, loss = 0.52598531\n",
            "Iteration 26, loss = 0.52373414\n",
            "Iteration 27, loss = 0.52164477\n",
            "Iteration 28, loss = 0.51960402\n",
            "Iteration 29, loss = 0.51773245\n",
            "Iteration 30, loss = 0.51585809\n",
            "Iteration 31, loss = 0.51409652\n",
            "Iteration 32, loss = 0.51243035\n",
            "Iteration 33, loss = 0.51092005\n",
            "Iteration 34, loss = 0.50932306\n",
            "Iteration 35, loss = 0.50785693\n",
            "Iteration 36, loss = 0.50634365\n",
            "Iteration 37, loss = 0.50489773\n",
            "Iteration 38, loss = 0.50351921\n",
            "Iteration 39, loss = 0.50225309\n",
            "Iteration 40, loss = 0.50107880\n",
            "Iteration 41, loss = 0.49990449\n",
            "Iteration 42, loss = 0.49875729\n",
            "Iteration 43, loss = 0.49767858\n",
            "Iteration 44, loss = 0.49664896\n",
            "Iteration 45, loss = 0.49571357\n",
            "Iteration 46, loss = 0.49476341\n",
            "Iteration 47, loss = 0.49380619\n",
            "Iteration 48, loss = 0.49291479\n",
            "Iteration 49, loss = 0.49205108\n",
            "Iteration 50, loss = 0.49127119\n",
            "Iteration 51, loss = 0.49043638\n",
            "Iteration 52, loss = 0.48968301\n",
            "Iteration 53, loss = 0.48904401\n",
            "Iteration 54, loss = 0.48840421\n",
            "Iteration 55, loss = 0.48764859\n",
            "Iteration 56, loss = 0.48714319\n",
            "Iteration 57, loss = 0.48647373\n",
            "Iteration 58, loss = 0.48589212\n",
            "Iteration 59, loss = 0.48528685\n",
            "Iteration 60, loss = 0.48476338\n",
            "Iteration 61, loss = 0.48423711\n",
            "Iteration 62, loss = 0.48369358\n",
            "Iteration 63, loss = 0.48316620\n",
            "Iteration 64, loss = 0.48268801\n",
            "Iteration 65, loss = 0.48226094\n",
            "Iteration 66, loss = 0.48179458\n",
            "Iteration 67, loss = 0.48136858\n",
            "Iteration 68, loss = 0.48092005\n",
            "Iteration 69, loss = 0.48059150\n",
            "Iteration 70, loss = 0.48015844\n",
            "Iteration 71, loss = 0.47975535\n",
            "Iteration 72, loss = 0.47941500\n",
            "Iteration 73, loss = 0.47902281\n",
            "Iteration 74, loss = 0.47871437\n",
            "Iteration 75, loss = 0.47833109\n",
            "Iteration 76, loss = 0.47806256\n",
            "Iteration 77, loss = 0.47763170\n",
            "Iteration 78, loss = 0.47732151\n",
            "Iteration 79, loss = 0.47697941\n",
            "Iteration 80, loss = 0.47671451\n",
            "Iteration 81, loss = 0.47640537\n",
            "Iteration 82, loss = 0.47630508\n",
            "Iteration 83, loss = 0.47588510\n",
            "Iteration 84, loss = 0.47556579\n",
            "Iteration 85, loss = 0.47527726\n",
            "Iteration 86, loss = 0.47499692\n",
            "Iteration 87, loss = 0.47480896\n",
            "Iteration 88, loss = 0.47457265\n",
            "Iteration 89, loss = 0.47420765\n",
            "Iteration 90, loss = 0.47402315\n",
            "Iteration 91, loss = 0.47374408\n",
            "Iteration 92, loss = 0.47352613\n",
            "Iteration 93, loss = 0.47328775\n",
            "Iteration 94, loss = 0.47304761\n",
            "Iteration 95, loss = 0.47283447\n",
            "Iteration 96, loss = 0.47262846\n",
            "Iteration 97, loss = 0.47241032\n",
            "Iteration 98, loss = 0.47217451\n",
            "Iteration 99, loss = 0.47190058\n",
            "Iteration 100, loss = 0.47180383\n",
            "Iteration 101, loss = 0.47152612\n",
            "Iteration 102, loss = 0.47133714\n",
            "Iteration 103, loss = 0.47120274\n",
            "Iteration 104, loss = 0.47091000\n",
            "Iteration 105, loss = 0.47070994\n",
            "Iteration 106, loss = 0.47055727\n",
            "Iteration 107, loss = 0.47041974\n",
            "Iteration 108, loss = 0.47018663\n",
            "Iteration 109, loss = 0.46998883\n",
            "Iteration 110, loss = 0.46988048\n",
            "Iteration 111, loss = 0.46977757\n",
            "Iteration 112, loss = 0.46950443\n",
            "Iteration 113, loss = 0.46931306\n",
            "Iteration 114, loss = 0.46920640\n",
            "Iteration 115, loss = 0.46902092\n",
            "Iteration 116, loss = 0.46906010\n",
            "Iteration 117, loss = 0.46870115\n",
            "Iteration 118, loss = 0.46853016\n",
            "Iteration 119, loss = 0.46841460\n",
            "Iteration 120, loss = 0.46819445\n",
            "Iteration 121, loss = 0.46802399\n",
            "Iteration 122, loss = 0.46791765\n",
            "Iteration 123, loss = 0.46774005\n",
            "Iteration 124, loss = 0.46767583\n",
            "Iteration 125, loss = 0.46750836\n",
            "Iteration 126, loss = 0.46741599\n",
            "Iteration 127, loss = 0.46727666\n",
            "Iteration 128, loss = 0.46711179\n",
            "Iteration 129, loss = 0.46695352\n",
            "Iteration 130, loss = 0.46679118\n",
            "Iteration 131, loss = 0.46671495\n",
            "Iteration 132, loss = 0.46662021\n",
            "Iteration 133, loss = 0.46640887\n",
            "Iteration 134, loss = 0.46634247\n",
            "Iteration 135, loss = 0.46634919\n",
            "Iteration 136, loss = 0.46610793\n",
            "Iteration 137, loss = 0.46600061\n",
            "Iteration 138, loss = 0.46583961\n",
            "Iteration 139, loss = 0.46571223\n",
            "Iteration 140, loss = 0.46563108\n",
            "Iteration 141, loss = 0.46549568\n",
            "Iteration 142, loss = 0.46542412\n",
            "Iteration 143, loss = 0.46532903\n",
            "Iteration 144, loss = 0.46519436\n",
            "Iteration 145, loss = 0.46509119\n",
            "Iteration 146, loss = 0.46497032\n",
            "Iteration 147, loss = 0.46492363\n",
            "Iteration 148, loss = 0.46478301\n",
            "Iteration 149, loss = 0.46464917\n",
            "Iteration 150, loss = 0.46451597\n",
            "Iteration 151, loss = 0.46443641\n",
            "Iteration 152, loss = 0.46438816\n",
            "Iteration 153, loss = 0.46424406\n",
            "Iteration 154, loss = 0.46418547\n",
            "Iteration 155, loss = 0.46409174\n",
            "Iteration 156, loss = 0.46408555\n",
            "Iteration 157, loss = 0.46396871\n",
            "Iteration 158, loss = 0.46371356\n",
            "Iteration 159, loss = 0.46368806\n",
            "Iteration 160, loss = 0.46357678\n",
            "Iteration 161, loss = 0.46346657\n",
            "Iteration 162, loss = 0.46337277\n",
            "Iteration 163, loss = 0.46338385\n",
            "Iteration 164, loss = 0.46320799\n",
            "Iteration 165, loss = 0.46312580\n",
            "Iteration 166, loss = 0.46303248\n",
            "Iteration 167, loss = 0.46299825\n",
            "Iteration 168, loss = 0.46285803\n",
            "Iteration 169, loss = 0.46279570\n",
            "Iteration 170, loss = 0.46267940\n",
            "Iteration 171, loss = 0.46256694\n",
            "Iteration 172, loss = 0.46255275\n",
            "Iteration 173, loss = 0.46248105\n",
            "Iteration 174, loss = 0.46232121\n",
            "Iteration 175, loss = 0.46231476\n",
            "Iteration 176, loss = 0.46217169\n",
            "Iteration 177, loss = 0.46214104\n",
            "Iteration 178, loss = 0.46209308\n",
            "Iteration 179, loss = 0.46201782\n",
            "Iteration 180, loss = 0.46187548\n",
            "Iteration 181, loss = 0.46179570\n",
            "Iteration 182, loss = 0.46178884\n",
            "Iteration 183, loss = 0.46163946\n",
            "Iteration 184, loss = 0.46163213\n",
            "Iteration 185, loss = 0.46150401\n",
            "Iteration 186, loss = 0.46151144\n",
            "Iteration 187, loss = 0.46131573\n",
            "Iteration 188, loss = 0.46128495\n",
            "Iteration 189, loss = 0.46126605\n",
            "Iteration 190, loss = 0.46118402\n",
            "Iteration 191, loss = 0.46108243\n",
            "Iteration 192, loss = 0.46106016\n",
            "Iteration 193, loss = 0.46107617\n",
            "Iteration 194, loss = 0.46087722\n",
            "Iteration 195, loss = 0.46084530\n",
            "Iteration 196, loss = 0.46093442\n",
            "Iteration 197, loss = 0.46067543\n",
            "Iteration 198, loss = 0.46059482\n",
            "Iteration 199, loss = 0.46058336\n",
            "Iteration 200, loss = 0.46052932\n",
            "Iteration 1, loss = 0.67358612\n",
            "Iteration 2, loss = 0.65987038\n",
            "Iteration 3, loss = 0.64657158\n",
            "Iteration 4, loss = 0.63444496\n",
            "Iteration 5, loss = 0.62380790\n",
            "Iteration 6, loss = 0.61397934\n",
            "Iteration 7, loss = 0.60501431\n",
            "Iteration 8, loss = 0.59652623\n",
            "Iteration 9, loss = 0.58870639\n",
            "Iteration 10, loss = 0.58161459\n",
            "Iteration 11, loss = 0.57526101\n",
            "Iteration 12, loss = 0.56955911\n",
            "Iteration 13, loss = 0.56445604\n",
            "Iteration 14, loss = 0.55974899\n",
            "Iteration 15, loss = 0.55544896\n",
            "Iteration 16, loss = 0.55141192\n",
            "Iteration 17, loss = 0.54768674\n",
            "Iteration 18, loss = 0.54419641\n",
            "Iteration 19, loss = 0.54085874\n",
            "Iteration 20, loss = 0.53779063\n",
            "Iteration 21, loss = 0.53498310\n",
            "Iteration 22, loss = 0.53217836\n",
            "Iteration 23, loss = 0.52957310\n",
            "Iteration 24, loss = 0.52718058\n",
            "Iteration 25, loss = 0.52485230\n",
            "Iteration 26, loss = 0.52265390\n",
            "Iteration 27, loss = 0.52065464\n",
            "Iteration 28, loss = 0.51865760\n",
            "Iteration 29, loss = 0.51686690\n",
            "Iteration 30, loss = 0.51503715\n",
            "Iteration 31, loss = 0.51334652\n",
            "Iteration 32, loss = 0.51172682\n",
            "Iteration 33, loss = 0.51024516\n",
            "Iteration 34, loss = 0.50872961\n",
            "Iteration 35, loss = 0.50732834\n",
            "Iteration 36, loss = 0.50589639\n",
            "Iteration 37, loss = 0.50452428\n",
            "Iteration 38, loss = 0.50323136\n",
            "Iteration 39, loss = 0.50199599\n",
            "Iteration 40, loss = 0.50091320\n",
            "Iteration 41, loss = 0.49971047\n",
            "Iteration 42, loss = 0.49859351\n",
            "Iteration 43, loss = 0.49754924\n",
            "Iteration 44, loss = 0.49652798\n",
            "Iteration 45, loss = 0.49562731\n",
            "Iteration 46, loss = 0.49472380\n",
            "Iteration 47, loss = 0.49381577\n",
            "Iteration 48, loss = 0.49297623\n",
            "Iteration 49, loss = 0.49216856\n",
            "Iteration 50, loss = 0.49139431\n",
            "Iteration 51, loss = 0.49066142\n",
            "Iteration 52, loss = 0.48994535\n",
            "Iteration 53, loss = 0.48928420\n",
            "Iteration 54, loss = 0.48865515\n",
            "Iteration 55, loss = 0.48794848\n",
            "Iteration 56, loss = 0.48743045\n",
            "Iteration 57, loss = 0.48672786\n",
            "Iteration 58, loss = 0.48612650\n",
            "Iteration 59, loss = 0.48554186\n",
            "Iteration 60, loss = 0.48503901\n",
            "Iteration 61, loss = 0.48450971\n",
            "Iteration 62, loss = 0.48394166\n",
            "Iteration 63, loss = 0.48348309\n",
            "Iteration 64, loss = 0.48296607\n",
            "Iteration 65, loss = 0.48253749\n",
            "Iteration 66, loss = 0.48205742\n",
            "Iteration 67, loss = 0.48160411\n",
            "Iteration 68, loss = 0.48120743\n",
            "Iteration 69, loss = 0.48076044\n",
            "Iteration 70, loss = 0.48038731\n",
            "Iteration 71, loss = 0.47999649\n",
            "Iteration 72, loss = 0.47964130\n",
            "Iteration 73, loss = 0.47926897\n",
            "Iteration 74, loss = 0.47897821\n",
            "Iteration 75, loss = 0.47856319\n",
            "Iteration 76, loss = 0.47832608\n",
            "Iteration 77, loss = 0.47789003\n",
            "Iteration 78, loss = 0.47758745\n",
            "Iteration 79, loss = 0.47726346\n",
            "Iteration 80, loss = 0.47698950\n",
            "Iteration 81, loss = 0.47668694\n",
            "Iteration 82, loss = 0.47651838\n",
            "Iteration 83, loss = 0.47619259\n",
            "Iteration 84, loss = 0.47592489\n",
            "Iteration 85, loss = 0.47561534\n",
            "Iteration 86, loss = 0.47532475\n",
            "Iteration 87, loss = 0.47515800\n",
            "Iteration 88, loss = 0.47496762\n",
            "Iteration 89, loss = 0.47458581\n",
            "Iteration 90, loss = 0.47438739\n",
            "Iteration 91, loss = 0.47412953\n",
            "Iteration 92, loss = 0.47389828\n",
            "Iteration 93, loss = 0.47363047\n",
            "Iteration 94, loss = 0.47345650\n",
            "Iteration 95, loss = 0.47321622\n",
            "Iteration 96, loss = 0.47305448\n",
            "Iteration 97, loss = 0.47284755\n",
            "Iteration 98, loss = 0.47261509\n",
            "Iteration 99, loss = 0.47238310\n",
            "Iteration 100, loss = 0.47229631\n",
            "Iteration 101, loss = 0.47208092\n",
            "Iteration 102, loss = 0.47184780\n",
            "Iteration 103, loss = 0.47167596\n",
            "Iteration 104, loss = 0.47146326\n",
            "Iteration 105, loss = 0.47134868\n",
            "Iteration 106, loss = 0.47112036\n",
            "Iteration 107, loss = 0.47098538\n",
            "Iteration 108, loss = 0.47076572\n",
            "Iteration 109, loss = 0.47059209\n",
            "Iteration 110, loss = 0.47045841\n",
            "Iteration 111, loss = 0.47032039\n",
            "Iteration 112, loss = 0.47017725\n",
            "Iteration 113, loss = 0.46994836\n",
            "Iteration 114, loss = 0.46985388\n",
            "Iteration 115, loss = 0.46964837\n",
            "Iteration 116, loss = 0.46963002\n",
            "Iteration 117, loss = 0.46932260\n",
            "Iteration 118, loss = 0.46915174\n",
            "Iteration 119, loss = 0.46900387\n",
            "Iteration 120, loss = 0.46884674\n",
            "Iteration 121, loss = 0.46868641\n",
            "Iteration 122, loss = 0.46854667\n",
            "Iteration 123, loss = 0.46838065\n",
            "Iteration 124, loss = 0.46834253\n",
            "Iteration 125, loss = 0.46812827\n",
            "Iteration 126, loss = 0.46801314\n",
            "Iteration 127, loss = 0.46786733\n",
            "Iteration 128, loss = 0.46771263\n",
            "Iteration 129, loss = 0.46754397\n",
            "Iteration 130, loss = 0.46740535\n",
            "Iteration 131, loss = 0.46736610\n",
            "Iteration 132, loss = 0.46716021\n",
            "Iteration 133, loss = 0.46697923\n",
            "Iteration 134, loss = 0.46697397\n",
            "Iteration 135, loss = 0.46682836\n",
            "Iteration 136, loss = 0.46663799\n",
            "Iteration 137, loss = 0.46653773\n",
            "Iteration 138, loss = 0.46637296\n",
            "Iteration 139, loss = 0.46625061\n",
            "Iteration 140, loss = 0.46619173\n",
            "Iteration 141, loss = 0.46606813\n",
            "Iteration 142, loss = 0.46591448\n",
            "Iteration 143, loss = 0.46582863\n",
            "Iteration 144, loss = 0.46565924\n",
            "Iteration 145, loss = 0.46557409\n",
            "Iteration 146, loss = 0.46541904\n",
            "Iteration 147, loss = 0.46534904\n",
            "Iteration 148, loss = 0.46517911\n",
            "Iteration 149, loss = 0.46504516\n",
            "Iteration 150, loss = 0.46492568\n",
            "Iteration 151, loss = 0.46487479\n",
            "Iteration 152, loss = 0.46469997\n",
            "Iteration 153, loss = 0.46460563\n",
            "Iteration 154, loss = 0.46450892\n",
            "Iteration 155, loss = 0.46440677\n",
            "Iteration 156, loss = 0.46439975\n",
            "Iteration 157, loss = 0.46424896\n",
            "Iteration 158, loss = 0.46405324\n",
            "Iteration 159, loss = 0.46398650\n",
            "Iteration 160, loss = 0.46389780\n",
            "Iteration 161, loss = 0.46376498\n",
            "Iteration 162, loss = 0.46364651\n",
            "Iteration 163, loss = 0.46364097\n",
            "Iteration 164, loss = 0.46349173\n",
            "Iteration 165, loss = 0.46343639\n",
            "Iteration 166, loss = 0.46333739\n",
            "Iteration 167, loss = 0.46324371\n",
            "Iteration 168, loss = 0.46313241\n",
            "Iteration 169, loss = 0.46311620\n",
            "Iteration 170, loss = 0.46296557\n",
            "Iteration 171, loss = 0.46285875\n",
            "Iteration 172, loss = 0.46285755\n",
            "Iteration 173, loss = 0.46270344\n",
            "Iteration 174, loss = 0.46258410\n",
            "Iteration 175, loss = 0.46258281\n",
            "Iteration 176, loss = 0.46242635\n",
            "Iteration 177, loss = 0.46238668\n",
            "Iteration 178, loss = 0.46235775\n",
            "Iteration 179, loss = 0.46235873\n",
            "Iteration 180, loss = 0.46209893\n",
            "Iteration 181, loss = 0.46201255\n",
            "Iteration 182, loss = 0.46205747\n",
            "Iteration 183, loss = 0.46186141\n",
            "Iteration 184, loss = 0.46183392\n",
            "Iteration 185, loss = 0.46170524\n",
            "Iteration 186, loss = 0.46168287\n",
            "Iteration 187, loss = 0.46154027\n",
            "Iteration 188, loss = 0.46151145\n",
            "Iteration 189, loss = 0.46148055\n",
            "Iteration 190, loss = 0.46135001\n",
            "Iteration 191, loss = 0.46128377\n",
            "Iteration 192, loss = 0.46120683\n",
            "Iteration 193, loss = 0.46113608\n",
            "Iteration 194, loss = 0.46106587\n",
            "Iteration 195, loss = 0.46099329\n",
            "Iteration 196, loss = 0.46101519\n",
            "Iteration 197, loss = 0.46083298\n",
            "Iteration 198, loss = 0.46074222\n",
            "Iteration 199, loss = 0.46073724\n",
            "Iteration 200, loss = 0.46062510\n",
            "Iteration 1, loss = 0.67400753\n",
            "Iteration 2, loss = 0.66090986\n",
            "Iteration 3, loss = 0.64823519\n",
            "Iteration 4, loss = 0.63641794\n",
            "Iteration 5, loss = 0.62602757\n",
            "Iteration 6, loss = 0.61633488\n",
            "Iteration 7, loss = 0.60751479\n",
            "Iteration 8, loss = 0.59913356\n",
            "Iteration 9, loss = 0.59143684\n",
            "Iteration 10, loss = 0.58445621\n",
            "Iteration 11, loss = 0.57807706\n",
            "Iteration 12, loss = 0.57243262\n",
            "Iteration 13, loss = 0.56735692\n",
            "Iteration 14, loss = 0.56271574\n",
            "Iteration 15, loss = 0.55843288\n",
            "Iteration 16, loss = 0.55442796\n",
            "Iteration 17, loss = 0.55070223\n",
            "Iteration 18, loss = 0.54723296\n",
            "Iteration 19, loss = 0.54391749\n",
            "Iteration 20, loss = 0.54090196\n",
            "Iteration 21, loss = 0.53803922\n",
            "Iteration 22, loss = 0.53527821\n",
            "Iteration 23, loss = 0.53271877\n",
            "Iteration 24, loss = 0.53031167\n",
            "Iteration 25, loss = 0.52802260\n",
            "Iteration 26, loss = 0.52577887\n",
            "Iteration 27, loss = 0.52379201\n",
            "Iteration 28, loss = 0.52169147\n",
            "Iteration 29, loss = 0.51989570\n",
            "Iteration 30, loss = 0.51801498\n",
            "Iteration 31, loss = 0.51626555\n",
            "Iteration 32, loss = 0.51459277\n",
            "Iteration 33, loss = 0.51308026\n",
            "Iteration 34, loss = 0.51151960\n",
            "Iteration 35, loss = 0.51007755\n",
            "Iteration 36, loss = 0.50858535\n",
            "Iteration 37, loss = 0.50709038\n",
            "Iteration 38, loss = 0.50575634\n",
            "Iteration 39, loss = 0.50443413\n",
            "Iteration 40, loss = 0.50329405\n",
            "Iteration 41, loss = 0.50204193\n",
            "Iteration 42, loss = 0.50087723\n",
            "Iteration 43, loss = 0.49982289\n",
            "Iteration 44, loss = 0.49876598\n",
            "Iteration 45, loss = 0.49781587\n",
            "Iteration 46, loss = 0.49691861\n",
            "Iteration 47, loss = 0.49599765\n",
            "Iteration 48, loss = 0.49514190\n",
            "Iteration 49, loss = 0.49426873\n",
            "Iteration 50, loss = 0.49344470\n",
            "Iteration 51, loss = 0.49266577\n",
            "Iteration 52, loss = 0.49191107\n",
            "Iteration 53, loss = 0.49128315\n",
            "Iteration 54, loss = 0.49056367\n",
            "Iteration 55, loss = 0.48985384\n",
            "Iteration 56, loss = 0.48932892\n",
            "Iteration 57, loss = 0.48862041\n",
            "Iteration 58, loss = 0.48803132\n",
            "Iteration 59, loss = 0.48747463\n",
            "Iteration 60, loss = 0.48696999\n",
            "Iteration 61, loss = 0.48647060\n",
            "Iteration 62, loss = 0.48586860\n",
            "Iteration 63, loss = 0.48540532\n",
            "Iteration 64, loss = 0.48491251\n",
            "Iteration 65, loss = 0.48452887\n",
            "Iteration 66, loss = 0.48399442\n",
            "Iteration 67, loss = 0.48353227\n",
            "Iteration 68, loss = 0.48316855\n",
            "Iteration 69, loss = 0.48268809\n",
            "Iteration 70, loss = 0.48228220\n",
            "Iteration 71, loss = 0.48189142\n",
            "Iteration 72, loss = 0.48153115\n",
            "Iteration 73, loss = 0.48115960\n",
            "Iteration 74, loss = 0.48087271\n",
            "Iteration 75, loss = 0.48041230\n",
            "Iteration 76, loss = 0.48013256\n",
            "Iteration 77, loss = 0.47971848\n",
            "Iteration 78, loss = 0.47941679\n",
            "Iteration 79, loss = 0.47908289\n",
            "Iteration 80, loss = 0.47877380\n",
            "Iteration 81, loss = 0.47847593\n",
            "Iteration 82, loss = 0.47836152\n",
            "Iteration 83, loss = 0.47790790\n",
            "Iteration 84, loss = 0.47762079\n",
            "Iteration 85, loss = 0.47732445\n",
            "Iteration 86, loss = 0.47704380\n",
            "Iteration 87, loss = 0.47684837\n",
            "Iteration 88, loss = 0.47666204\n",
            "Iteration 89, loss = 0.47626305\n",
            "Iteration 90, loss = 0.47598525\n",
            "Iteration 91, loss = 0.47570781\n",
            "Iteration 92, loss = 0.47548266\n",
            "Iteration 93, loss = 0.47525846\n",
            "Iteration 94, loss = 0.47501209\n",
            "Iteration 95, loss = 0.47473137\n",
            "Iteration 96, loss = 0.47460251\n",
            "Iteration 97, loss = 0.47433046\n",
            "Iteration 98, loss = 0.47411530\n",
            "Iteration 99, loss = 0.47387768\n",
            "Iteration 100, loss = 0.47367478\n",
            "Iteration 101, loss = 0.47343385\n",
            "Iteration 102, loss = 0.47323984\n",
            "Iteration 103, loss = 0.47304715\n",
            "Iteration 104, loss = 0.47283052\n",
            "Iteration 105, loss = 0.47261920\n",
            "Iteration 106, loss = 0.47244978\n",
            "Iteration 107, loss = 0.47224097\n",
            "Iteration 108, loss = 0.47199663\n",
            "Iteration 109, loss = 0.47185570\n",
            "Iteration 110, loss = 0.47168740\n",
            "Iteration 111, loss = 0.47160845\n",
            "Iteration 112, loss = 0.47140263\n",
            "Iteration 113, loss = 0.47110708\n",
            "Iteration 114, loss = 0.47096365\n",
            "Iteration 115, loss = 0.47084206\n",
            "Iteration 116, loss = 0.47069471\n",
            "Iteration 117, loss = 0.47047044\n",
            "Iteration 118, loss = 0.47032601\n",
            "Iteration 119, loss = 0.47019628\n",
            "Iteration 120, loss = 0.47008466\n",
            "Iteration 121, loss = 0.46987672\n",
            "Iteration 122, loss = 0.46976543\n",
            "Iteration 123, loss = 0.46956714\n",
            "Iteration 124, loss = 0.46956481\n",
            "Iteration 125, loss = 0.46930110\n",
            "Iteration 126, loss = 0.46919432\n",
            "Iteration 127, loss = 0.46908667\n",
            "Iteration 128, loss = 0.46896397\n",
            "Iteration 129, loss = 0.46878151\n",
            "Iteration 130, loss = 0.46868056\n",
            "Iteration 131, loss = 0.46860390\n",
            "Iteration 132, loss = 0.46842495\n",
            "Iteration 133, loss = 0.46824898\n",
            "Iteration 134, loss = 0.46833097\n",
            "Iteration 135, loss = 0.46810631\n",
            "Iteration 136, loss = 0.46790554\n",
            "Iteration 137, loss = 0.46785463\n",
            "Iteration 138, loss = 0.46770391\n",
            "Iteration 139, loss = 0.46751892\n",
            "Iteration 140, loss = 0.46749947\n",
            "Iteration 141, loss = 0.46745449\n",
            "Iteration 142, loss = 0.46728225\n",
            "Iteration 143, loss = 0.46717251\n",
            "Iteration 144, loss = 0.46702944\n",
            "Iteration 145, loss = 0.46690485\n",
            "Iteration 146, loss = 0.46679379\n",
            "Iteration 147, loss = 0.46664356\n",
            "Iteration 148, loss = 0.46660220\n",
            "Iteration 149, loss = 0.46648991\n",
            "Iteration 150, loss = 0.46639480\n",
            "Iteration 151, loss = 0.46635873\n",
            "Iteration 152, loss = 0.46616222\n",
            "Iteration 153, loss = 0.46602591\n",
            "Iteration 154, loss = 0.46592786\n",
            "Iteration 155, loss = 0.46580187\n",
            "Iteration 156, loss = 0.46590233\n",
            "Iteration 157, loss = 0.46566984\n",
            "Iteration 158, loss = 0.46552389\n",
            "Iteration 159, loss = 0.46545387\n",
            "Iteration 160, loss = 0.46534589\n",
            "Iteration 161, loss = 0.46525266\n",
            "Iteration 162, loss = 0.46513447\n",
            "Iteration 163, loss = 0.46519823\n",
            "Iteration 164, loss = 0.46495853\n",
            "Iteration 165, loss = 0.46495543\n",
            "Iteration 166, loss = 0.46481683\n",
            "Iteration 167, loss = 0.46475424\n",
            "Iteration 168, loss = 0.46460736\n",
            "Iteration 169, loss = 0.46458288\n",
            "Iteration 170, loss = 0.46450975\n",
            "Iteration 171, loss = 0.46440889\n",
            "Iteration 172, loss = 0.46448169\n",
            "Iteration 173, loss = 0.46423540\n",
            "Iteration 174, loss = 0.46409412\n",
            "Iteration 175, loss = 0.46406192\n",
            "Iteration 176, loss = 0.46394759\n",
            "Iteration 177, loss = 0.46391180\n",
            "Iteration 178, loss = 0.46385277\n",
            "Iteration 179, loss = 0.46384091\n",
            "Iteration 180, loss = 0.46364142\n",
            "Iteration 181, loss = 0.46353893\n",
            "Iteration 182, loss = 0.46364954\n",
            "Iteration 183, loss = 0.46344270\n",
            "Iteration 184, loss = 0.46332973\n",
            "Iteration 185, loss = 0.46324295\n",
            "Iteration 186, loss = 0.46324703\n",
            "Iteration 187, loss = 0.46309187\n",
            "Iteration 188, loss = 0.46306769\n",
            "Iteration 189, loss = 0.46301170\n",
            "Iteration 190, loss = 0.46286955\n",
            "Iteration 191, loss = 0.46285931\n",
            "Iteration 192, loss = 0.46274733\n",
            "Iteration 193, loss = 0.46259424\n",
            "Iteration 194, loss = 0.46263204\n",
            "Iteration 195, loss = 0.46254521\n",
            "Iteration 196, loss = 0.46255148\n",
            "Iteration 197, loss = 0.46238979\n",
            "Iteration 198, loss = 0.46229764\n",
            "Iteration 199, loss = 0.46226481\n",
            "Iteration 200, loss = 0.46216397\n",
            "Iteration 1, loss = 0.67397935\n",
            "Iteration 2, loss = 0.66083473\n",
            "Iteration 3, loss = 0.64809173\n",
            "Iteration 4, loss = 0.63630432\n",
            "Iteration 5, loss = 0.62587383\n",
            "Iteration 6, loss = 0.61622138\n",
            "Iteration 7, loss = 0.60734787\n",
            "Iteration 8, loss = 0.59891692\n",
            "Iteration 9, loss = 0.59113178\n",
            "Iteration 10, loss = 0.58404279\n",
            "Iteration 11, loss = 0.57756516\n",
            "Iteration 12, loss = 0.57176480\n",
            "Iteration 13, loss = 0.56659742\n",
            "Iteration 14, loss = 0.56186934\n",
            "Iteration 15, loss = 0.55742898\n",
            "Iteration 16, loss = 0.55339671\n",
            "Iteration 17, loss = 0.54959993\n",
            "Iteration 18, loss = 0.54610219\n",
            "Iteration 19, loss = 0.54274427\n",
            "Iteration 20, loss = 0.53975115\n",
            "Iteration 21, loss = 0.53683650\n",
            "Iteration 22, loss = 0.53407838\n",
            "Iteration 23, loss = 0.53152614\n",
            "Iteration 24, loss = 0.52909089\n",
            "Iteration 25, loss = 0.52682915\n",
            "Iteration 26, loss = 0.52457928\n",
            "Iteration 27, loss = 0.52255512\n",
            "Iteration 28, loss = 0.52048537\n",
            "Iteration 29, loss = 0.51866496\n",
            "Iteration 30, loss = 0.51680931\n",
            "Iteration 31, loss = 0.51509381\n",
            "Iteration 32, loss = 0.51345549\n",
            "Iteration 33, loss = 0.51196986\n",
            "Iteration 34, loss = 0.51047853\n",
            "Iteration 35, loss = 0.50898825\n",
            "Iteration 36, loss = 0.50751821\n",
            "Iteration 37, loss = 0.50608712\n",
            "Iteration 38, loss = 0.50481352\n",
            "Iteration 39, loss = 0.50351366\n",
            "Iteration 40, loss = 0.50239860\n",
            "Iteration 41, loss = 0.50125870\n",
            "Iteration 42, loss = 0.50013634\n",
            "Iteration 43, loss = 0.49911570\n",
            "Iteration 44, loss = 0.49808530\n",
            "Iteration 45, loss = 0.49713562\n",
            "Iteration 46, loss = 0.49625973\n",
            "Iteration 47, loss = 0.49534945\n",
            "Iteration 48, loss = 0.49449664\n",
            "Iteration 49, loss = 0.49364631\n",
            "Iteration 50, loss = 0.49284360\n",
            "Iteration 51, loss = 0.49205770\n",
            "Iteration 52, loss = 0.49130134\n",
            "Iteration 53, loss = 0.49063301\n",
            "Iteration 54, loss = 0.48997030\n",
            "Iteration 55, loss = 0.48924734\n",
            "Iteration 56, loss = 0.48875379\n",
            "Iteration 57, loss = 0.48804043\n",
            "Iteration 58, loss = 0.48748208\n",
            "Iteration 59, loss = 0.48690758\n",
            "Iteration 60, loss = 0.48643164\n",
            "Iteration 61, loss = 0.48593954\n",
            "Iteration 62, loss = 0.48531081\n",
            "Iteration 63, loss = 0.48484847\n",
            "Iteration 64, loss = 0.48440073\n",
            "Iteration 65, loss = 0.48400364\n",
            "Iteration 66, loss = 0.48347095\n",
            "Iteration 67, loss = 0.48300474\n",
            "Iteration 68, loss = 0.48263670\n",
            "Iteration 69, loss = 0.48217282\n",
            "Iteration 70, loss = 0.48175441\n",
            "Iteration 71, loss = 0.48137623\n",
            "Iteration 72, loss = 0.48101706\n",
            "Iteration 73, loss = 0.48066657\n",
            "Iteration 74, loss = 0.48038953\n",
            "Iteration 75, loss = 0.47993673\n",
            "Iteration 76, loss = 0.47964541\n",
            "Iteration 77, loss = 0.47925905\n",
            "Iteration 78, loss = 0.47895438\n",
            "Iteration 79, loss = 0.47865634\n",
            "Iteration 80, loss = 0.47836971\n",
            "Iteration 81, loss = 0.47804562\n",
            "Iteration 82, loss = 0.47789235\n",
            "Iteration 83, loss = 0.47751713\n",
            "Iteration 84, loss = 0.47723848\n",
            "Iteration 85, loss = 0.47698345\n",
            "Iteration 86, loss = 0.47672890\n",
            "Iteration 87, loss = 0.47652969\n",
            "Iteration 88, loss = 0.47627844\n",
            "Iteration 89, loss = 0.47597346\n",
            "Iteration 90, loss = 0.47572895\n",
            "Iteration 91, loss = 0.47547848\n",
            "Iteration 92, loss = 0.47531798\n",
            "Iteration 93, loss = 0.47509849\n",
            "Iteration 94, loss = 0.47482422\n",
            "Iteration 95, loss = 0.47459601\n",
            "Iteration 96, loss = 0.47448244\n",
            "Iteration 97, loss = 0.47422863\n",
            "Iteration 98, loss = 0.47404491\n",
            "Iteration 99, loss = 0.47379849\n",
            "Iteration 100, loss = 0.47363424\n",
            "Iteration 101, loss = 0.47342024\n",
            "Iteration 102, loss = 0.47320686\n",
            "Iteration 103, loss = 0.47304840\n",
            "Iteration 104, loss = 0.47291764\n",
            "Iteration 105, loss = 0.47264739\n",
            "Iteration 106, loss = 0.47247880\n",
            "Iteration 107, loss = 0.47231140\n",
            "Iteration 108, loss = 0.47206288\n",
            "Iteration 109, loss = 0.47197094\n",
            "Iteration 110, loss = 0.47180883\n",
            "Iteration 111, loss = 0.47166671\n",
            "Iteration 112, loss = 0.47144015\n",
            "Iteration 113, loss = 0.47122816\n",
            "Iteration 114, loss = 0.47113885\n",
            "Iteration 115, loss = 0.47093947\n",
            "Iteration 116, loss = 0.47084034\n",
            "Iteration 117, loss = 0.47061954\n",
            "Iteration 118, loss = 0.47050168\n",
            "Iteration 119, loss = 0.47037939\n",
            "Iteration 120, loss = 0.47024618\n",
            "Iteration 121, loss = 0.47009971\n",
            "Iteration 122, loss = 0.46997443\n",
            "Iteration 123, loss = 0.46975954\n",
            "Iteration 124, loss = 0.46973134\n",
            "Iteration 125, loss = 0.46949189\n",
            "Iteration 126, loss = 0.46938456\n",
            "Iteration 127, loss = 0.46929100\n",
            "Iteration 128, loss = 0.46923357\n",
            "Iteration 129, loss = 0.46904888\n",
            "Iteration 130, loss = 0.46889654\n",
            "Iteration 131, loss = 0.46879608\n",
            "Iteration 132, loss = 0.46868330\n",
            "Iteration 133, loss = 0.46851984\n",
            "Iteration 134, loss = 0.46854854\n",
            "Iteration 135, loss = 0.46840742\n",
            "Iteration 136, loss = 0.46823572\n",
            "Iteration 137, loss = 0.46814035\n",
            "Iteration 138, loss = 0.46799230\n",
            "Iteration 139, loss = 0.46786176\n",
            "Iteration 140, loss = 0.46784373\n",
            "Iteration 141, loss = 0.46777985\n",
            "Iteration 142, loss = 0.46765749\n",
            "Iteration 143, loss = 0.46756177\n",
            "Iteration 144, loss = 0.46737635\n",
            "Iteration 145, loss = 0.46726854\n",
            "Iteration 146, loss = 0.46718459\n",
            "Iteration 147, loss = 0.46707903\n",
            "Iteration 148, loss = 0.46701646\n",
            "Iteration 149, loss = 0.46694653\n",
            "Iteration 150, loss = 0.46686615\n",
            "Iteration 151, loss = 0.46687790\n",
            "Iteration 152, loss = 0.46669550\n",
            "Iteration 153, loss = 0.46651443\n",
            "Iteration 154, loss = 0.46644269\n",
            "Iteration 155, loss = 0.46632829\n",
            "Iteration 156, loss = 0.46634526\n",
            "Iteration 157, loss = 0.46616351\n",
            "Iteration 158, loss = 0.46606049\n",
            "Iteration 159, loss = 0.46596315\n",
            "Iteration 160, loss = 0.46589471\n",
            "Iteration 161, loss = 0.46583346\n",
            "Iteration 162, loss = 0.46571779\n",
            "Iteration 163, loss = 0.46576049\n",
            "Iteration 164, loss = 0.46552410\n",
            "Iteration 165, loss = 0.46553548\n",
            "Iteration 166, loss = 0.46540563\n",
            "Iteration 167, loss = 0.46532871\n",
            "Iteration 168, loss = 0.46521006\n",
            "Iteration 169, loss = 0.46514620\n",
            "Iteration 170, loss = 0.46509718\n",
            "Iteration 171, loss = 0.46501398\n",
            "Iteration 172, loss = 0.46503608\n",
            "Iteration 173, loss = 0.46478121\n",
            "Iteration 174, loss = 0.46469354\n",
            "Iteration 175, loss = 0.46467230\n",
            "Iteration 176, loss = 0.46456916\n",
            "Iteration 177, loss = 0.46454784\n",
            "Iteration 178, loss = 0.46446630\n",
            "Iteration 179, loss = 0.46441308\n",
            "Iteration 180, loss = 0.46429401\n",
            "Iteration 181, loss = 0.46416396\n",
            "Iteration 182, loss = 0.46419749\n",
            "Iteration 183, loss = 0.46403275\n",
            "Iteration 184, loss = 0.46397086\n",
            "Iteration 185, loss = 0.46388428\n",
            "Iteration 186, loss = 0.46387195\n",
            "Iteration 187, loss = 0.46375279\n",
            "Iteration 188, loss = 0.46374873\n",
            "Iteration 189, loss = 0.46369854\n",
            "Iteration 190, loss = 0.46357337\n",
            "Iteration 191, loss = 0.46349189\n",
            "Iteration 192, loss = 0.46342293\n",
            "Iteration 193, loss = 0.46326241\n",
            "Iteration 194, loss = 0.46326266\n",
            "Iteration 195, loss = 0.46320970\n",
            "Iteration 196, loss = 0.46317391\n",
            "Iteration 197, loss = 0.46307999\n",
            "Iteration 198, loss = 0.46298625\n",
            "Iteration 199, loss = 0.46297867\n",
            "Iteration 200, loss = 0.46286728\n",
            "Iteration 1, loss = 0.67366716\n",
            "Iteration 2, loss = 0.66006856\n",
            "Iteration 3, loss = 0.64681488\n",
            "Iteration 4, loss = 0.63481215\n",
            "Iteration 5, loss = 0.62393504\n",
            "Iteration 6, loss = 0.61400322\n",
            "Iteration 7, loss = 0.60483302\n",
            "Iteration 8, loss = 0.59620359\n",
            "Iteration 9, loss = 0.58834215\n",
            "Iteration 10, loss = 0.58106360\n",
            "Iteration 11, loss = 0.57455904\n",
            "Iteration 12, loss = 0.56884607\n",
            "Iteration 13, loss = 0.56356487\n",
            "Iteration 14, loss = 0.55890791\n",
            "Iteration 15, loss = 0.55445012\n",
            "Iteration 16, loss = 0.55029230\n",
            "Iteration 17, loss = 0.54641778\n",
            "Iteration 18, loss = 0.54280713\n",
            "Iteration 19, loss = 0.53944934\n",
            "Iteration 20, loss = 0.53628724\n",
            "Iteration 21, loss = 0.53326701\n",
            "Iteration 22, loss = 0.53046955\n",
            "Iteration 23, loss = 0.52794623\n",
            "Iteration 24, loss = 0.52541317\n",
            "Iteration 25, loss = 0.52316118\n",
            "Iteration 26, loss = 0.52097299\n",
            "Iteration 27, loss = 0.51900363\n",
            "Iteration 28, loss = 0.51699335\n",
            "Iteration 29, loss = 0.51503836\n",
            "Iteration 30, loss = 0.51333544\n",
            "Iteration 31, loss = 0.51156853\n",
            "Iteration 32, loss = 0.50999025\n",
            "Iteration 33, loss = 0.50844950\n",
            "Iteration 34, loss = 0.50708385\n",
            "Iteration 35, loss = 0.50551051\n",
            "Iteration 36, loss = 0.50406423\n",
            "Iteration 37, loss = 0.50274436\n",
            "Iteration 38, loss = 0.50150556\n",
            "Iteration 39, loss = 0.50025448\n",
            "Iteration 40, loss = 0.49906335\n",
            "Iteration 41, loss = 0.49794252\n",
            "Iteration 42, loss = 0.49696974\n",
            "Iteration 43, loss = 0.49587441\n",
            "Iteration 44, loss = 0.49494261\n",
            "Iteration 45, loss = 0.49410468\n",
            "Iteration 46, loss = 0.49305873\n",
            "Iteration 47, loss = 0.49229731\n",
            "Iteration 48, loss = 0.49142828\n",
            "Iteration 49, loss = 0.49065015\n",
            "Iteration 50, loss = 0.49005877\n",
            "Iteration 51, loss = 0.48935951\n",
            "Iteration 52, loss = 0.48855068\n",
            "Iteration 53, loss = 0.48787981\n",
            "Iteration 54, loss = 0.48737715\n",
            "Iteration 55, loss = 0.48662254\n",
            "Iteration 56, loss = 0.48599948\n",
            "Iteration 57, loss = 0.48543960\n",
            "Iteration 58, loss = 0.48490139\n",
            "Iteration 59, loss = 0.48437245\n",
            "Iteration 60, loss = 0.48385944\n",
            "Iteration 61, loss = 0.48337215\n",
            "Iteration 62, loss = 0.48285757\n",
            "Iteration 63, loss = 0.48240865\n",
            "Iteration 64, loss = 0.48203459\n",
            "Iteration 65, loss = 0.48149180\n",
            "Iteration 66, loss = 0.48112515\n",
            "Iteration 67, loss = 0.48068590\n",
            "Iteration 68, loss = 0.48038744\n",
            "Iteration 69, loss = 0.47999700\n",
            "Iteration 70, loss = 0.47949810\n",
            "Iteration 71, loss = 0.47917660\n",
            "Iteration 72, loss = 0.47875730\n",
            "Iteration 73, loss = 0.47844169\n",
            "Iteration 74, loss = 0.47814388\n",
            "Iteration 75, loss = 0.47774049\n",
            "Iteration 76, loss = 0.47743078\n",
            "Iteration 77, loss = 0.47707005\n",
            "Iteration 78, loss = 0.47680530\n",
            "Iteration 79, loss = 0.47652607\n",
            "Iteration 80, loss = 0.47624824\n",
            "Iteration 81, loss = 0.47601253\n",
            "Iteration 82, loss = 0.47573682\n",
            "Iteration 83, loss = 0.47542162\n",
            "Iteration 84, loss = 0.47513925\n",
            "Iteration 85, loss = 0.47492901\n",
            "Iteration 86, loss = 0.47466100\n",
            "Iteration 87, loss = 0.47442558\n",
            "Iteration 88, loss = 0.47415592\n",
            "Iteration 89, loss = 0.47396704\n",
            "Iteration 90, loss = 0.47384292\n",
            "Iteration 91, loss = 0.47345885\n",
            "Iteration 92, loss = 0.47321126\n",
            "Iteration 93, loss = 0.47303510\n",
            "Iteration 94, loss = 0.47290911\n",
            "Iteration 95, loss = 0.47267010\n",
            "Iteration 96, loss = 0.47238942\n",
            "Iteration 97, loss = 0.47225243\n",
            "Iteration 98, loss = 0.47201712\n",
            "Iteration 99, loss = 0.47181942\n",
            "Iteration 100, loss = 0.47162870\n",
            "Iteration 101, loss = 0.47146365\n",
            "Iteration 102, loss = 0.47130878\n",
            "Iteration 103, loss = 0.47111126\n",
            "Iteration 104, loss = 0.47096450\n",
            "Iteration 105, loss = 0.47072936\n",
            "Iteration 106, loss = 0.47057994\n",
            "Iteration 107, loss = 0.47040194\n",
            "Iteration 108, loss = 0.47025815\n",
            "Iteration 109, loss = 0.47011102\n",
            "Iteration 110, loss = 0.46994712\n",
            "Iteration 111, loss = 0.46976900\n",
            "Iteration 112, loss = 0.46963856\n",
            "Iteration 113, loss = 0.46956763\n",
            "Iteration 114, loss = 0.46931132\n",
            "Iteration 115, loss = 0.46918191\n",
            "Iteration 116, loss = 0.46915772\n",
            "Iteration 117, loss = 0.46886664\n",
            "Iteration 118, loss = 0.46885408\n",
            "Iteration 119, loss = 0.46866517\n",
            "Iteration 120, loss = 0.46847897\n",
            "Iteration 121, loss = 0.46838380\n",
            "Iteration 122, loss = 0.46820384\n",
            "Iteration 123, loss = 0.46813796\n",
            "Iteration 124, loss = 0.46804599\n",
            "Iteration 125, loss = 0.46788671\n",
            "Iteration 126, loss = 0.46773946\n",
            "Iteration 127, loss = 0.46757396\n",
            "Iteration 128, loss = 0.46757936\n",
            "Iteration 129, loss = 0.46737501\n",
            "Iteration 130, loss = 0.46726349\n",
            "Iteration 131, loss = 0.46712418\n",
            "Iteration 132, loss = 0.46711196\n",
            "Iteration 133, loss = 0.46686461\n",
            "Iteration 134, loss = 0.46680068\n",
            "Iteration 135, loss = 0.46675044\n",
            "Iteration 136, loss = 0.46656239\n",
            "Iteration 137, loss = 0.46648138\n",
            "Iteration 138, loss = 0.46638420\n",
            "Iteration 139, loss = 0.46625026\n",
            "Iteration 140, loss = 0.46613619\n",
            "Iteration 141, loss = 0.46602063\n",
            "Iteration 142, loss = 0.46598518\n",
            "Iteration 143, loss = 0.46586592\n",
            "Iteration 144, loss = 0.46571778\n",
            "Iteration 145, loss = 0.46570210\n",
            "Iteration 146, loss = 0.46552078\n",
            "Iteration 147, loss = 0.46545849\n",
            "Iteration 148, loss = 0.46539433\n",
            "Iteration 149, loss = 0.46522813\n",
            "Iteration 150, loss = 0.46514079\n",
            "Iteration 151, loss = 0.46517876\n",
            "Iteration 152, loss = 0.46497027\n",
            "Iteration 153, loss = 0.46483499\n",
            "Iteration 154, loss = 0.46475934\n",
            "Iteration 155, loss = 0.46462372\n",
            "Iteration 156, loss = 0.46460147\n",
            "Iteration 157, loss = 0.46464865\n",
            "Iteration 158, loss = 0.46434874\n",
            "Iteration 159, loss = 0.46425003\n",
            "Iteration 160, loss = 0.46426942\n",
            "Iteration 161, loss = 0.46412648\n",
            "Iteration 162, loss = 0.46400237\n",
            "Iteration 163, loss = 0.46389660\n",
            "Iteration 164, loss = 0.46380166\n",
            "Iteration 165, loss = 0.46375983\n",
            "Iteration 166, loss = 0.46359584\n",
            "Iteration 167, loss = 0.46352750\n",
            "Iteration 168, loss = 0.46345255\n",
            "Iteration 169, loss = 0.46333355\n",
            "Iteration 170, loss = 0.46325434\n",
            "Iteration 171, loss = 0.46319043\n",
            "Iteration 172, loss = 0.46317434\n",
            "Iteration 173, loss = 0.46299601\n",
            "Iteration 174, loss = 0.46297594\n",
            "Iteration 175, loss = 0.46290333\n",
            "Iteration 176, loss = 0.46277762\n",
            "Iteration 177, loss = 0.46268027\n",
            "Iteration 178, loss = 0.46262216\n",
            "Iteration 179, loss = 0.46255399\n",
            "Iteration 180, loss = 0.46252659\n",
            "Iteration 181, loss = 0.46243739\n",
            "Iteration 182, loss = 0.46244145\n",
            "Iteration 183, loss = 0.46223457\n",
            "Iteration 184, loss = 0.46218043\n",
            "Iteration 185, loss = 0.46218717\n",
            "Iteration 186, loss = 0.46216304\n",
            "Iteration 187, loss = 0.46191910\n",
            "Iteration 188, loss = 0.46189567\n",
            "Iteration 189, loss = 0.46176972\n",
            "Iteration 190, loss = 0.46174926\n",
            "Iteration 191, loss = 0.46163179\n",
            "Iteration 192, loss = 0.46159029\n",
            "Iteration 193, loss = 0.46153870\n",
            "Iteration 194, loss = 0.46141759\n",
            "Iteration 195, loss = 0.46137219\n",
            "Iteration 196, loss = 0.46134308\n",
            "Iteration 197, loss = 0.46126527\n",
            "Iteration 198, loss = 0.46111435\n",
            "Iteration 199, loss = 0.46112484\n",
            "Iteration 200, loss = 0.46102665\n",
            "Iteration 1, loss = 0.67385827\n",
            "Iteration 2, loss = 0.66039211\n",
            "Iteration 3, loss = 0.64738028\n",
            "Iteration 4, loss = 0.63563298\n",
            "Iteration 5, loss = 0.62494387\n",
            "Iteration 6, loss = 0.61523342\n",
            "Iteration 7, loss = 0.60625957\n",
            "Iteration 8, loss = 0.59775521\n",
            "Iteration 9, loss = 0.59003241\n",
            "Iteration 10, loss = 0.58287061\n",
            "Iteration 11, loss = 0.57642570\n",
            "Iteration 12, loss = 0.57071486\n",
            "Iteration 13, loss = 0.56548216\n",
            "Iteration 14, loss = 0.56086751\n",
            "Iteration 15, loss = 0.55642255\n",
            "Iteration 16, loss = 0.55234759\n",
            "Iteration 17, loss = 0.54847719\n",
            "Iteration 18, loss = 0.54488830\n",
            "Iteration 19, loss = 0.54150624\n",
            "Iteration 20, loss = 0.53838860\n",
            "Iteration 21, loss = 0.53545496\n",
            "Iteration 22, loss = 0.53260570\n",
            "Iteration 23, loss = 0.53007584\n",
            "Iteration 24, loss = 0.52748475\n",
            "Iteration 25, loss = 0.52524050\n",
            "Iteration 26, loss = 0.52304975\n",
            "Iteration 27, loss = 0.52102376\n",
            "Iteration 28, loss = 0.51898184\n",
            "Iteration 29, loss = 0.51711455\n",
            "Iteration 30, loss = 0.51533582\n",
            "Iteration 31, loss = 0.51369884\n",
            "Iteration 32, loss = 0.51194675\n",
            "Iteration 33, loss = 0.51043316\n",
            "Iteration 34, loss = 0.50908585\n",
            "Iteration 35, loss = 0.50752282\n",
            "Iteration 36, loss = 0.50612438\n",
            "Iteration 37, loss = 0.50484958\n",
            "Iteration 38, loss = 0.50359028\n",
            "Iteration 39, loss = 0.50236437\n",
            "Iteration 40, loss = 0.50115147\n",
            "Iteration 41, loss = 0.50003957\n",
            "Iteration 42, loss = 0.49899168\n",
            "Iteration 43, loss = 0.49789979\n",
            "Iteration 44, loss = 0.49697988\n",
            "Iteration 45, loss = 0.49612561\n",
            "Iteration 46, loss = 0.49506981\n",
            "Iteration 47, loss = 0.49427154\n",
            "Iteration 48, loss = 0.49344768\n",
            "Iteration 49, loss = 0.49270758\n",
            "Iteration 50, loss = 0.49202974\n",
            "Iteration 51, loss = 0.49138608\n",
            "Iteration 52, loss = 0.49063568\n",
            "Iteration 53, loss = 0.48997432\n",
            "Iteration 54, loss = 0.48945174\n",
            "Iteration 55, loss = 0.48869771\n",
            "Iteration 56, loss = 0.48806553\n",
            "Iteration 57, loss = 0.48748177\n",
            "Iteration 58, loss = 0.48693443\n",
            "Iteration 59, loss = 0.48632465\n",
            "Iteration 60, loss = 0.48580511\n",
            "Iteration 61, loss = 0.48538513\n",
            "Iteration 62, loss = 0.48479214\n",
            "Iteration 63, loss = 0.48436301\n",
            "Iteration 64, loss = 0.48396622\n",
            "Iteration 65, loss = 0.48340908\n",
            "Iteration 66, loss = 0.48296706\n",
            "Iteration 67, loss = 0.48259902\n",
            "Iteration 68, loss = 0.48231438\n",
            "Iteration 69, loss = 0.48189687\n",
            "Iteration 70, loss = 0.48143730\n",
            "Iteration 71, loss = 0.48105343\n",
            "Iteration 72, loss = 0.48069979\n",
            "Iteration 73, loss = 0.48032552\n",
            "Iteration 74, loss = 0.48001053\n",
            "Iteration 75, loss = 0.47963197\n",
            "Iteration 76, loss = 0.47933549\n",
            "Iteration 77, loss = 0.47899179\n",
            "Iteration 78, loss = 0.47873844\n",
            "Iteration 79, loss = 0.47842378\n",
            "Iteration 80, loss = 0.47816906\n",
            "Iteration 81, loss = 0.47785831\n",
            "Iteration 82, loss = 0.47768402\n",
            "Iteration 83, loss = 0.47734332\n",
            "Iteration 84, loss = 0.47709589\n",
            "Iteration 85, loss = 0.47682591\n",
            "Iteration 86, loss = 0.47650557\n",
            "Iteration 87, loss = 0.47627904\n",
            "Iteration 88, loss = 0.47599716\n",
            "Iteration 89, loss = 0.47580268\n",
            "Iteration 90, loss = 0.47559767\n",
            "Iteration 91, loss = 0.47525811\n",
            "Iteration 92, loss = 0.47503645\n",
            "Iteration 93, loss = 0.47484196\n",
            "Iteration 94, loss = 0.47474454\n",
            "Iteration 95, loss = 0.47448047\n",
            "Iteration 96, loss = 0.47417880\n",
            "Iteration 97, loss = 0.47402213\n",
            "Iteration 98, loss = 0.47376157\n",
            "Iteration 99, loss = 0.47357305\n",
            "Iteration 100, loss = 0.47337790\n",
            "Iteration 101, loss = 0.47319286\n",
            "Iteration 102, loss = 0.47296149\n",
            "Iteration 103, loss = 0.47281429\n",
            "Iteration 104, loss = 0.47262627\n",
            "Iteration 105, loss = 0.47237799\n",
            "Iteration 106, loss = 0.47225569\n",
            "Iteration 107, loss = 0.47201333\n",
            "Iteration 108, loss = 0.47184730\n",
            "Iteration 109, loss = 0.47171403\n",
            "Iteration 110, loss = 0.47152737\n",
            "Iteration 111, loss = 0.47136164\n",
            "Iteration 112, loss = 0.47123196\n",
            "Iteration 113, loss = 0.47109072\n",
            "Iteration 114, loss = 0.47086172\n",
            "Iteration 115, loss = 0.47070736\n",
            "Iteration 116, loss = 0.47072755\n",
            "Iteration 117, loss = 0.47042552\n",
            "Iteration 118, loss = 0.47027818\n",
            "Iteration 119, loss = 0.47015429\n",
            "Iteration 120, loss = 0.46995085\n",
            "Iteration 121, loss = 0.46982212\n",
            "Iteration 122, loss = 0.46967670\n",
            "Iteration 123, loss = 0.46955569\n",
            "Iteration 124, loss = 0.46946367\n",
            "Iteration 125, loss = 0.46924048\n",
            "Iteration 126, loss = 0.46907838\n",
            "Iteration 127, loss = 0.46895727\n",
            "Iteration 128, loss = 0.46886964\n",
            "Iteration 129, loss = 0.46869296\n",
            "Iteration 130, loss = 0.46855717\n",
            "Iteration 131, loss = 0.46847475\n",
            "Iteration 132, loss = 0.46836262\n",
            "Iteration 133, loss = 0.46810787\n",
            "Iteration 134, loss = 0.46807036\n",
            "Iteration 135, loss = 0.46796557\n",
            "Iteration 136, loss = 0.46772521\n",
            "Iteration 137, loss = 0.46764729\n",
            "Iteration 138, loss = 0.46757148\n",
            "Iteration 139, loss = 0.46749181\n",
            "Iteration 140, loss = 0.46731762\n",
            "Iteration 141, loss = 0.46724406\n",
            "Iteration 142, loss = 0.46714674\n",
            "Iteration 143, loss = 0.46702027\n",
            "Iteration 144, loss = 0.46684416\n",
            "Iteration 145, loss = 0.46681917\n",
            "Iteration 146, loss = 0.46669073\n",
            "Iteration 147, loss = 0.46658516\n",
            "Iteration 148, loss = 0.46657150\n",
            "Iteration 149, loss = 0.46641255\n",
            "Iteration 150, loss = 0.46628516\n",
            "Iteration 151, loss = 0.46631904\n",
            "Iteration 152, loss = 0.46617210\n",
            "Iteration 153, loss = 0.46597083\n",
            "Iteration 154, loss = 0.46593512\n",
            "Iteration 155, loss = 0.46580247\n",
            "Iteration 156, loss = 0.46566670\n",
            "Iteration 157, loss = 0.46576955\n",
            "Iteration 158, loss = 0.46551171\n",
            "Iteration 159, loss = 0.46539457\n",
            "Iteration 160, loss = 0.46538457\n",
            "Iteration 161, loss = 0.46525636\n",
            "Iteration 162, loss = 0.46515970\n",
            "Iteration 163, loss = 0.46512193\n",
            "Iteration 164, loss = 0.46499125\n",
            "Iteration 165, loss = 0.46488168\n",
            "Iteration 166, loss = 0.46481839\n",
            "Iteration 167, loss = 0.46471087\n",
            "Iteration 168, loss = 0.46467440\n",
            "Iteration 169, loss = 0.46451103\n",
            "Iteration 170, loss = 0.46443308\n",
            "Iteration 171, loss = 0.46438081\n",
            "Iteration 172, loss = 0.46426544\n",
            "Iteration 173, loss = 0.46418017\n",
            "Iteration 174, loss = 0.46416545\n",
            "Iteration 175, loss = 0.46407859\n",
            "Iteration 176, loss = 0.46392264\n",
            "Iteration 177, loss = 0.46386150\n",
            "Iteration 178, loss = 0.46375738\n",
            "Iteration 179, loss = 0.46372679\n",
            "Iteration 180, loss = 0.46366894\n",
            "Iteration 181, loss = 0.46356460\n",
            "Iteration 182, loss = 0.46355984\n",
            "Iteration 183, loss = 0.46341071\n",
            "Iteration 184, loss = 0.46335794\n",
            "Iteration 185, loss = 0.46335403\n",
            "Iteration 186, loss = 0.46324876\n",
            "Iteration 187, loss = 0.46305408\n",
            "Iteration 188, loss = 0.46302169\n",
            "Iteration 189, loss = 0.46293990\n",
            "Iteration 190, loss = 0.46293457\n",
            "Iteration 191, loss = 0.46279027\n",
            "Iteration 192, loss = 0.46270820\n",
            "Iteration 193, loss = 0.46266872\n",
            "Iteration 194, loss = 0.46262138\n",
            "Iteration 195, loss = 0.46246893\n",
            "Iteration 196, loss = 0.46243903\n",
            "Iteration 197, loss = 0.46243212\n",
            "Iteration 198, loss = 0.46227646\n",
            "Iteration 199, loss = 0.46221669\n",
            "Iteration 200, loss = 0.46214375\n",
            "Iteration 1, loss = 0.67437550\n",
            "Iteration 2, loss = 0.66146030\n",
            "Iteration 3, loss = 0.64898746\n",
            "Iteration 4, loss = 0.63758985\n",
            "Iteration 5, loss = 0.62713784\n",
            "Iteration 6, loss = 0.61761308\n",
            "Iteration 7, loss = 0.60881738\n",
            "Iteration 8, loss = 0.60045558\n",
            "Iteration 9, loss = 0.59283332\n",
            "Iteration 10, loss = 0.58568074\n",
            "Iteration 11, loss = 0.57919997\n",
            "Iteration 12, loss = 0.57358283\n",
            "Iteration 13, loss = 0.56855875\n",
            "Iteration 14, loss = 0.56400143\n",
            "Iteration 15, loss = 0.55961791\n",
            "Iteration 16, loss = 0.55565630\n",
            "Iteration 17, loss = 0.55189644\n",
            "Iteration 18, loss = 0.54844601\n",
            "Iteration 19, loss = 0.54515169\n",
            "Iteration 20, loss = 0.54213425\n",
            "Iteration 21, loss = 0.53925503\n",
            "Iteration 22, loss = 0.53659373\n",
            "Iteration 23, loss = 0.53403908\n",
            "Iteration 24, loss = 0.53159106\n",
            "Iteration 25, loss = 0.52941155\n",
            "Iteration 26, loss = 0.52729137\n",
            "Iteration 27, loss = 0.52532377\n",
            "Iteration 28, loss = 0.52326059\n",
            "Iteration 29, loss = 0.52140409\n",
            "Iteration 30, loss = 0.51961884\n",
            "Iteration 31, loss = 0.51794229\n",
            "Iteration 32, loss = 0.51630077\n",
            "Iteration 33, loss = 0.51481496\n",
            "Iteration 34, loss = 0.51342544\n",
            "Iteration 35, loss = 0.51189321\n",
            "Iteration 36, loss = 0.51047883\n",
            "Iteration 37, loss = 0.50913114\n",
            "Iteration 38, loss = 0.50792156\n",
            "Iteration 39, loss = 0.50668796\n",
            "Iteration 40, loss = 0.50546539\n",
            "Iteration 41, loss = 0.50432719\n",
            "Iteration 42, loss = 0.50335520\n",
            "Iteration 43, loss = 0.50227898\n",
            "Iteration 44, loss = 0.50134945\n",
            "Iteration 45, loss = 0.50054226\n",
            "Iteration 46, loss = 0.49946130\n",
            "Iteration 47, loss = 0.49867313\n",
            "Iteration 48, loss = 0.49787081\n",
            "Iteration 49, loss = 0.49713634\n",
            "Iteration 50, loss = 0.49637299\n",
            "Iteration 51, loss = 0.49567495\n",
            "Iteration 52, loss = 0.49497325\n",
            "Iteration 53, loss = 0.49429561\n",
            "Iteration 54, loss = 0.49376487\n",
            "Iteration 55, loss = 0.49304457\n",
            "Iteration 56, loss = 0.49239043\n",
            "Iteration 57, loss = 0.49177997\n",
            "Iteration 58, loss = 0.49123761\n",
            "Iteration 59, loss = 0.49067263\n",
            "Iteration 60, loss = 0.49013336\n",
            "Iteration 61, loss = 0.48971557\n",
            "Iteration 62, loss = 0.48911390\n",
            "Iteration 63, loss = 0.48868551\n",
            "Iteration 64, loss = 0.48827417\n",
            "Iteration 65, loss = 0.48776322\n",
            "Iteration 66, loss = 0.48727762\n",
            "Iteration 67, loss = 0.48693471\n",
            "Iteration 68, loss = 0.48660193\n",
            "Iteration 69, loss = 0.48619168\n",
            "Iteration 70, loss = 0.48575081\n",
            "Iteration 71, loss = 0.48538551\n",
            "Iteration 72, loss = 0.48502045\n",
            "Iteration 73, loss = 0.48465923\n",
            "Iteration 74, loss = 0.48431774\n",
            "Iteration 75, loss = 0.48398298\n",
            "Iteration 76, loss = 0.48368238\n",
            "Iteration 77, loss = 0.48333524\n",
            "Iteration 78, loss = 0.48306850\n",
            "Iteration 79, loss = 0.48270332\n",
            "Iteration 80, loss = 0.48246501\n",
            "Iteration 81, loss = 0.48213760\n",
            "Iteration 82, loss = 0.48190448\n",
            "Iteration 83, loss = 0.48167370\n",
            "Iteration 84, loss = 0.48145178\n",
            "Iteration 85, loss = 0.48110452\n",
            "Iteration 86, loss = 0.48083425\n",
            "Iteration 87, loss = 0.48062235\n",
            "Iteration 88, loss = 0.48035326\n",
            "Iteration 89, loss = 0.48017375\n",
            "Iteration 90, loss = 0.47994138\n",
            "Iteration 91, loss = 0.47963974\n",
            "Iteration 92, loss = 0.47941883\n",
            "Iteration 93, loss = 0.47927643\n",
            "Iteration 94, loss = 0.47911935\n",
            "Iteration 95, loss = 0.47892165\n",
            "Iteration 96, loss = 0.47859700\n",
            "Iteration 97, loss = 0.47838424\n",
            "Iteration 98, loss = 0.47814182\n",
            "Iteration 99, loss = 0.47793766\n",
            "Iteration 100, loss = 0.47780298\n",
            "Iteration 101, loss = 0.47754889\n",
            "Iteration 102, loss = 0.47737873\n",
            "Iteration 103, loss = 0.47720876\n",
            "Iteration 104, loss = 0.47702771\n",
            "Iteration 105, loss = 0.47682016\n",
            "Iteration 106, loss = 0.47664951\n",
            "Iteration 107, loss = 0.47646653\n",
            "Iteration 108, loss = 0.47634748\n",
            "Iteration 109, loss = 0.47615056\n",
            "Iteration 110, loss = 0.47596819\n",
            "Iteration 111, loss = 0.47586904\n",
            "Iteration 112, loss = 0.47569910\n",
            "Iteration 113, loss = 0.47557616\n",
            "Iteration 114, loss = 0.47537834\n",
            "Iteration 115, loss = 0.47523759\n",
            "Iteration 116, loss = 0.47519112\n",
            "Iteration 117, loss = 0.47493785\n",
            "Iteration 118, loss = 0.47485202\n",
            "Iteration 119, loss = 0.47473236\n",
            "Iteration 120, loss = 0.47450864\n",
            "Iteration 121, loss = 0.47449029\n",
            "Iteration 122, loss = 0.47428337\n",
            "Iteration 123, loss = 0.47414220\n",
            "Iteration 124, loss = 0.47405107\n",
            "Iteration 125, loss = 0.47390338\n",
            "Iteration 126, loss = 0.47377870\n",
            "Iteration 127, loss = 0.47371097\n",
            "Iteration 128, loss = 0.47360151\n",
            "Iteration 129, loss = 0.47351999\n",
            "Iteration 130, loss = 0.47334398\n",
            "Iteration 131, loss = 0.47324932\n",
            "Iteration 132, loss = 0.47313275\n",
            "Iteration 133, loss = 0.47296653\n",
            "Iteration 134, loss = 0.47286360\n",
            "Iteration 135, loss = 0.47281531\n",
            "Iteration 136, loss = 0.47268396\n",
            "Iteration 137, loss = 0.47255246\n",
            "Iteration 138, loss = 0.47252624\n",
            "Iteration 139, loss = 0.47247279\n",
            "Iteration 140, loss = 0.47224612\n",
            "Iteration 141, loss = 0.47216185\n",
            "Iteration 142, loss = 0.47205759\n",
            "Iteration 143, loss = 0.47204013\n",
            "Iteration 144, loss = 0.47183522\n",
            "Iteration 145, loss = 0.47184105\n",
            "Iteration 146, loss = 0.47167698\n",
            "Iteration 147, loss = 0.47162093\n",
            "Iteration 148, loss = 0.47156757\n",
            "Iteration 149, loss = 0.47143236\n",
            "Iteration 150, loss = 0.47131313\n",
            "Iteration 151, loss = 0.47127748\n",
            "Iteration 152, loss = 0.47110627\n",
            "Iteration 153, loss = 0.47103175\n",
            "Iteration 154, loss = 0.47101425\n",
            "Iteration 155, loss = 0.47085441\n",
            "Iteration 156, loss = 0.47071745\n",
            "Iteration 157, loss = 0.47071960\n",
            "Iteration 158, loss = 0.47060815\n",
            "Iteration 159, loss = 0.47049009\n",
            "Iteration 160, loss = 0.47051924\n",
            "Iteration 161, loss = 0.47037119\n",
            "Iteration 162, loss = 0.47025710\n",
            "Iteration 163, loss = 0.47026772\n",
            "Iteration 164, loss = 0.47006709\n",
            "Iteration 165, loss = 0.47000771\n",
            "Iteration 166, loss = 0.46990525\n",
            "Iteration 167, loss = 0.46984351\n",
            "Iteration 168, loss = 0.46981070\n",
            "Iteration 169, loss = 0.46966093\n",
            "Iteration 170, loss = 0.46958259\n",
            "Iteration 171, loss = 0.46951116\n",
            "Iteration 172, loss = 0.46945215\n",
            "Iteration 173, loss = 0.46933674\n",
            "Iteration 174, loss = 0.46930860\n",
            "Iteration 175, loss = 0.46921882\n",
            "Iteration 176, loss = 0.46907472\n",
            "Iteration 177, loss = 0.46905434\n",
            "Iteration 178, loss = 0.46895652\n",
            "Iteration 179, loss = 0.46891519\n",
            "Iteration 180, loss = 0.46881112\n",
            "Iteration 181, loss = 0.46874133\n",
            "Iteration 182, loss = 0.46870450\n",
            "Iteration 183, loss = 0.46864187\n",
            "Iteration 184, loss = 0.46861063\n",
            "Iteration 185, loss = 0.46851344\n",
            "Iteration 186, loss = 0.46844641\n",
            "Iteration 187, loss = 0.46831963\n",
            "Iteration 188, loss = 0.46826240\n",
            "Iteration 189, loss = 0.46818307\n",
            "Iteration 190, loss = 0.46817393\n",
            "Iteration 191, loss = 0.46808094\n",
            "Iteration 192, loss = 0.46798436\n",
            "Iteration 193, loss = 0.46793662\n",
            "Iteration 194, loss = 0.46792330\n",
            "Iteration 195, loss = 0.46782099\n",
            "Iteration 196, loss = 0.46778896\n",
            "Iteration 197, loss = 0.46779793\n",
            "Iteration 198, loss = 0.46764815\n",
            "Iteration 199, loss = 0.46763951\n",
            "Iteration 200, loss = 0.46750849\n",
            "Iteration 1, loss = 0.67413553\n",
            "Iteration 2, loss = 0.66081676\n",
            "Iteration 3, loss = 0.64778075\n",
            "Iteration 4, loss = 0.63593282\n",
            "Iteration 5, loss = 0.62508965\n",
            "Iteration 6, loss = 0.61527839\n",
            "Iteration 7, loss = 0.60625652\n",
            "Iteration 8, loss = 0.59770878\n",
            "Iteration 9, loss = 0.58989735\n",
            "Iteration 10, loss = 0.58271955\n",
            "Iteration 11, loss = 0.57620851\n",
            "Iteration 12, loss = 0.57044681\n",
            "Iteration 13, loss = 0.56522179\n",
            "Iteration 14, loss = 0.56053796\n",
            "Iteration 15, loss = 0.55606197\n",
            "Iteration 16, loss = 0.55201825\n",
            "Iteration 17, loss = 0.54814710\n",
            "Iteration 18, loss = 0.54462516\n",
            "Iteration 19, loss = 0.54126789\n",
            "Iteration 20, loss = 0.53817642\n",
            "Iteration 21, loss = 0.53522573\n",
            "Iteration 22, loss = 0.53247813\n",
            "Iteration 23, loss = 0.52982852\n",
            "Iteration 24, loss = 0.52733414\n",
            "Iteration 25, loss = 0.52499037\n",
            "Iteration 26, loss = 0.52275132\n",
            "Iteration 27, loss = 0.52081305\n",
            "Iteration 28, loss = 0.51869224\n",
            "Iteration 29, loss = 0.51677121\n",
            "Iteration 30, loss = 0.51499564\n",
            "Iteration 31, loss = 0.51330634\n",
            "Iteration 32, loss = 0.51163955\n",
            "Iteration 33, loss = 0.51014057\n",
            "Iteration 34, loss = 0.50871557\n",
            "Iteration 35, loss = 0.50717073\n",
            "Iteration 36, loss = 0.50574700\n",
            "Iteration 37, loss = 0.50433840\n",
            "Iteration 38, loss = 0.50316335\n",
            "Iteration 39, loss = 0.50182467\n",
            "Iteration 40, loss = 0.50060198\n",
            "Iteration 41, loss = 0.49945960\n",
            "Iteration 42, loss = 0.49848829\n",
            "Iteration 43, loss = 0.49739560\n",
            "Iteration 44, loss = 0.49648159\n",
            "Iteration 45, loss = 0.49569915\n",
            "Iteration 46, loss = 0.49450639\n",
            "Iteration 47, loss = 0.49369099\n",
            "Iteration 48, loss = 0.49286209\n",
            "Iteration 49, loss = 0.49203681\n",
            "Iteration 50, loss = 0.49127173\n",
            "Iteration 51, loss = 0.49044526\n",
            "Iteration 52, loss = 0.48974046\n",
            "Iteration 53, loss = 0.48912683\n",
            "Iteration 54, loss = 0.48838611\n",
            "Iteration 55, loss = 0.48776255\n",
            "Iteration 56, loss = 0.48705914\n",
            "Iteration 57, loss = 0.48643380\n",
            "Iteration 58, loss = 0.48586191\n",
            "Iteration 59, loss = 0.48524961\n",
            "Iteration 60, loss = 0.48469562\n",
            "Iteration 61, loss = 0.48415761\n",
            "Iteration 62, loss = 0.48364535\n",
            "Iteration 63, loss = 0.48315440\n",
            "Iteration 64, loss = 0.48264711\n",
            "Iteration 65, loss = 0.48217319\n",
            "Iteration 66, loss = 0.48167039\n",
            "Iteration 67, loss = 0.48123416\n",
            "Iteration 68, loss = 0.48092001\n",
            "Iteration 69, loss = 0.48048466\n",
            "Iteration 70, loss = 0.48000505\n",
            "Iteration 71, loss = 0.47954890\n",
            "Iteration 72, loss = 0.47918648\n",
            "Iteration 73, loss = 0.47874904\n",
            "Iteration 74, loss = 0.47839822\n",
            "Iteration 75, loss = 0.47805512\n",
            "Iteration 76, loss = 0.47770647\n",
            "Iteration 77, loss = 0.47739724\n",
            "Iteration 78, loss = 0.47701581\n",
            "Iteration 79, loss = 0.47666055\n",
            "Iteration 80, loss = 0.47636170\n",
            "Iteration 81, loss = 0.47602901\n",
            "Iteration 82, loss = 0.47575429\n",
            "Iteration 83, loss = 0.47551742\n",
            "Iteration 84, loss = 0.47529003\n",
            "Iteration 85, loss = 0.47482696\n",
            "Iteration 86, loss = 0.47452333\n",
            "Iteration 87, loss = 0.47425576\n",
            "Iteration 88, loss = 0.47400010\n",
            "Iteration 89, loss = 0.47377658\n",
            "Iteration 90, loss = 0.47355854\n",
            "Iteration 91, loss = 0.47319449\n",
            "Iteration 92, loss = 0.47295827\n",
            "Iteration 93, loss = 0.47278716\n",
            "Iteration 94, loss = 0.47259984\n",
            "Iteration 95, loss = 0.47237836\n",
            "Iteration 96, loss = 0.47205844\n",
            "Iteration 97, loss = 0.47188791\n",
            "Iteration 98, loss = 0.47160507\n",
            "Iteration 99, loss = 0.47143830\n",
            "Iteration 100, loss = 0.47119899\n",
            "Iteration 101, loss = 0.47098021\n",
            "Iteration 102, loss = 0.47081121\n",
            "Iteration 103, loss = 0.47061964\n",
            "Iteration 104, loss = 0.47044920\n",
            "Iteration 105, loss = 0.47016409\n",
            "Iteration 106, loss = 0.46999877\n",
            "Iteration 107, loss = 0.46985303\n",
            "Iteration 108, loss = 0.46968702\n",
            "Iteration 109, loss = 0.46951551\n",
            "Iteration 110, loss = 0.46933815\n",
            "Iteration 111, loss = 0.46918549\n",
            "Iteration 112, loss = 0.46904191\n",
            "Iteration 113, loss = 0.46893274\n",
            "Iteration 114, loss = 0.46871801\n",
            "Iteration 115, loss = 0.46855346\n",
            "Iteration 116, loss = 0.46847397\n",
            "Iteration 117, loss = 0.46819452\n",
            "Iteration 118, loss = 0.46812626\n",
            "Iteration 119, loss = 0.46794304\n",
            "Iteration 120, loss = 0.46777098\n",
            "Iteration 121, loss = 0.46775154\n",
            "Iteration 122, loss = 0.46755294\n",
            "Iteration 123, loss = 0.46735795\n",
            "Iteration 124, loss = 0.46721714\n",
            "Iteration 125, loss = 0.46707528\n",
            "Iteration 126, loss = 0.46694442\n",
            "Iteration 127, loss = 0.46681171\n",
            "Iteration 128, loss = 0.46663720\n",
            "Iteration 129, loss = 0.46664755\n",
            "Iteration 130, loss = 0.46642245\n",
            "Iteration 131, loss = 0.46628022\n",
            "Iteration 132, loss = 0.46611792\n",
            "Iteration 133, loss = 0.46600155\n",
            "Iteration 134, loss = 0.46589336\n",
            "Iteration 135, loss = 0.46577685\n",
            "Iteration 136, loss = 0.46563061\n",
            "Iteration 137, loss = 0.46550203\n",
            "Iteration 138, loss = 0.46543840\n",
            "Iteration 139, loss = 0.46538443\n",
            "Iteration 140, loss = 0.46516547\n",
            "Iteration 141, loss = 0.46507571\n",
            "Iteration 142, loss = 0.46495206\n",
            "Iteration 143, loss = 0.46491005\n",
            "Iteration 144, loss = 0.46469789\n",
            "Iteration 145, loss = 0.46465315\n",
            "Iteration 146, loss = 0.46461413\n",
            "Iteration 147, loss = 0.46443481\n",
            "Iteration 148, loss = 0.46432503\n",
            "Iteration 149, loss = 0.46426231\n",
            "Iteration 150, loss = 0.46414259\n",
            "Iteration 151, loss = 0.46407782\n",
            "Iteration 152, loss = 0.46383770\n",
            "Iteration 153, loss = 0.46384966\n",
            "Iteration 154, loss = 0.46377745\n",
            "Iteration 155, loss = 0.46363194\n",
            "Iteration 156, loss = 0.46346087\n",
            "Iteration 157, loss = 0.46343941\n",
            "Iteration 158, loss = 0.46333738\n",
            "Iteration 159, loss = 0.46324148\n",
            "Iteration 160, loss = 0.46320909\n",
            "Iteration 161, loss = 0.46302338\n",
            "Iteration 162, loss = 0.46294016\n",
            "Iteration 163, loss = 0.46290013\n",
            "Iteration 164, loss = 0.46269953\n",
            "Iteration 165, loss = 0.46269321\n",
            "Iteration 166, loss = 0.46255517\n",
            "Iteration 167, loss = 0.46254855\n",
            "Iteration 168, loss = 0.46241578\n",
            "Iteration 169, loss = 0.46232346\n",
            "Iteration 170, loss = 0.46219552\n",
            "Iteration 171, loss = 0.46216195\n",
            "Iteration 172, loss = 0.46200454\n",
            "Iteration 173, loss = 0.46197712\n",
            "Iteration 174, loss = 0.46191014\n",
            "Iteration 175, loss = 0.46180716\n",
            "Iteration 176, loss = 0.46167673\n",
            "Iteration 177, loss = 0.46167049\n",
            "Iteration 178, loss = 0.46156570\n",
            "Iteration 179, loss = 0.46150822\n",
            "Iteration 180, loss = 0.46150464\n",
            "Iteration 181, loss = 0.46130181\n",
            "Iteration 182, loss = 0.46123898\n",
            "Iteration 183, loss = 0.46113985\n",
            "Iteration 184, loss = 0.46111212\n",
            "Iteration 185, loss = 0.46103217\n",
            "Iteration 186, loss = 0.46091795\n",
            "Iteration 187, loss = 0.46086841\n",
            "Iteration 188, loss = 0.46079827\n",
            "Iteration 189, loss = 0.46069130\n",
            "Iteration 190, loss = 0.46062259\n",
            "Iteration 191, loss = 0.46057418\n",
            "Iteration 192, loss = 0.46043679\n",
            "Iteration 193, loss = 0.46038118\n",
            "Iteration 194, loss = 0.46035235\n",
            "Iteration 195, loss = 0.46027117\n",
            "Iteration 196, loss = 0.46021685\n",
            "Iteration 197, loss = 0.46031698\n",
            "Iteration 198, loss = 0.46007572\n",
            "Iteration 199, loss = 0.46007314\n",
            "Iteration 200, loss = 0.45989141\n",
            "Iteration 1, loss = 0.67417311\n",
            "Iteration 2, loss = 0.66117417\n",
            "Iteration 3, loss = 0.64851378\n",
            "Iteration 4, loss = 0.63691252\n",
            "Iteration 5, loss = 0.62624817\n",
            "Iteration 6, loss = 0.61658730\n",
            "Iteration 7, loss = 0.60770170\n",
            "Iteration 8, loss = 0.59919135\n",
            "Iteration 9, loss = 0.59143999\n",
            "Iteration 10, loss = 0.58433799\n",
            "Iteration 11, loss = 0.57780191\n",
            "Iteration 12, loss = 0.57203100\n",
            "Iteration 13, loss = 0.56676375\n",
            "Iteration 14, loss = 0.56205667\n",
            "Iteration 15, loss = 0.55766473\n",
            "Iteration 16, loss = 0.55364612\n",
            "Iteration 17, loss = 0.54983892\n",
            "Iteration 18, loss = 0.54630699\n",
            "Iteration 19, loss = 0.54301175\n",
            "Iteration 20, loss = 0.53993742\n",
            "Iteration 21, loss = 0.53702456\n",
            "Iteration 22, loss = 0.53436869\n",
            "Iteration 23, loss = 0.53174211\n",
            "Iteration 24, loss = 0.52936944\n",
            "Iteration 25, loss = 0.52705029\n",
            "Iteration 26, loss = 0.52496720\n",
            "Iteration 27, loss = 0.52300471\n",
            "Iteration 28, loss = 0.52100390\n",
            "Iteration 29, loss = 0.51918249\n",
            "Iteration 30, loss = 0.51741379\n",
            "Iteration 31, loss = 0.51575632\n",
            "Iteration 32, loss = 0.51412422\n",
            "Iteration 33, loss = 0.51263412\n",
            "Iteration 34, loss = 0.51115449\n",
            "Iteration 35, loss = 0.50982468\n",
            "Iteration 36, loss = 0.50849448\n",
            "Iteration 37, loss = 0.50715523\n",
            "Iteration 38, loss = 0.50594808\n",
            "Iteration 39, loss = 0.50471591\n",
            "Iteration 40, loss = 0.50350492\n",
            "Iteration 41, loss = 0.50235828\n",
            "Iteration 42, loss = 0.50136168\n",
            "Iteration 43, loss = 0.50028941\n",
            "Iteration 44, loss = 0.49937110\n",
            "Iteration 45, loss = 0.49853634\n",
            "Iteration 46, loss = 0.49752910\n",
            "Iteration 47, loss = 0.49673866\n",
            "Iteration 48, loss = 0.49586715\n",
            "Iteration 49, loss = 0.49510848\n",
            "Iteration 50, loss = 0.49437125\n",
            "Iteration 51, loss = 0.49360531\n",
            "Iteration 52, loss = 0.49290616\n",
            "Iteration 53, loss = 0.49230842\n",
            "Iteration 54, loss = 0.49162911\n",
            "Iteration 55, loss = 0.49099700\n",
            "Iteration 56, loss = 0.49034188\n",
            "Iteration 57, loss = 0.48977401\n",
            "Iteration 58, loss = 0.48928612\n",
            "Iteration 59, loss = 0.48867742\n",
            "Iteration 60, loss = 0.48814584\n",
            "Iteration 61, loss = 0.48764631\n",
            "Iteration 62, loss = 0.48714850\n",
            "Iteration 63, loss = 0.48669746\n",
            "Iteration 64, loss = 0.48623280\n",
            "Iteration 65, loss = 0.48582434\n",
            "Iteration 66, loss = 0.48536849\n",
            "Iteration 67, loss = 0.48495788\n",
            "Iteration 68, loss = 0.48462370\n",
            "Iteration 69, loss = 0.48422386\n",
            "Iteration 70, loss = 0.48384353\n",
            "Iteration 71, loss = 0.48342002\n",
            "Iteration 72, loss = 0.48307593\n",
            "Iteration 73, loss = 0.48273848\n",
            "Iteration 74, loss = 0.48241714\n",
            "Iteration 75, loss = 0.48207451\n",
            "Iteration 76, loss = 0.48173703\n",
            "Iteration 77, loss = 0.48148706\n",
            "Iteration 78, loss = 0.48112202\n",
            "Iteration 79, loss = 0.48081537\n",
            "Iteration 80, loss = 0.48062336\n",
            "Iteration 81, loss = 0.48030641\n",
            "Iteration 82, loss = 0.48008947\n",
            "Iteration 83, loss = 0.47982358\n",
            "Iteration 84, loss = 0.47957980\n",
            "Iteration 85, loss = 0.47926933\n",
            "Iteration 86, loss = 0.47899942\n",
            "Iteration 87, loss = 0.47876572\n",
            "Iteration 88, loss = 0.47850494\n",
            "Iteration 89, loss = 0.47832177\n",
            "Iteration 90, loss = 0.47812178\n",
            "Iteration 91, loss = 0.47783111\n",
            "Iteration 92, loss = 0.47761203\n",
            "Iteration 93, loss = 0.47742579\n",
            "Iteration 94, loss = 0.47729259\n",
            "Iteration 95, loss = 0.47705340\n",
            "Iteration 96, loss = 0.47679565\n",
            "Iteration 97, loss = 0.47661056\n",
            "Iteration 98, loss = 0.47640494\n",
            "Iteration 99, loss = 0.47627885\n",
            "Iteration 100, loss = 0.47606003\n",
            "Iteration 101, loss = 0.47588358\n",
            "Iteration 102, loss = 0.47569959\n",
            "Iteration 103, loss = 0.47558312\n",
            "Iteration 104, loss = 0.47545868\n",
            "Iteration 105, loss = 0.47515263\n",
            "Iteration 106, loss = 0.47505570\n",
            "Iteration 107, loss = 0.47493008\n",
            "Iteration 108, loss = 0.47478335\n",
            "Iteration 109, loss = 0.47461657\n",
            "Iteration 110, loss = 0.47446031\n",
            "Iteration 111, loss = 0.47435724\n",
            "Iteration 112, loss = 0.47422534\n",
            "Iteration 113, loss = 0.47404185\n",
            "Iteration 114, loss = 0.47391981\n",
            "Iteration 115, loss = 0.47375630\n",
            "Iteration 116, loss = 0.47370110\n",
            "Iteration 117, loss = 0.47347120\n",
            "Iteration 118, loss = 0.47337687\n",
            "Iteration 119, loss = 0.47326383\n",
            "Iteration 120, loss = 0.47309879\n",
            "Iteration 121, loss = 0.47299181\n",
            "Iteration 122, loss = 0.47284273\n",
            "Iteration 123, loss = 0.47268596\n",
            "Iteration 124, loss = 0.47268213\n",
            "Iteration 125, loss = 0.47248038\n",
            "Iteration 126, loss = 0.47236639\n",
            "Iteration 127, loss = 0.47224450\n",
            "Iteration 128, loss = 0.47215371\n",
            "Iteration 129, loss = 0.47214686\n",
            "Iteration 130, loss = 0.47192591\n",
            "Iteration 131, loss = 0.47187430\n",
            "Iteration 132, loss = 0.47168477\n",
            "Iteration 133, loss = 0.47163241\n",
            "Iteration 134, loss = 0.47158448\n",
            "Iteration 135, loss = 0.47145032\n",
            "Iteration 136, loss = 0.47140042\n",
            "Iteration 137, loss = 0.47123210\n",
            "Iteration 138, loss = 0.47119577\n",
            "Iteration 139, loss = 0.47110409\n",
            "Iteration 140, loss = 0.47094966\n",
            "Iteration 141, loss = 0.47089127\n",
            "Iteration 142, loss = 0.47082238\n",
            "Iteration 143, loss = 0.47074324\n",
            "Iteration 144, loss = 0.47060101\n",
            "Iteration 145, loss = 0.47052546\n",
            "Iteration 146, loss = 0.47053253\n",
            "Iteration 147, loss = 0.47049826\n",
            "Iteration 148, loss = 0.47031837\n",
            "Iteration 149, loss = 0.47025620\n",
            "Iteration 150, loss = 0.47026553\n",
            "Iteration 151, loss = 0.47010585\n",
            "Iteration 152, loss = 0.46995580\n",
            "Iteration 153, loss = 0.46999944\n",
            "Iteration 154, loss = 0.46988434\n",
            "Iteration 155, loss = 0.46975529\n",
            "Iteration 156, loss = 0.46962778\n",
            "Iteration 157, loss = 0.46959683\n",
            "Iteration 158, loss = 0.46952044\n",
            "Iteration 159, loss = 0.46938174\n",
            "Iteration 160, loss = 0.46943816\n",
            "Iteration 161, loss = 0.46932291\n",
            "Iteration 162, loss = 0.46918992\n",
            "Iteration 163, loss = 0.46914123\n",
            "Iteration 164, loss = 0.46900902\n",
            "Iteration 165, loss = 0.46901141\n",
            "Iteration 166, loss = 0.46885893\n",
            "Iteration 167, loss = 0.46883426\n",
            "Iteration 168, loss = 0.46886464\n",
            "Iteration 169, loss = 0.46867373\n",
            "Iteration 170, loss = 0.46859331\n",
            "Iteration 171, loss = 0.46853703\n",
            "Iteration 172, loss = 0.46843503\n",
            "Iteration 173, loss = 0.46837751\n",
            "Iteration 174, loss = 0.46831009\n",
            "Iteration 175, loss = 0.46823794\n",
            "Iteration 176, loss = 0.46812954\n",
            "Iteration 177, loss = 0.46822488\n",
            "Iteration 178, loss = 0.46810803\n",
            "Iteration 179, loss = 0.46800343\n",
            "Iteration 180, loss = 0.46791833\n",
            "Iteration 181, loss = 0.46780593\n",
            "Iteration 182, loss = 0.46776407\n",
            "Iteration 183, loss = 0.46764176\n",
            "Iteration 184, loss = 0.46761840\n",
            "Iteration 185, loss = 0.46757867\n",
            "Iteration 186, loss = 0.46751947\n",
            "Iteration 187, loss = 0.46746785\n",
            "Iteration 188, loss = 0.46734364\n",
            "Iteration 189, loss = 0.46734639\n",
            "Iteration 190, loss = 0.46724894\n",
            "Iteration 191, loss = 0.46726982\n",
            "Iteration 192, loss = 0.46711196\n",
            "Iteration 193, loss = 0.46702510\n",
            "Iteration 194, loss = 0.46702754\n",
            "Iteration 195, loss = 0.46696786\n",
            "Iteration 196, loss = 0.46692135\n",
            "Iteration 197, loss = 0.46699854\n",
            "Iteration 198, loss = 0.46680392\n",
            "Iteration 199, loss = 0.46673826\n",
            "Iteration 200, loss = 0.46662056\n",
            "Iteration 1, loss = 0.67432182\n",
            "Iteration 2, loss = 0.66095882\n",
            "Iteration 3, loss = 0.64800044\n",
            "Iteration 4, loss = 0.63626860\n",
            "Iteration 5, loss = 0.62543999\n",
            "Iteration 6, loss = 0.61573305\n",
            "Iteration 7, loss = 0.60670235\n",
            "Iteration 8, loss = 0.59807500\n",
            "Iteration 9, loss = 0.59027246\n",
            "Iteration 10, loss = 0.58311609\n",
            "Iteration 11, loss = 0.57661132\n",
            "Iteration 12, loss = 0.57093712\n",
            "Iteration 13, loss = 0.56572562\n",
            "Iteration 14, loss = 0.56104879\n",
            "Iteration 15, loss = 0.55664496\n",
            "Iteration 16, loss = 0.55264521\n",
            "Iteration 17, loss = 0.54881983\n",
            "Iteration 18, loss = 0.54526203\n",
            "Iteration 19, loss = 0.54190268\n",
            "Iteration 20, loss = 0.53884126\n",
            "Iteration 21, loss = 0.53586929\n",
            "Iteration 22, loss = 0.53316071\n",
            "Iteration 23, loss = 0.53044581\n",
            "Iteration 24, loss = 0.52803851\n",
            "Iteration 25, loss = 0.52558646\n",
            "Iteration 26, loss = 0.52346217\n",
            "Iteration 27, loss = 0.52141722\n",
            "Iteration 28, loss = 0.51929785\n",
            "Iteration 29, loss = 0.51739117\n",
            "Iteration 30, loss = 0.51554474\n",
            "Iteration 31, loss = 0.51380082\n",
            "Iteration 32, loss = 0.51218988\n",
            "Iteration 33, loss = 0.51056574\n",
            "Iteration 34, loss = 0.50902679\n",
            "Iteration 35, loss = 0.50760440\n",
            "Iteration 36, loss = 0.50626172\n",
            "Iteration 37, loss = 0.50486145\n",
            "Iteration 38, loss = 0.50363462\n",
            "Iteration 39, loss = 0.50230786\n",
            "Iteration 40, loss = 0.50103660\n",
            "Iteration 41, loss = 0.49987004\n",
            "Iteration 42, loss = 0.49876703\n",
            "Iteration 43, loss = 0.49770508\n",
            "Iteration 44, loss = 0.49675787\n",
            "Iteration 45, loss = 0.49587129\n",
            "Iteration 46, loss = 0.49487090\n",
            "Iteration 47, loss = 0.49409244\n",
            "Iteration 48, loss = 0.49327028\n",
            "Iteration 49, loss = 0.49248701\n",
            "Iteration 50, loss = 0.49172846\n",
            "Iteration 51, loss = 0.49097052\n",
            "Iteration 52, loss = 0.49029089\n",
            "Iteration 53, loss = 0.48974246\n",
            "Iteration 54, loss = 0.48895407\n",
            "Iteration 55, loss = 0.48834023\n",
            "Iteration 56, loss = 0.48769688\n",
            "Iteration 57, loss = 0.48709318\n",
            "Iteration 58, loss = 0.48653117\n",
            "Iteration 59, loss = 0.48600428\n",
            "Iteration 60, loss = 0.48544564\n",
            "Iteration 61, loss = 0.48491998\n",
            "Iteration 62, loss = 0.48445417\n",
            "Iteration 63, loss = 0.48393372\n",
            "Iteration 64, loss = 0.48349434\n",
            "Iteration 65, loss = 0.48315123\n",
            "Iteration 66, loss = 0.48264897\n",
            "Iteration 67, loss = 0.48216422\n",
            "Iteration 68, loss = 0.48179806\n",
            "Iteration 69, loss = 0.48141510\n",
            "Iteration 70, loss = 0.48111519\n",
            "Iteration 71, loss = 0.48058884\n",
            "Iteration 72, loss = 0.48027926\n",
            "Iteration 73, loss = 0.47993699\n",
            "Iteration 74, loss = 0.47960113\n",
            "Iteration 75, loss = 0.47921027\n",
            "Iteration 76, loss = 0.47888438\n",
            "Iteration 77, loss = 0.47862110\n",
            "Iteration 78, loss = 0.47822777\n",
            "Iteration 79, loss = 0.47791644\n",
            "Iteration 80, loss = 0.47766338\n",
            "Iteration 81, loss = 0.47738630\n",
            "Iteration 82, loss = 0.47709108\n",
            "Iteration 83, loss = 0.47681382\n",
            "Iteration 84, loss = 0.47663855\n",
            "Iteration 85, loss = 0.47625868\n",
            "Iteration 86, loss = 0.47601131\n",
            "Iteration 87, loss = 0.47576971\n",
            "Iteration 88, loss = 0.47551886\n",
            "Iteration 89, loss = 0.47527369\n",
            "Iteration 90, loss = 0.47508271\n",
            "Iteration 91, loss = 0.47478896\n",
            "Iteration 92, loss = 0.47460682\n",
            "Iteration 93, loss = 0.47438322\n",
            "Iteration 94, loss = 0.47428646\n",
            "Iteration 95, loss = 0.47393627\n",
            "Iteration 96, loss = 0.47370673\n",
            "Iteration 97, loss = 0.47348313\n",
            "Iteration 98, loss = 0.47329434\n",
            "Iteration 99, loss = 0.47312419\n",
            "Iteration 100, loss = 0.47291116\n",
            "Iteration 101, loss = 0.47266415\n",
            "Iteration 102, loss = 0.47251792\n",
            "Iteration 103, loss = 0.47236172\n",
            "Iteration 104, loss = 0.47220949\n",
            "Iteration 105, loss = 0.47190979\n",
            "Iteration 106, loss = 0.47179214\n",
            "Iteration 107, loss = 0.47162239\n",
            "Iteration 108, loss = 0.47150814\n",
            "Iteration 109, loss = 0.47130957\n",
            "Iteration 110, loss = 0.47116103\n",
            "Iteration 111, loss = 0.47097501\n",
            "Iteration 112, loss = 0.47086065\n",
            "Iteration 113, loss = 0.47063684\n",
            "Iteration 114, loss = 0.47047648\n",
            "Iteration 115, loss = 0.47034760\n",
            "Iteration 116, loss = 0.47023538\n",
            "Iteration 117, loss = 0.46999576\n",
            "Iteration 118, loss = 0.46985885\n",
            "Iteration 119, loss = 0.46971113\n",
            "Iteration 120, loss = 0.46952787\n",
            "Iteration 121, loss = 0.46940366\n",
            "Iteration 122, loss = 0.46923470\n",
            "Iteration 123, loss = 0.46906716\n",
            "Iteration 124, loss = 0.46899171\n",
            "Iteration 125, loss = 0.46884124\n",
            "Iteration 126, loss = 0.46875620\n",
            "Iteration 127, loss = 0.46864413\n",
            "Iteration 128, loss = 0.46846416\n",
            "Iteration 129, loss = 0.46840421\n",
            "Iteration 130, loss = 0.46823711\n",
            "Iteration 131, loss = 0.46815552\n",
            "Iteration 132, loss = 0.46795538\n",
            "Iteration 133, loss = 0.46788375\n",
            "Iteration 134, loss = 0.46781889\n",
            "Iteration 135, loss = 0.46771792\n",
            "Iteration 136, loss = 0.46762848\n",
            "Iteration 137, loss = 0.46741903\n",
            "Iteration 138, loss = 0.46738604\n",
            "Iteration 139, loss = 0.46724834\n",
            "Iteration 140, loss = 0.46710987\n",
            "Iteration 141, loss = 0.46704827\n",
            "Iteration 142, loss = 0.46698255\n",
            "Iteration 143, loss = 0.46680935\n",
            "Iteration 144, loss = 0.46664470\n",
            "Iteration 145, loss = 0.46663710\n",
            "Iteration 146, loss = 0.46657334\n",
            "Iteration 147, loss = 0.46651878\n",
            "Iteration 148, loss = 0.46636818\n",
            "Iteration 149, loss = 0.46629646\n",
            "Iteration 150, loss = 0.46622348\n",
            "Iteration 151, loss = 0.46605817\n",
            "Iteration 152, loss = 0.46593095\n",
            "Iteration 153, loss = 0.46591594\n",
            "Iteration 154, loss = 0.46580127\n",
            "Iteration 155, loss = 0.46568403\n",
            "Iteration 156, loss = 0.46557884\n",
            "Iteration 157, loss = 0.46550157\n",
            "Iteration 158, loss = 0.46545430\n",
            "Iteration 159, loss = 0.46530582\n",
            "Iteration 160, loss = 0.46526578\n",
            "Iteration 161, loss = 0.46517339\n",
            "Iteration 162, loss = 0.46501330\n",
            "Iteration 163, loss = 0.46499707\n",
            "Iteration 164, loss = 0.46485615\n",
            "Iteration 165, loss = 0.46482788\n",
            "Iteration 166, loss = 0.46466926\n",
            "Iteration 167, loss = 0.46468569\n",
            "Iteration 168, loss = 0.46460991\n",
            "Iteration 169, loss = 0.46446336\n",
            "Iteration 170, loss = 0.46435722\n",
            "Iteration 171, loss = 0.46437206\n",
            "Iteration 172, loss = 0.46416224\n",
            "Iteration 173, loss = 0.46412082\n",
            "Iteration 174, loss = 0.46400980\n",
            "Iteration 175, loss = 0.46394470\n",
            "Iteration 176, loss = 0.46381644\n",
            "Iteration 177, loss = 0.46383345\n",
            "Iteration 178, loss = 0.46374978\n",
            "Iteration 179, loss = 0.46366632\n",
            "Iteration 180, loss = 0.46351127\n",
            "Iteration 181, loss = 0.46351863\n",
            "Iteration 182, loss = 0.46338009\n",
            "Iteration 183, loss = 0.46329096\n",
            "Iteration 184, loss = 0.46324705\n",
            "Iteration 185, loss = 0.46322204\n",
            "Iteration 186, loss = 0.46309388\n",
            "Iteration 187, loss = 0.46301580\n",
            "Iteration 188, loss = 0.46293109\n",
            "Iteration 189, loss = 0.46286596\n",
            "Iteration 190, loss = 0.46282784\n",
            "Iteration 191, loss = 0.46283602\n",
            "Iteration 192, loss = 0.46266787\n",
            "Iteration 193, loss = 0.46260380\n",
            "Iteration 194, loss = 0.46255035\n",
            "Iteration 195, loss = 0.46253963\n",
            "Iteration 196, loss = 0.46244392\n",
            "Iteration 197, loss = 0.46250988\n",
            "Iteration 198, loss = 0.46233178\n",
            "Iteration 199, loss = 0.46223584\n",
            "Iteration 200, loss = 0.46214216\n",
            "Confusion Matrix\n",
            "[[2906  783]\n",
            " [1024 3391]]\n",
            "Attractive  -1.0   1.0\n",
            "row_0                 \n",
            "-1.0        2892   788\n",
            " 1.0        1038  3386\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        -1.0       0.74      0.79      0.76      3689\n",
            "         1.0       0.81      0.77      0.79      4415\n",
            "\n",
            "    accuracy                           0.78      8104\n",
            "   macro avg       0.78      0.78      0.78      8104\n",
            "weighted avg       0.78      0.78      0.78      8104\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmvc2w3bOp09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}