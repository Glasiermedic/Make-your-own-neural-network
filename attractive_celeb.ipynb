{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attractive_celeb.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Glasiermedic/Make-your-own-neural-network/blob/master/attractive_celeb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wBqGJnFObzX",
        "colab_type": "code",
        "outputId": "2cbdd140-becb-4736-ef3f-62408d319c1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "!pip3 install seaborn==0.9.0\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import ensemble\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_style('white')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seaborn==0.9.0 in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0) (0.24.2)\n",
            "Requirement already satisfied: matplotlib>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0) (3.0.3)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.9.3 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0) (1.16.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn==0.9.0) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn==0.9.0) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0) (2.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas>=0.15.2->seaborn==0.9.0) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn==0.9.0) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "migx0sciQiLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "celebs = pd.read_csv(\"https://raw.githubusercontent.com/Glasiermedic/Make-your-own-neural-network/master/list_attr_celeba.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx4k2vm53FpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "celeb = celebs.sample(frac=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB6NHnyHT4Kq",
        "colab_type": "code",
        "outputId": "b141eb4c-f292-4e56-dd89-228dfcd8783b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 822
        }
      },
      "source": [
        "celeb.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 40520 entries, 51230 to 68707\n",
            "Data columns (total 41 columns):\n",
            "image_id               40520 non-null object\n",
            "5_o_Clock_Shadow       40520 non-null int64\n",
            "Arched_Eyebrows        40520 non-null int64\n",
            "Attractive             40520 non-null int64\n",
            "Bags_Under_Eyes        40520 non-null int64\n",
            "Bald                   40520 non-null int64\n",
            "Bangs                  40520 non-null int64\n",
            "Big_Lips               40520 non-null int64\n",
            "Big_Nose               40520 non-null int64\n",
            "Black_Hair             40520 non-null int64\n",
            "Blond_Hair             40520 non-null int64\n",
            "Blurry                 40520 non-null int64\n",
            "Brown_Hair             40520 non-null int64\n",
            "Bushy_Eyebrows         40520 non-null int64\n",
            "Chubby                 40520 non-null int64\n",
            "Double_Chin            40520 non-null int64\n",
            "Eyeglasses             40520 non-null int64\n",
            "Goatee                 40520 non-null int64\n",
            "Gray_Hair              40520 non-null int64\n",
            "Heavy_Makeup           40520 non-null int64\n",
            "High_Cheekbones        40520 non-null int64\n",
            "Male                   40520 non-null int64\n",
            "Mouth_Slightly_Open    40520 non-null int64\n",
            "Mustache               40520 non-null int64\n",
            "Narrow_Eyes            40520 non-null int64\n",
            "No_Beard               40520 non-null int64\n",
            "Oval_Face              40520 non-null int64\n",
            "Pale_Skin              40520 non-null int64\n",
            "Pointy_Nose            40520 non-null int64\n",
            "Receding_Hairline      40520 non-null int64\n",
            "Rosy_Cheeks            40520 non-null int64\n",
            "Sideburns              40520 non-null int64\n",
            "Smiling                40520 non-null int64\n",
            "Straight_Hair          40520 non-null int64\n",
            "Wavy_Hair              40520 non-null int64\n",
            "Wearing_Earrings       40520 non-null int64\n",
            "Wearing_Hat            40520 non-null int64\n",
            "Wearing_Lipstick       40520 non-null int64\n",
            "Wearing_Necklace       40520 non-null int64\n",
            "Wearing_Necktie        40520 non-null int64\n",
            "Young                  40520 non-null int64\n",
            "dtypes: int64(40), object(1)\n",
            "memory usage: 13.0+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBd0mv97-21x",
        "colab_type": "code",
        "outputId": "c942458a-6f0e-4455-a747-6729954aff04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "source": [
        "celeb.columns\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['image_id', '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive',\n",
              "       'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose',\n",
              "       'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows',\n",
              "       'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair',\n",
              "       'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open',\n",
              "       'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin',\n",
              "       'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns',\n",
              "       'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings',\n",
              "       'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace',\n",
              "       'Wearing_Necktie', 'Young'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUpWZ6JMqVBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "celeb = celeb.drop(['image_id'], axis = 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFXnDPnrpli6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfolds = StratifiedShuffleSplit(n_splits =10, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRECcWq83vXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = MinMaxScaler(copy=True, feature_range=(-1, 1))\n",
        "celeb_scal = scaler.fit_transform(celeb)\n",
        "celeb_1 = pd.DataFrame(celeb_scal)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvnvUleomCLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "celebs = celebs.drop(['image_id'], axis = 1)\n",
        "celebs_scal = scaler.fit_transform(celebs)\n",
        "celebs1 =pd.DataFrame(celebs_scal)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYQp4DL1CsR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "celeb_1.columns = celeb.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlACp0r7Cvt4",
        "colab_type": "code",
        "outputId": "c6a9b3fa-a085-4bea-b750-ea127257cd43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "celeb_1.describe()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>5_o_Clock_Shadow</th>\n",
              "      <th>Arched_Eyebrows</th>\n",
              "      <th>Attractive</th>\n",
              "      <th>Bags_Under_Eyes</th>\n",
              "      <th>Bald</th>\n",
              "      <th>Bangs</th>\n",
              "      <th>Big_Lips</th>\n",
              "      <th>Big_Nose</th>\n",
              "      <th>Black_Hair</th>\n",
              "      <th>Blond_Hair</th>\n",
              "      <th>Blurry</th>\n",
              "      <th>Brown_Hair</th>\n",
              "      <th>Bushy_Eyebrows</th>\n",
              "      <th>Chubby</th>\n",
              "      <th>Double_Chin</th>\n",
              "      <th>Eyeglasses</th>\n",
              "      <th>Goatee</th>\n",
              "      <th>Gray_Hair</th>\n",
              "      <th>Heavy_Makeup</th>\n",
              "      <th>High_Cheekbones</th>\n",
              "      <th>Male</th>\n",
              "      <th>Mouth_Slightly_Open</th>\n",
              "      <th>Mustache</th>\n",
              "      <th>Narrow_Eyes</th>\n",
              "      <th>No_Beard</th>\n",
              "      <th>Oval_Face</th>\n",
              "      <th>Pale_Skin</th>\n",
              "      <th>Pointy_Nose</th>\n",
              "      <th>Receding_Hairline</th>\n",
              "      <th>Rosy_Cheeks</th>\n",
              "      <th>Sideburns</th>\n",
              "      <th>Smiling</th>\n",
              "      <th>Straight_Hair</th>\n",
              "      <th>Wavy_Hair</th>\n",
              "      <th>Wearing_Earrings</th>\n",
              "      <th>Wearing_Hat</th>\n",
              "      <th>Wearing_Lipstick</th>\n",
              "      <th>Wearing_Necklace</th>\n",
              "      <th>Wearing_Necktie</th>\n",
              "      <th>Young</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "      <td>40520.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-0.778332</td>\n",
              "      <td>-0.467029</td>\n",
              "      <td>0.034600</td>\n",
              "      <td>-0.592448</td>\n",
              "      <td>-0.955183</td>\n",
              "      <td>-0.697779</td>\n",
              "      <td>-0.515696</td>\n",
              "      <td>-0.535143</td>\n",
              "      <td>-0.520582</td>\n",
              "      <td>-0.698963</td>\n",
              "      <td>-0.898618</td>\n",
              "      <td>-0.595311</td>\n",
              "      <td>-0.719299</td>\n",
              "      <td>-0.888351</td>\n",
              "      <td>-0.907453</td>\n",
              "      <td>-0.871372</td>\n",
              "      <td>-0.878332</td>\n",
              "      <td>-0.919546</td>\n",
              "      <td>-0.222162</td>\n",
              "      <td>-0.086871</td>\n",
              "      <td>-0.170336</td>\n",
              "      <td>-0.033662</td>\n",
              "      <td>-0.916683</td>\n",
              "      <td>-0.768263</td>\n",
              "      <td>0.673198</td>\n",
              "      <td>-0.430800</td>\n",
              "      <td>-0.912043</td>\n",
              "      <td>-0.438598</td>\n",
              "      <td>-0.841658</td>\n",
              "      <td>-0.866683</td>\n",
              "      <td>-0.889388</td>\n",
              "      <td>-0.042892</td>\n",
              "      <td>-0.586772</td>\n",
              "      <td>-0.356170</td>\n",
              "      <td>-0.621767</td>\n",
              "      <td>-0.904442</td>\n",
              "      <td>-0.049309</td>\n",
              "      <td>-0.754886</td>\n",
              "      <td>-0.853504</td>\n",
              "      <td>0.552468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.627861</td>\n",
              "      <td>0.884253</td>\n",
              "      <td>0.999414</td>\n",
              "      <td>0.805619</td>\n",
              "      <td>0.296021</td>\n",
              "      <td>0.716322</td>\n",
              "      <td>0.856782</td>\n",
              "      <td>0.844772</td>\n",
              "      <td>0.853822</td>\n",
              "      <td>0.715166</td>\n",
              "      <td>0.438737</td>\n",
              "      <td>0.803505</td>\n",
              "      <td>0.694709</td>\n",
              "      <td>0.459170</td>\n",
              "      <td>0.420159</td>\n",
              "      <td>0.490629</td>\n",
              "      <td>0.478058</td>\n",
              "      <td>0.392987</td>\n",
              "      <td>0.975022</td>\n",
              "      <td>0.996232</td>\n",
              "      <td>0.985398</td>\n",
              "      <td>0.999446</td>\n",
              "      <td>0.399620</td>\n",
              "      <td>0.640143</td>\n",
              "      <td>0.739471</td>\n",
              "      <td>0.902459</td>\n",
              "      <td>0.410099</td>\n",
              "      <td>0.898694</td>\n",
              "      <td>0.540017</td>\n",
              "      <td>0.498865</td>\n",
              "      <td>0.457159</td>\n",
              "      <td>0.999092</td>\n",
              "      <td>0.809762</td>\n",
              "      <td>0.934433</td>\n",
              "      <td>0.783212</td>\n",
              "      <td>0.426601</td>\n",
              "      <td>0.998796</td>\n",
              "      <td>0.655864</td>\n",
              "      <td>0.521092</td>\n",
              "      <td>0.833544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       5_o_Clock_Shadow  Arched_Eyebrows  ...  Wearing_Necktie         Young\n",
              "count      40520.000000     40520.000000  ...     40520.000000  40520.000000\n",
              "mean          -0.778332        -0.467029  ...        -0.853504      0.552468\n",
              "std            0.627861         0.884253  ...         0.521092      0.833544\n",
              "min           -1.000000        -1.000000  ...        -1.000000     -1.000000\n",
              "25%           -1.000000        -1.000000  ...        -1.000000      1.000000\n",
              "50%           -1.000000        -1.000000  ...        -1.000000      1.000000\n",
              "75%           -1.000000         1.000000  ...        -1.000000      1.000000\n",
              "max            1.000000         1.000000  ...         1.000000      1.000000\n",
              "\n",
              "[8 rows x 40 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOGkJQAw1Jf8",
        "colab_type": "text"
      },
      "source": [
        "for col in celeb_1:\n",
        "  sns_plot = sns.distplot(celeb_1[col], label = col)\n",
        "  #plt.title(col)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSSOSxWp4Mkv",
        "colab_type": "text"
      },
      "source": [
        "%%time\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "for item in celeb_1.columns:\n",
        "  plt.figure(figsize=(15,5))\n",
        "  sns.scatterplot(x =celeb_1[item], y = celeb_1['Attractive'])\n",
        "  plt.title(item)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB85HpQENaGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = celeb_1.drop(['Attractive'], axis = 1)\n",
        "Y = celeb_1['Attractive']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_3uesupEoMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMUvr3qvOqFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K7g4_lZMtvz",
        "colab_type": "code",
        "outputId": "6c6606ae-a95e-4caf-b615-e37bce94c0be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        }
      },
      "source": [
        "forest = RandomForestClassifier( n_jobs=-1, class_weight='balanced', n_estimators = 500, max_features = 7, max_depth=12, random_state = 1)\n",
        "\n",
        "print(cross_val_score(forest,X_train, y_train, cv=kfolds, scoring = 'recall', verbose = 2))\n",
        "pred_y_sklearn =cross_val_predict(forest, X_test, y_test, cv=10)\n",
        "y_true = y_test\n",
        "y_pred = cross_val_predict(forest, X_test, y_test, cv=10)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(y_pred, y_true))\n",
        "print(pd.crosstab(pred_y_sklearn, y_test))\n",
        "print(\"\")\n",
        "print(classification_report(y_pred, y_true))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=  11.0s\n",
            "[CV]  ................................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.1s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ................................................. , total=   9.1s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   9.1s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   9.1s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   9.1s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   9.0s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   8.9s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   9.1s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   9.2s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   9.2s\n",
            "[0.77757794 0.77517986 0.78177458 0.76618705 0.78357314 0.77338129\n",
            " 0.76858513 0.76498801 0.77997602 0.78417266]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  1.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "[[3003  920]\n",
            " [ 818 3363]]\n",
            "Attractive  -1.0   1.0\n",
            "row_0                 \n",
            "-1.0        3003   920\n",
            " 1.0         818  3363\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        -1.0       0.79      0.77      0.78      3923\n",
            "         1.0       0.79      0.80      0.79      4181\n",
            "\n",
            "    accuracy                           0.79      8104\n",
            "   macro avg       0.79      0.78      0.79      8104\n",
            "weighted avg       0.79      0.79      0.79      8104\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1rFd3JfRkMH",
        "colab_type": "code",
        "outputId": "5310f854-c70b-45c2-f17e-3abf5bcad643",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "forest.fit(X_train, y_train)\n",
        "importances_for = forest.feature_importances_\n",
        "\n",
        "indices = np.argsort(importances_for)[::-1]\n",
        "for_imp_feat = []\n",
        "\n",
        "for f in range(X.shape[1]):\n",
        "    for_imp_feat.append(X.columns[indices[f]])\n",
        "\n",
        "importances_df = pd.Series(importances_for, index=X.columns)\n",
        "importances_df.nlargest(10).sort_values().plot(kind='barh', figsize=(12, 6)) \n",
        "plt.title(\"Random Forest Importances\")\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAx0AAAF4CAYAAAAxP5RTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7ertb2UUFDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = celeb_1.drop(['Attractive'], axis = 1)\n",
        "X_2 = X[list(for_imp_feat)[:20]]\n",
        "Y = celeb_1['Attractive']\n",
        "\n",
        "X3 =celebs.drop(['Attractive'],axis =1)\n",
        "Y3 =celebs['Attractive']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5K9MuLxU97U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X_2, Y, test_size=0.2, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YYEzUn9pzNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, Y3, test_size=0.2, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4sqpgAaXfcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcaUWrS0T41p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = MLPClassifier(alpha=0.01, hidden_layer_sizes=(5,100,5), activation='relu', solver='sgd', \n",
        "                    learning_rate='adaptive', warm_start=True,  verbose = 1,\n",
        "                    max_iter = 50, random_state=1, tol=0.0001)\n",
        "#mlp.fit(X_train, y_train)\n",
        "#mlp.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkaIDqqHYfkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cross_val_score(mlp, X, Y, cv=kfolds, verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MfioMDUaYmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha_list = [ .01, .05]\n",
        "layer_lists = [(5,100,5),(100,)]\n",
        "parameters = {'alpha':alpha_list, 'hidden_layer_sizes': layer_lists,\n",
        "                    'solver': [ 'sgd', 'adam']}\n",
        " #if marked out once it has ran and the best estimator is identified.  The best estimator is then put into the mlp algorithm below for the final model\n",
        "#mlp_grid = GridSearchCV(mlp, parameters, cv=kfolds, scoring='neg_mean_squared_error')\n",
        "#mlp_grid.fit(X3_train,y3_train)        \n",
        "#print (mlp_grid.best_estimator_)\n",
        "#print(np.sqrt(-mlp_grid.best_score_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJQgVm_rQxYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = MLPClassifier(alpha=0.01, hidden_layer_sizes=(5,100,5,5), activation='relu', solver='sgd', \n",
        "                    learning_rate='adaptive', warm_start=True,  verbose = 1,\n",
        "                    max_iter = 200, random_state=1, tol=0.0001)\n",
        "#mlp.fit(X_train, y_train)\n",
        "#mlp.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmvc2w3bOp09",
        "colab_type": "code",
        "outputId": "9d65d341-a0c7-4974-c9cd-86b42926cb21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(cross_val_score(mlp,X3_train, y3_train, cv=kfolds, scoring = 'neg_mean_squared_error', verbose = 1))\n",
        "pred3_y_sklearn =cross_val_predict(mlp, X3_test, y3_test, cv=5)\n",
        "y3_true = y_test\n",
        "y3_pred = cross_val_predict(mlp, X3_test, y3_test, cv=5)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(y3_pred, y3_true))\n",
        "print(pd.crosstab(pred3_y_sklearn, y3_test))\n",
        "print(\"\")\n",
        "print(classification_report(y3_pred, y3_true))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.69718094\n",
            "Iteration 2, loss = 0.69156712\n",
            "Iteration 3, loss = 0.65857057\n",
            "Iteration 4, loss = 0.57186286\n",
            "Iteration 5, loss = 0.53695682\n",
            "Iteration 6, loss = 0.51291211\n",
            "Iteration 7, loss = 0.49193370\n",
            "Iteration 8, loss = 0.47734793\n",
            "Iteration 9, loss = 0.46900328\n",
            "Iteration 10, loss = 0.46393210\n",
            "Iteration 11, loss = 0.46050992\n",
            "Iteration 12, loss = 0.45803563\n",
            "Iteration 13, loss = 0.45609528\n",
            "Iteration 14, loss = 0.45446113\n",
            "Iteration 15, loss = 0.45315905\n",
            "Iteration 16, loss = 0.45208735\n",
            "Iteration 17, loss = 0.45110789\n",
            "Iteration 18, loss = 0.45027794\n",
            "Iteration 19, loss = 0.44965613\n",
            "Iteration 20, loss = 0.44905115\n",
            "Iteration 21, loss = 0.44857952\n",
            "Iteration 22, loss = 0.44805294\n",
            "Iteration 23, loss = 0.44760719\n",
            "Iteration 24, loss = 0.44717200\n",
            "Iteration 25, loss = 0.44678241\n",
            "Iteration 26, loss = 0.44648444\n",
            "Iteration 27, loss = 0.44620198\n",
            "Iteration 28, loss = 0.44590723\n",
            "Iteration 29, loss = 0.44564931\n",
            "Iteration 30, loss = 0.44533193\n",
            "Iteration 31, loss = 0.44514806\n",
            "Iteration 32, loss = 0.44486704\n",
            "Iteration 33, loss = 0.44463882\n",
            "Iteration 34, loss = 0.44447886\n",
            "Iteration 35, loss = 0.44436627\n",
            "Iteration 36, loss = 0.44410292\n",
            "Iteration 37, loss = 0.44387629\n",
            "Iteration 38, loss = 0.44376180\n",
            "Iteration 39, loss = 0.44351309\n",
            "Iteration 40, loss = 0.44325742\n",
            "Iteration 41, loss = 0.44321044\n",
            "Iteration 42, loss = 0.44299928\n",
            "Iteration 43, loss = 0.44284298\n",
            "Iteration 44, loss = 0.44278762\n",
            "Iteration 45, loss = 0.44270437\n",
            "Iteration 46, loss = 0.44247602\n",
            "Iteration 47, loss = 0.44223693\n",
            "Iteration 48, loss = 0.44222359\n",
            "Iteration 49, loss = 0.44202283\n",
            "Iteration 50, loss = 0.44198079\n",
            "Iteration 51, loss = 0.44182528\n",
            "Iteration 52, loss = 0.44173263\n",
            "Iteration 53, loss = 0.44161178\n",
            "Iteration 54, loss = 0.44144058\n",
            "Iteration 55, loss = 0.44134619\n",
            "Iteration 56, loss = 0.44130604\n",
            "Iteration 57, loss = 0.44115798\n",
            "Iteration 58, loss = 0.44104805\n",
            "Iteration 59, loss = 0.44099686\n",
            "Iteration 60, loss = 0.44089606\n",
            "Iteration 61, loss = 0.44077824\n",
            "Iteration 62, loss = 0.44075703\n",
            "Iteration 63, loss = 0.44068709\n",
            "Iteration 64, loss = 0.44064708\n",
            "Iteration 65, loss = 0.44049859\n",
            "Iteration 66, loss = 0.44043907\n",
            "Iteration 67, loss = 0.44040558\n",
            "Iteration 68, loss = 0.44032964\n",
            "Iteration 69, loss = 0.44032589\n",
            "Iteration 70, loss = 0.44026711\n",
            "Iteration 71, loss = 0.44009777\n",
            "Iteration 72, loss = 0.44003535\n",
            "Iteration 73, loss = 0.44006250\n",
            "Iteration 74, loss = 0.43990638\n",
            "Iteration 75, loss = 0.43985395\n",
            "Iteration 76, loss = 0.43987038\n",
            "Iteration 77, loss = 0.43982662\n",
            "Iteration 78, loss = 0.43976004\n",
            "Iteration 79, loss = 0.43966152\n",
            "Iteration 80, loss = 0.43965504\n",
            "Iteration 81, loss = 0.43966335\n",
            "Iteration 82, loss = 0.43963672\n",
            "Iteration 83, loss = 0.43946947\n",
            "Iteration 84, loss = 0.43945189\n",
            "Iteration 85, loss = 0.43947515\n",
            "Iteration 86, loss = 0.43946944\n",
            "Iteration 87, loss = 0.43937391\n",
            "Iteration 88, loss = 0.43936657\n",
            "Iteration 89, loss = 0.43938890\n",
            "Iteration 90, loss = 0.43924958\n",
            "Iteration 91, loss = 0.43932089\n",
            "Iteration 92, loss = 0.43924452\n",
            "Iteration 93, loss = 0.43921320\n",
            "Iteration 94, loss = 0.43916991\n",
            "Iteration 95, loss = 0.43916718\n",
            "Iteration 96, loss = 0.43904792\n",
            "Iteration 97, loss = 0.43910866\n",
            "Iteration 98, loss = 0.43918753\n",
            "Iteration 99, loss = 0.43901285\n",
            "Iteration 100, loss = 0.43892120\n",
            "Iteration 101, loss = 0.43891845\n",
            "Iteration 102, loss = 0.43891519\n",
            "Iteration 103, loss = 0.43897562\n",
            "Iteration 104, loss = 0.43883060\n",
            "Iteration 105, loss = 0.43884434\n",
            "Iteration 106, loss = 0.43873061\n",
            "Iteration 107, loss = 0.43875327\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 108, loss = 0.43835738\n",
            "Iteration 109, loss = 0.43837882\n",
            "Iteration 110, loss = 0.43834955\n",
            "Iteration 111, loss = 0.43833806\n",
            "Iteration 112, loss = 0.43830684\n",
            "Iteration 113, loss = 0.43833510\n",
            "Iteration 114, loss = 0.43831186\n",
            "Iteration 115, loss = 0.43830375\n",
            "Iteration 116, loss = 0.43832128\n",
            "Iteration 117, loss = 0.43827735\n",
            "Iteration 118, loss = 0.43831763\n",
            "Iteration 119, loss = 0.43826227\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 120, loss = 0.43821800\n",
            "Iteration 121, loss = 0.43817604\n",
            "Iteration 122, loss = 0.43816941\n",
            "Iteration 123, loss = 0.43816584\n",
            "Iteration 124, loss = 0.43817413\n",
            "Iteration 125, loss = 0.43817012\n",
            "Iteration 126, loss = 0.43817665\n",
            "Iteration 127, loss = 0.43817042\n",
            "Iteration 128, loss = 0.43816452\n",
            "Iteration 129, loss = 0.43817153\n",
            "Iteration 130, loss = 0.43815029\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 131, loss = 0.43815573\n",
            "Iteration 132, loss = 0.43814292\n",
            "Iteration 133, loss = 0.43814057\n",
            "Iteration 134, loss = 0.43814093\n",
            "Iteration 135, loss = 0.43814064\n",
            "Iteration 136, loss = 0.43814290\n",
            "Iteration 137, loss = 0.43813944\n",
            "Iteration 138, loss = 0.43813992\n",
            "Iteration 139, loss = 0.43813953\n",
            "Iteration 140, loss = 0.43814051\n",
            "Iteration 141, loss = 0.43813724\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 142, loss = 0.43813536\n",
            "Iteration 143, loss = 0.43813392\n",
            "Iteration 144, loss = 0.43813324\n",
            "Iteration 145, loss = 0.43813297\n",
            "Iteration 146, loss = 0.43813306\n",
            "Iteration 147, loss = 0.43813276\n",
            "Iteration 148, loss = 0.43813296\n",
            "Iteration 149, loss = 0.43813260\n",
            "Iteration 150, loss = 0.43813276\n",
            "Iteration 151, loss = 0.43813267\n",
            "Iteration 152, loss = 0.43813243\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 153, loss = 0.43813138\n",
            "Iteration 154, loss = 0.43813138\n",
            "Iteration 155, loss = 0.43813133\n",
            "Iteration 156, loss = 0.43813128\n",
            "Iteration 157, loss = 0.43813137\n",
            "Iteration 158, loss = 0.43813129\n",
            "Iteration 159, loss = 0.43813134\n",
            "Iteration 160, loss = 0.43813130\n",
            "Iteration 161, loss = 0.43813124\n",
            "Iteration 162, loss = 0.43813125\n",
            "Iteration 163, loss = 0.43813125\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.69717092\n",
            "Iteration 2, loss = 0.69145754\n",
            "Iteration 3, loss = 0.65669553\n",
            "Iteration 4, loss = 0.57002803\n",
            "Iteration 5, loss = 0.53575214\n",
            "Iteration 6, loss = 0.51227395\n",
            "Iteration 7, loss = 0.49151984\n",
            "Iteration 8, loss = 0.47703357\n",
            "Iteration 9, loss = 0.46875247\n",
            "Iteration 10, loss = 0.46368464\n",
            "Iteration 11, loss = 0.46032910\n",
            "Iteration 12, loss = 0.45779182\n",
            "Iteration 13, loss = 0.45588727\n",
            "Iteration 14, loss = 0.45423876\n",
            "Iteration 15, loss = 0.45292294\n",
            "Iteration 16, loss = 0.45176747\n",
            "Iteration 17, loss = 0.45092345\n",
            "Iteration 18, loss = 0.45015415\n",
            "Iteration 19, loss = 0.44947631\n",
            "Iteration 20, loss = 0.44883739\n",
            "Iteration 21, loss = 0.44829925\n",
            "Iteration 22, loss = 0.44780648\n",
            "Iteration 23, loss = 0.44730974\n",
            "Iteration 24, loss = 0.44700589\n",
            "Iteration 25, loss = 0.44666175\n",
            "Iteration 26, loss = 0.44632069\n",
            "Iteration 27, loss = 0.44595236\n",
            "Iteration 28, loss = 0.44567645\n",
            "Iteration 29, loss = 0.44548581\n",
            "Iteration 30, loss = 0.44517401\n",
            "Iteration 31, loss = 0.44491936\n",
            "Iteration 32, loss = 0.44463129\n",
            "Iteration 33, loss = 0.44447693\n",
            "Iteration 34, loss = 0.44424658\n",
            "Iteration 35, loss = 0.44404157\n",
            "Iteration 36, loss = 0.44384697\n",
            "Iteration 37, loss = 0.44371686\n",
            "Iteration 38, loss = 0.44348536\n",
            "Iteration 39, loss = 0.44328672\n",
            "Iteration 40, loss = 0.44307113\n",
            "Iteration 41, loss = 0.44290760\n",
            "Iteration 42, loss = 0.44278687\n",
            "Iteration 43, loss = 0.44266579\n",
            "Iteration 44, loss = 0.44252569\n",
            "Iteration 45, loss = 0.44233164\n",
            "Iteration 46, loss = 0.44213199\n",
            "Iteration 47, loss = 0.44197373\n",
            "Iteration 48, loss = 0.44185890\n",
            "Iteration 49, loss = 0.44170246\n",
            "Iteration 50, loss = 0.44160926\n",
            "Iteration 51, loss = 0.44153572\n",
            "Iteration 52, loss = 0.44140944\n",
            "Iteration 53, loss = 0.44132860\n",
            "Iteration 54, loss = 0.44112571\n",
            "Iteration 55, loss = 0.44101864\n",
            "Iteration 56, loss = 0.44099070\n",
            "Iteration 57, loss = 0.44082015\n",
            "Iteration 58, loss = 0.44081552\n",
            "Iteration 59, loss = 0.44067091\n",
            "Iteration 60, loss = 0.44058366\n",
            "Iteration 61, loss = 0.44052397\n",
            "Iteration 62, loss = 0.44054869\n",
            "Iteration 63, loss = 0.44031415\n",
            "Iteration 64, loss = 0.44037553\n",
            "Iteration 65, loss = 0.44016583\n",
            "Iteration 66, loss = 0.44007160\n",
            "Iteration 67, loss = 0.44012914\n",
            "Iteration 68, loss = 0.43994537\n",
            "Iteration 69, loss = 0.43992376\n",
            "Iteration 70, loss = 0.43978130\n",
            "Iteration 71, loss = 0.43984681\n",
            "Iteration 72, loss = 0.43969564\n",
            "Iteration 73, loss = 0.43974735\n",
            "Iteration 74, loss = 0.43956508\n",
            "Iteration 75, loss = 0.43953997\n",
            "Iteration 76, loss = 0.43948844\n",
            "Iteration 77, loss = 0.43938803\n",
            "Iteration 78, loss = 0.43946081\n",
            "Iteration 79, loss = 0.43962216\n",
            "Iteration 80, loss = 0.43936635\n",
            "Iteration 81, loss = 0.43924361\n",
            "Iteration 82, loss = 0.43920773\n",
            "Iteration 83, loss = 0.43924656\n",
            "Iteration 84, loss = 0.43915403\n",
            "Iteration 85, loss = 0.43906776\n",
            "Iteration 86, loss = 0.43908252\n",
            "Iteration 87, loss = 0.43902231\n",
            "Iteration 88, loss = 0.43904012\n",
            "Iteration 89, loss = 0.43894336\n",
            "Iteration 90, loss = 0.43888779\n",
            "Iteration 91, loss = 0.43891037\n",
            "Iteration 92, loss = 0.43877236\n",
            "Iteration 93, loss = 0.43886272\n",
            "Iteration 94, loss = 0.43879937\n",
            "Iteration 95, loss = 0.43870171\n",
            "Iteration 96, loss = 0.43872397\n",
            "Iteration 97, loss = 0.43866848\n",
            "Iteration 98, loss = 0.43867622\n",
            "Iteration 99, loss = 0.43856064\n",
            "Iteration 100, loss = 0.43858226\n",
            "Iteration 101, loss = 0.43864301\n",
            "Iteration 102, loss = 0.43853679\n",
            "Iteration 103, loss = 0.43855313\n",
            "Iteration 104, loss = 0.43854655\n",
            "Iteration 105, loss = 0.43835976\n",
            "Iteration 106, loss = 0.43843498\n",
            "Iteration 107, loss = 0.43840710\n",
            "Iteration 108, loss = 0.43830770\n",
            "Iteration 109, loss = 0.43825592\n",
            "Iteration 110, loss = 0.43830901\n",
            "Iteration 111, loss = 0.43827841\n",
            "Iteration 112, loss = 0.43820773\n",
            "Iteration 113, loss = 0.43816229\n",
            "Iteration 114, loss = 0.43815996\n",
            "Iteration 115, loss = 0.43819551\n",
            "Iteration 116, loss = 0.43813074\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 117, loss = 0.43777110\n",
            "Iteration 118, loss = 0.43772676\n",
            "Iteration 119, loss = 0.43777599\n",
            "Iteration 120, loss = 0.43773805\n",
            "Iteration 121, loss = 0.43775472\n",
            "Iteration 122, loss = 0.43774945\n",
            "Iteration 123, loss = 0.43773172\n",
            "Iteration 124, loss = 0.43774789\n",
            "Iteration 125, loss = 0.43770956\n",
            "Iteration 126, loss = 0.43766116\n",
            "Iteration 127, loss = 0.43771608\n",
            "Iteration 128, loss = 0.43769687\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 129, loss = 0.43762421\n",
            "Iteration 130, loss = 0.43759846\n",
            "Iteration 131, loss = 0.43759047\n",
            "Iteration 132, loss = 0.43760052\n",
            "Iteration 133, loss = 0.43759548\n",
            "Iteration 134, loss = 0.43759402\n",
            "Iteration 135, loss = 0.43759653\n",
            "Iteration 136, loss = 0.43759765\n",
            "Iteration 137, loss = 0.43759708\n",
            "Iteration 138, loss = 0.43758398\n",
            "Iteration 139, loss = 0.43758390\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 140, loss = 0.43757265\n",
            "Iteration 141, loss = 0.43756456\n",
            "Iteration 142, loss = 0.43756391\n",
            "Iteration 143, loss = 0.43756409\n",
            "Iteration 144, loss = 0.43756219\n",
            "Iteration 145, loss = 0.43756506\n",
            "Iteration 146, loss = 0.43756287\n",
            "Iteration 147, loss = 0.43756362\n",
            "Iteration 148, loss = 0.43755878\n",
            "Iteration 149, loss = 0.43756343\n",
            "Iteration 150, loss = 0.43756111\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 151, loss = 0.43755742\n",
            "Iteration 152, loss = 0.43755693\n",
            "Iteration 153, loss = 0.43755646\n",
            "Iteration 154, loss = 0.43755655\n",
            "Iteration 155, loss = 0.43755614\n",
            "Iteration 156, loss = 0.43755635\n",
            "Iteration 157, loss = 0.43755604\n",
            "Iteration 158, loss = 0.43755623\n",
            "Iteration 159, loss = 0.43755617\n",
            "Iteration 160, loss = 0.43755598\n",
            "Iteration 161, loss = 0.43755630\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 162, loss = 0.43755460\n",
            "Iteration 163, loss = 0.43755458\n",
            "Iteration 164, loss = 0.43755461\n",
            "Iteration 165, loss = 0.43755459\n",
            "Iteration 166, loss = 0.43755459\n",
            "Iteration 167, loss = 0.43755459\n",
            "Iteration 168, loss = 0.43755454\n",
            "Iteration 169, loss = 0.43755456\n",
            "Iteration 170, loss = 0.43755455\n",
            "Iteration 171, loss = 0.43755455\n",
            "Iteration 172, loss = 0.43755457\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.69707004\n",
            "Iteration 2, loss = 0.69148002\n",
            "Iteration 3, loss = 0.65721948\n",
            "Iteration 4, loss = 0.57050184\n",
            "Iteration 5, loss = 0.53605058\n",
            "Iteration 6, loss = 0.51206105\n",
            "Iteration 7, loss = 0.49102364\n",
            "Iteration 8, loss = 0.47652216\n",
            "Iteration 9, loss = 0.46820660\n",
            "Iteration 10, loss = 0.46327532\n",
            "Iteration 11, loss = 0.45993949\n",
            "Iteration 12, loss = 0.45742851\n",
            "Iteration 13, loss = 0.45550594\n",
            "Iteration 14, loss = 0.45380939\n",
            "Iteration 15, loss = 0.45251184\n",
            "Iteration 16, loss = 0.45148794\n",
            "Iteration 17, loss = 0.45052977\n",
            "Iteration 18, loss = 0.44969161\n",
            "Iteration 19, loss = 0.44901097\n",
            "Iteration 20, loss = 0.44843773\n",
            "Iteration 21, loss = 0.44790283\n",
            "Iteration 22, loss = 0.44738970\n",
            "Iteration 23, loss = 0.44692856\n",
            "Iteration 24, loss = 0.44657758\n",
            "Iteration 25, loss = 0.44622253\n",
            "Iteration 26, loss = 0.44580212\n",
            "Iteration 27, loss = 0.44558105\n",
            "Iteration 28, loss = 0.44528223\n",
            "Iteration 29, loss = 0.44495947\n",
            "Iteration 30, loss = 0.44471681\n",
            "Iteration 31, loss = 0.44456769\n",
            "Iteration 32, loss = 0.44420766\n",
            "Iteration 33, loss = 0.44402352\n",
            "Iteration 34, loss = 0.44379495\n",
            "Iteration 35, loss = 0.44354654\n",
            "Iteration 36, loss = 0.44335539\n",
            "Iteration 37, loss = 0.44316440\n",
            "Iteration 38, loss = 0.44299544\n",
            "Iteration 39, loss = 0.44278919\n",
            "Iteration 40, loss = 0.44268878\n",
            "Iteration 41, loss = 0.44237708\n",
            "Iteration 42, loss = 0.44228925\n",
            "Iteration 43, loss = 0.44220225\n",
            "Iteration 44, loss = 0.44199766\n",
            "Iteration 45, loss = 0.44186556\n",
            "Iteration 46, loss = 0.44170348\n",
            "Iteration 47, loss = 0.44155501\n",
            "Iteration 48, loss = 0.44140768\n",
            "Iteration 49, loss = 0.44123751\n",
            "Iteration 50, loss = 0.44108557\n",
            "Iteration 51, loss = 0.44100585\n",
            "Iteration 52, loss = 0.44092603\n",
            "Iteration 53, loss = 0.44087705\n",
            "Iteration 54, loss = 0.44078447\n",
            "Iteration 55, loss = 0.44061354\n",
            "Iteration 56, loss = 0.44053903\n",
            "Iteration 57, loss = 0.44042855\n",
            "Iteration 58, loss = 0.44035944\n",
            "Iteration 59, loss = 0.44033355\n",
            "Iteration 60, loss = 0.44009754\n",
            "Iteration 61, loss = 0.44018315\n",
            "Iteration 62, loss = 0.43997239\n",
            "Iteration 63, loss = 0.43993277\n",
            "Iteration 64, loss = 0.43991468\n",
            "Iteration 65, loss = 0.43988066\n",
            "Iteration 66, loss = 0.43974726\n",
            "Iteration 67, loss = 0.43969056\n",
            "Iteration 68, loss = 0.43958881\n",
            "Iteration 69, loss = 0.43958069\n",
            "Iteration 70, loss = 0.43949357\n",
            "Iteration 71, loss = 0.43934119\n",
            "Iteration 72, loss = 0.43930208\n",
            "Iteration 73, loss = 0.43943410\n",
            "Iteration 74, loss = 0.43919277\n",
            "Iteration 75, loss = 0.43922489\n",
            "Iteration 76, loss = 0.43912996\n",
            "Iteration 77, loss = 0.43912010\n",
            "Iteration 78, loss = 0.43904765\n",
            "Iteration 79, loss = 0.43895950\n",
            "Iteration 80, loss = 0.43894884\n",
            "Iteration 81, loss = 0.43893460\n",
            "Iteration 82, loss = 0.43880652\n",
            "Iteration 83, loss = 0.43881454\n",
            "Iteration 84, loss = 0.43880517\n",
            "Iteration 85, loss = 0.43872067\n",
            "Iteration 86, loss = 0.43875715\n",
            "Iteration 87, loss = 0.43869063\n",
            "Iteration 88, loss = 0.43858263\n",
            "Iteration 89, loss = 0.43853746\n",
            "Iteration 90, loss = 0.43844939\n",
            "Iteration 91, loss = 0.43848413\n",
            "Iteration 92, loss = 0.43841230\n",
            "Iteration 93, loss = 0.43846018\n",
            "Iteration 94, loss = 0.43834036\n",
            "Iteration 95, loss = 0.43837084\n",
            "Iteration 96, loss = 0.43850542\n",
            "Iteration 97, loss = 0.43828195\n",
            "Iteration 98, loss = 0.43825549\n",
            "Iteration 99, loss = 0.43811743\n",
            "Iteration 100, loss = 0.43826604\n",
            "Iteration 101, loss = 0.43810234\n",
            "Iteration 102, loss = 0.43819814\n",
            "Iteration 103, loss = 0.43827637\n",
            "Iteration 104, loss = 0.43808910\n",
            "Iteration 105, loss = 0.43797405\n",
            "Iteration 106, loss = 0.43803192\n",
            "Iteration 107, loss = 0.43797864\n",
            "Iteration 108, loss = 0.43801183\n",
            "Iteration 109, loss = 0.43791823\n",
            "Iteration 110, loss = 0.43787190\n",
            "Iteration 111, loss = 0.43791446\n",
            "Iteration 112, loss = 0.43787684\n",
            "Iteration 113, loss = 0.43782413\n",
            "Iteration 114, loss = 0.43775120\n",
            "Iteration 115, loss = 0.43777959\n",
            "Iteration 116, loss = 0.43779505\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 117, loss = 0.43734209\n",
            "Iteration 118, loss = 0.43733025\n",
            "Iteration 119, loss = 0.43733026\n",
            "Iteration 120, loss = 0.43727391\n",
            "Iteration 121, loss = 0.43729058\n",
            "Iteration 122, loss = 0.43733867\n",
            "Iteration 123, loss = 0.43731920\n",
            "Iteration 124, loss = 0.43730713\n",
            "Iteration 125, loss = 0.43727813\n",
            "Iteration 126, loss = 0.43727310\n",
            "Iteration 127, loss = 0.43728095\n",
            "Iteration 128, loss = 0.43728313\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 129, loss = 0.43717945\n",
            "Iteration 130, loss = 0.43716688\n",
            "Iteration 131, loss = 0.43717035\n",
            "Iteration 132, loss = 0.43716494\n",
            "Iteration 133, loss = 0.43716037\n",
            "Iteration 134, loss = 0.43717501\n",
            "Iteration 135, loss = 0.43717487\n",
            "Iteration 136, loss = 0.43716536\n",
            "Iteration 137, loss = 0.43716066\n",
            "Iteration 138, loss = 0.43715535\n",
            "Iteration 139, loss = 0.43716315\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 140, loss = 0.43714342\n",
            "Iteration 141, loss = 0.43713810\n",
            "Iteration 142, loss = 0.43713688\n",
            "Iteration 143, loss = 0.43713573\n",
            "Iteration 144, loss = 0.43713700\n",
            "Iteration 145, loss = 0.43713652\n",
            "Iteration 146, loss = 0.43713346\n",
            "Iteration 147, loss = 0.43713636\n",
            "Iteration 148, loss = 0.43713708\n",
            "Iteration 149, loss = 0.43713617\n",
            "Iteration 150, loss = 0.43713449\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 151, loss = 0.43713029\n",
            "Iteration 152, loss = 0.43712931\n",
            "Iteration 153, loss = 0.43712919\n",
            "Iteration 154, loss = 0.43712922\n",
            "Iteration 155, loss = 0.43712906\n",
            "Iteration 156, loss = 0.43712866\n",
            "Iteration 157, loss = 0.43712887\n",
            "Iteration 158, loss = 0.43712880\n",
            "Iteration 159, loss = 0.43712870\n",
            "Iteration 160, loss = 0.43712883\n",
            "Iteration 161, loss = 0.43712856\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 162, loss = 0.43712742\n",
            "Iteration 163, loss = 0.43712742\n",
            "Iteration 164, loss = 0.43712745\n",
            "Iteration 165, loss = 0.43712753\n",
            "Iteration 166, loss = 0.43712741\n",
            "Iteration 167, loss = 0.43712743\n",
            "Iteration 168, loss = 0.43712738\n",
            "Iteration 169, loss = 0.43712736\n",
            "Iteration 170, loss = 0.43712730\n",
            "Iteration 171, loss = 0.43712733\n",
            "Iteration 172, loss = 0.43712731\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.69711440\n",
            "Iteration 2, loss = 0.69161333\n",
            "Iteration 3, loss = 0.65896139\n",
            "Iteration 4, loss = 0.57159789\n",
            "Iteration 5, loss = 0.53661584\n",
            "Iteration 6, loss = 0.51268915\n",
            "Iteration 7, loss = 0.49152876\n",
            "Iteration 8, loss = 0.47677637\n",
            "Iteration 9, loss = 0.46838183\n",
            "Iteration 10, loss = 0.46335827\n",
            "Iteration 11, loss = 0.45998790\n",
            "Iteration 12, loss = 0.45747929\n",
            "Iteration 13, loss = 0.45554071\n",
            "Iteration 14, loss = 0.45405403\n",
            "Iteration 15, loss = 0.45260982\n",
            "Iteration 16, loss = 0.45160173\n",
            "Iteration 17, loss = 0.45065467\n",
            "Iteration 18, loss = 0.44987343\n",
            "Iteration 19, loss = 0.44918348\n",
            "Iteration 20, loss = 0.44855377\n",
            "Iteration 21, loss = 0.44804620\n",
            "Iteration 22, loss = 0.44759445\n",
            "Iteration 23, loss = 0.44722323\n",
            "Iteration 24, loss = 0.44674667\n",
            "Iteration 25, loss = 0.44639434\n",
            "Iteration 26, loss = 0.44609889\n",
            "Iteration 27, loss = 0.44576305\n",
            "Iteration 28, loss = 0.44543693\n",
            "Iteration 29, loss = 0.44519238\n",
            "Iteration 30, loss = 0.44503387\n",
            "Iteration 31, loss = 0.44482275\n",
            "Iteration 32, loss = 0.44458411\n",
            "Iteration 33, loss = 0.44424473\n",
            "Iteration 34, loss = 0.44411180\n",
            "Iteration 35, loss = 0.44391428\n",
            "Iteration 36, loss = 0.44370172\n",
            "Iteration 37, loss = 0.44355258\n",
            "Iteration 38, loss = 0.44333150\n",
            "Iteration 39, loss = 0.44323002\n",
            "Iteration 40, loss = 0.44297234\n",
            "Iteration 41, loss = 0.44286726\n",
            "Iteration 42, loss = 0.44271409\n",
            "Iteration 43, loss = 0.44247839\n",
            "Iteration 44, loss = 0.44238575\n",
            "Iteration 45, loss = 0.44227183\n",
            "Iteration 46, loss = 0.44211834\n",
            "Iteration 47, loss = 0.44200409\n",
            "Iteration 48, loss = 0.44189707\n",
            "Iteration 49, loss = 0.44168631\n",
            "Iteration 50, loss = 0.44166786\n",
            "Iteration 51, loss = 0.44143661\n",
            "Iteration 52, loss = 0.44132158\n",
            "Iteration 53, loss = 0.44121837\n",
            "Iteration 54, loss = 0.44113465\n",
            "Iteration 55, loss = 0.44105199\n",
            "Iteration 56, loss = 0.44098655\n",
            "Iteration 57, loss = 0.44073497\n",
            "Iteration 58, loss = 0.44066443\n",
            "Iteration 59, loss = 0.44060771\n",
            "Iteration 60, loss = 0.44048321\n",
            "Iteration 61, loss = 0.44039505\n",
            "Iteration 62, loss = 0.44034011\n",
            "Iteration 63, loss = 0.44033898\n",
            "Iteration 64, loss = 0.44023309\n",
            "Iteration 65, loss = 0.44013970\n",
            "Iteration 66, loss = 0.43995265\n",
            "Iteration 67, loss = 0.43996066\n",
            "Iteration 68, loss = 0.43986386\n",
            "Iteration 69, loss = 0.43990406\n",
            "Iteration 70, loss = 0.43979119\n",
            "Iteration 71, loss = 0.43973402\n",
            "Iteration 72, loss = 0.43959423\n",
            "Iteration 73, loss = 0.43965049\n",
            "Iteration 74, loss = 0.43953450\n",
            "Iteration 75, loss = 0.43951139\n",
            "Iteration 76, loss = 0.43946667\n",
            "Iteration 77, loss = 0.43928106\n",
            "Iteration 78, loss = 0.43943196\n",
            "Iteration 79, loss = 0.43923120\n",
            "Iteration 80, loss = 0.43932744\n",
            "Iteration 81, loss = 0.43915395\n",
            "Iteration 82, loss = 0.43918195\n",
            "Iteration 83, loss = 0.43915553\n",
            "Iteration 84, loss = 0.43911699\n",
            "Iteration 85, loss = 0.43905985\n",
            "Iteration 86, loss = 0.43908537\n",
            "Iteration 87, loss = 0.43906329\n",
            "Iteration 88, loss = 0.43892639\n",
            "Iteration 89, loss = 0.43889965\n",
            "Iteration 90, loss = 0.43883706\n",
            "Iteration 91, loss = 0.43878686\n",
            "Iteration 92, loss = 0.43882413\n",
            "Iteration 93, loss = 0.43873590\n",
            "Iteration 94, loss = 0.43882837\n",
            "Iteration 95, loss = 0.43864869\n",
            "Iteration 96, loss = 0.43857347\n",
            "Iteration 97, loss = 0.43865582\n",
            "Iteration 98, loss = 0.43860888\n",
            "Iteration 99, loss = 0.43862374\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 100, loss = 0.43822018\n",
            "Iteration 101, loss = 0.43814855\n",
            "Iteration 102, loss = 0.43816194\n",
            "Iteration 103, loss = 0.43815937\n",
            "Iteration 104, loss = 0.43814552\n",
            "Iteration 105, loss = 0.43814310\n",
            "Iteration 106, loss = 0.43813428\n",
            "Iteration 107, loss = 0.43806660\n",
            "Iteration 108, loss = 0.43812360\n",
            "Iteration 109, loss = 0.43810806\n",
            "Iteration 110, loss = 0.43807474\n",
            "Iteration 111, loss = 0.43810548\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 112, loss = 0.43800311\n",
            "Iteration 113, loss = 0.43798968\n",
            "Iteration 114, loss = 0.43797589\n",
            "Iteration 115, loss = 0.43799331\n",
            "Iteration 116, loss = 0.43799313\n",
            "Iteration 117, loss = 0.43798720\n",
            "Iteration 118, loss = 0.43798339\n",
            "Iteration 119, loss = 0.43798715\n",
            "Iteration 120, loss = 0.43799109\n",
            "Iteration 121, loss = 0.43798642\n",
            "Iteration 122, loss = 0.43797416\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 123, loss = 0.43795880\n",
            "Iteration 124, loss = 0.43795476\n",
            "Iteration 125, loss = 0.43795645\n",
            "Iteration 126, loss = 0.43795571\n",
            "Iteration 127, loss = 0.43795471\n",
            "Iteration 128, loss = 0.43795550\n",
            "Iteration 129, loss = 0.43795502\n",
            "Iteration 130, loss = 0.43795097\n",
            "Iteration 131, loss = 0.43795619\n",
            "Iteration 132, loss = 0.43795575\n",
            "Iteration 133, loss = 0.43795352\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 134, loss = 0.43794974\n",
            "Iteration 135, loss = 0.43794884\n",
            "Iteration 136, loss = 0.43794851\n",
            "Iteration 137, loss = 0.43794865\n",
            "Iteration 138, loss = 0.43794827\n",
            "Iteration 139, loss = 0.43794793\n",
            "Iteration 140, loss = 0.43794794\n",
            "Iteration 141, loss = 0.43794760\n",
            "Iteration 142, loss = 0.43794778\n",
            "Iteration 143, loss = 0.43794824\n",
            "Iteration 144, loss = 0.43794772\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 145, loss = 0.43794655\n",
            "Iteration 146, loss = 0.43794656\n",
            "Iteration 147, loss = 0.43794656\n",
            "Iteration 148, loss = 0.43794649\n",
            "Iteration 149, loss = 0.43794656\n",
            "Iteration 150, loss = 0.43794647\n",
            "Iteration 151, loss = 0.43794651\n",
            "Iteration 152, loss = 0.43794649\n",
            "Iteration 153, loss = 0.43794649\n",
            "Iteration 154, loss = 0.43794640\n",
            "Iteration 155, loss = 0.43794645\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.69713999\n",
            "Iteration 2, loss = 0.69146158\n",
            "Iteration 3, loss = 0.65658372\n",
            "Iteration 4, loss = 0.56953876\n",
            "Iteration 5, loss = 0.53505038\n",
            "Iteration 6, loss = 0.51104881\n",
            "Iteration 7, loss = 0.49032870\n",
            "Iteration 8, loss = 0.47600564\n",
            "Iteration 9, loss = 0.46775043\n",
            "Iteration 10, loss = 0.46281841\n",
            "Iteration 11, loss = 0.45948387\n",
            "Iteration 12, loss = 0.45703544\n",
            "Iteration 13, loss = 0.45509063\n",
            "Iteration 14, loss = 0.45359075\n",
            "Iteration 15, loss = 0.45229639\n",
            "Iteration 16, loss = 0.45127575\n",
            "Iteration 17, loss = 0.45033522\n",
            "Iteration 18, loss = 0.44959852\n",
            "Iteration 19, loss = 0.44891208\n",
            "Iteration 20, loss = 0.44826024\n",
            "Iteration 21, loss = 0.44769329\n",
            "Iteration 22, loss = 0.44725601\n",
            "Iteration 23, loss = 0.44682992\n",
            "Iteration 24, loss = 0.44645773\n",
            "Iteration 25, loss = 0.44612527\n",
            "Iteration 26, loss = 0.44574199\n",
            "Iteration 27, loss = 0.44541781\n",
            "Iteration 28, loss = 0.44522228\n",
            "Iteration 29, loss = 0.44488923\n",
            "Iteration 30, loss = 0.44473068\n",
            "Iteration 31, loss = 0.44443765\n",
            "Iteration 32, loss = 0.44419677\n",
            "Iteration 33, loss = 0.44392827\n",
            "Iteration 34, loss = 0.44376087\n",
            "Iteration 35, loss = 0.44356642\n",
            "Iteration 36, loss = 0.44336571\n",
            "Iteration 37, loss = 0.44327234\n",
            "Iteration 38, loss = 0.44299023\n",
            "Iteration 39, loss = 0.44292190\n",
            "Iteration 40, loss = 0.44267581\n",
            "Iteration 41, loss = 0.44252051\n",
            "Iteration 42, loss = 0.44232532\n",
            "Iteration 43, loss = 0.44223706\n",
            "Iteration 44, loss = 0.44210587\n",
            "Iteration 45, loss = 0.44197104\n",
            "Iteration 46, loss = 0.44184723\n",
            "Iteration 47, loss = 0.44164658\n",
            "Iteration 48, loss = 0.44149956\n",
            "Iteration 49, loss = 0.44138132\n",
            "Iteration 50, loss = 0.44133059\n",
            "Iteration 51, loss = 0.44113963\n",
            "Iteration 52, loss = 0.44100777\n",
            "Iteration 53, loss = 0.44098441\n",
            "Iteration 54, loss = 0.44082528\n",
            "Iteration 55, loss = 0.44068977\n",
            "Iteration 56, loss = 0.44070197\n",
            "Iteration 57, loss = 0.44049394\n",
            "Iteration 58, loss = 0.44037299\n",
            "Iteration 59, loss = 0.44030362\n",
            "Iteration 60, loss = 0.44022749\n",
            "Iteration 61, loss = 0.44015963\n",
            "Iteration 62, loss = 0.44012530\n",
            "Iteration 63, loss = 0.44005620\n",
            "Iteration 64, loss = 0.44005373\n",
            "Iteration 65, loss = 0.43981397\n",
            "Iteration 66, loss = 0.43977515\n",
            "Iteration 67, loss = 0.43976132\n",
            "Iteration 68, loss = 0.43978601\n",
            "Iteration 69, loss = 0.43952591\n",
            "Iteration 70, loss = 0.43959525\n",
            "Iteration 71, loss = 0.43949089\n",
            "Iteration 72, loss = 0.43942924\n",
            "Iteration 73, loss = 0.43942938\n",
            "Iteration 74, loss = 0.43933727\n",
            "Iteration 75, loss = 0.43930053\n",
            "Iteration 76, loss = 0.43922990\n",
            "Iteration 77, loss = 0.43916461\n",
            "Iteration 78, loss = 0.43916309\n",
            "Iteration 79, loss = 0.43912354\n",
            "Iteration 80, loss = 0.43909721\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 81, loss = 0.43868763\n",
            "Iteration 82, loss = 0.43863769\n",
            "Iteration 83, loss = 0.43865245\n",
            "Iteration 84, loss = 0.43862006\n",
            "Iteration 85, loss = 0.43860316\n",
            "Iteration 86, loss = 0.43858345\n",
            "Iteration 87, loss = 0.43856783\n",
            "Iteration 88, loss = 0.43859230\n",
            "Iteration 89, loss = 0.43858456\n",
            "Iteration 90, loss = 0.43856392\n",
            "Iteration 91, loss = 0.43855183\n",
            "Iteration 92, loss = 0.43856439\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 93, loss = 0.43846540\n",
            "Iteration 94, loss = 0.43845006\n",
            "Iteration 95, loss = 0.43845883\n",
            "Iteration 96, loss = 0.43844523\n",
            "Iteration 97, loss = 0.43844302\n",
            "Iteration 98, loss = 0.43845075\n",
            "Iteration 99, loss = 0.43845168\n",
            "Iteration 100, loss = 0.43844436\n",
            "Iteration 101, loss = 0.43844481\n",
            "Iteration 102, loss = 0.43844579\n",
            "Iteration 103, loss = 0.43844511\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 104, loss = 0.43841770\n",
            "Iteration 105, loss = 0.43841483\n",
            "Iteration 106, loss = 0.43841642\n",
            "Iteration 107, loss = 0.43841709\n",
            "Iteration 108, loss = 0.43841649\n",
            "Iteration 109, loss = 0.43841613\n",
            "Iteration 110, loss = 0.43841692\n",
            "Iteration 111, loss = 0.43841423\n",
            "Iteration 112, loss = 0.43841398\n",
            "Iteration 113, loss = 0.43841537\n",
            "Iteration 114, loss = 0.43841422\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 115, loss = 0.43840900\n",
            "Iteration 116, loss = 0.43840855\n",
            "Iteration 117, loss = 0.43840813\n",
            "Iteration 118, loss = 0.43840803\n",
            "Iteration 119, loss = 0.43840801\n",
            "Iteration 120, loss = 0.43840794\n",
            "Iteration 121, loss = 0.43840795\n",
            "Iteration 122, loss = 0.43840762\n",
            "Iteration 123, loss = 0.43840755\n",
            "Iteration 124, loss = 0.43840747\n",
            "Iteration 125, loss = 0.43840757\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 126, loss = 0.43840623\n",
            "Iteration 127, loss = 0.43840622\n",
            "Iteration 128, loss = 0.43840617\n",
            "Iteration 129, loss = 0.43840614\n",
            "Iteration 130, loss = 0.43840617\n",
            "Iteration 131, loss = 0.43840610\n",
            "Iteration 132, loss = 0.43840617\n",
            "Iteration 133, loss = 0.43840611\n",
            "Iteration 134, loss = 0.43840610\n",
            "Iteration 135, loss = 0.43840603\n",
            "Iteration 136, loss = 0.43840606\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.69704603\n",
            "Iteration 2, loss = 0.69153230\n",
            "Iteration 3, loss = 0.65755757\n",
            "Iteration 4, loss = 0.57018170\n",
            "Iteration 5, loss = 0.53533779\n",
            "Iteration 6, loss = 0.51144006\n",
            "Iteration 7, loss = 0.49061372\n",
            "Iteration 8, loss = 0.47627750\n",
            "Iteration 9, loss = 0.46814020\n",
            "Iteration 10, loss = 0.46319468\n",
            "Iteration 11, loss = 0.45990277\n",
            "Iteration 12, loss = 0.45747332\n",
            "Iteration 13, loss = 0.45547803\n",
            "Iteration 14, loss = 0.45396057\n",
            "Iteration 15, loss = 0.45264268\n",
            "Iteration 16, loss = 0.45157066\n",
            "Iteration 17, loss = 0.45070012\n",
            "Iteration 18, loss = 0.44985467\n",
            "Iteration 19, loss = 0.44927056\n",
            "Iteration 20, loss = 0.44857306\n",
            "Iteration 21, loss = 0.44815824\n",
            "Iteration 22, loss = 0.44760353\n",
            "Iteration 23, loss = 0.44714706\n",
            "Iteration 24, loss = 0.44679549\n",
            "Iteration 25, loss = 0.44647195\n",
            "Iteration 26, loss = 0.44609800\n",
            "Iteration 27, loss = 0.44573472\n",
            "Iteration 28, loss = 0.44556321\n",
            "Iteration 29, loss = 0.44527897\n",
            "Iteration 30, loss = 0.44498639\n",
            "Iteration 31, loss = 0.44485894\n",
            "Iteration 32, loss = 0.44460584\n",
            "Iteration 33, loss = 0.44439218\n",
            "Iteration 34, loss = 0.44415820\n",
            "Iteration 35, loss = 0.44396372\n",
            "Iteration 36, loss = 0.44372911\n",
            "Iteration 37, loss = 0.44357892\n",
            "Iteration 38, loss = 0.44342668\n",
            "Iteration 39, loss = 0.44338041\n",
            "Iteration 40, loss = 0.44312124\n",
            "Iteration 41, loss = 0.44303062\n",
            "Iteration 42, loss = 0.44280245\n",
            "Iteration 43, loss = 0.44266886\n",
            "Iteration 44, loss = 0.44255223\n",
            "Iteration 45, loss = 0.44237733\n",
            "Iteration 46, loss = 0.44229193\n",
            "Iteration 47, loss = 0.44212726\n",
            "Iteration 48, loss = 0.44198260\n",
            "Iteration 49, loss = 0.44193419\n",
            "Iteration 50, loss = 0.44178900\n",
            "Iteration 51, loss = 0.44162031\n",
            "Iteration 52, loss = 0.44147402\n",
            "Iteration 53, loss = 0.44140356\n",
            "Iteration 54, loss = 0.44123599\n",
            "Iteration 55, loss = 0.44112623\n",
            "Iteration 56, loss = 0.44100001\n",
            "Iteration 57, loss = 0.44093458\n",
            "Iteration 58, loss = 0.44077503\n",
            "Iteration 59, loss = 0.44072330\n",
            "Iteration 60, loss = 0.44059671\n",
            "Iteration 61, loss = 0.44051006\n",
            "Iteration 62, loss = 0.44048412\n",
            "Iteration 63, loss = 0.44024562\n",
            "Iteration 64, loss = 0.44023325\n",
            "Iteration 65, loss = 0.44025857\n",
            "Iteration 66, loss = 0.44009812\n",
            "Iteration 67, loss = 0.44004329\n",
            "Iteration 68, loss = 0.43988694\n",
            "Iteration 69, loss = 0.43984145\n",
            "Iteration 70, loss = 0.43975838\n",
            "Iteration 71, loss = 0.43981809\n",
            "Iteration 72, loss = 0.43961542\n",
            "Iteration 73, loss = 0.43950262\n",
            "Iteration 74, loss = 0.43954879\n",
            "Iteration 75, loss = 0.43943314\n",
            "Iteration 76, loss = 0.43945594\n",
            "Iteration 77, loss = 0.43941086\n",
            "Iteration 78, loss = 0.43935834\n",
            "Iteration 79, loss = 0.43928209\n",
            "Iteration 80, loss = 0.43922315\n",
            "Iteration 81, loss = 0.43931286\n",
            "Iteration 82, loss = 0.43914415\n",
            "Iteration 83, loss = 0.43919604\n",
            "Iteration 84, loss = 0.43903158\n",
            "Iteration 85, loss = 0.43900324\n",
            "Iteration 86, loss = 0.43903460\n",
            "Iteration 87, loss = 0.43892343\n",
            "Iteration 88, loss = 0.43877517\n",
            "Iteration 89, loss = 0.43881985\n",
            "Iteration 90, loss = 0.43890573\n",
            "Iteration 91, loss = 0.43878226\n",
            "Iteration 92, loss = 0.43876995\n",
            "Iteration 93, loss = 0.43869953\n",
            "Iteration 94, loss = 0.43865522\n",
            "Iteration 95, loss = 0.43867408\n",
            "Iteration 96, loss = 0.43864268\n",
            "Iteration 97, loss = 0.43856545\n",
            "Iteration 98, loss = 0.43857367\n",
            "Iteration 99, loss = 0.43854897\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 100, loss = 0.43810807\n",
            "Iteration 101, loss = 0.43810874\n",
            "Iteration 102, loss = 0.43807639\n",
            "Iteration 103, loss = 0.43808403\n",
            "Iteration 104, loss = 0.43806625\n",
            "Iteration 105, loss = 0.43805190\n",
            "Iteration 106, loss = 0.43806154\n",
            "Iteration 107, loss = 0.43805817\n",
            "Iteration 108, loss = 0.43804766\n",
            "Iteration 109, loss = 0.43803634\n",
            "Iteration 110, loss = 0.43804700\n",
            "Iteration 111, loss = 0.43803459\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 112, loss = 0.43792329\n",
            "Iteration 113, loss = 0.43791890\n",
            "Iteration 114, loss = 0.43790362\n",
            "Iteration 115, loss = 0.43791746\n",
            "Iteration 116, loss = 0.43791416\n",
            "Iteration 117, loss = 0.43790871\n",
            "Iteration 118, loss = 0.43791096\n",
            "Iteration 119, loss = 0.43791505\n",
            "Iteration 120, loss = 0.43790466\n",
            "Iteration 121, loss = 0.43790203\n",
            "Iteration 122, loss = 0.43790314\n",
            "Iteration 123, loss = 0.43790690\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 124, loss = 0.43789082\n",
            "Iteration 125, loss = 0.43788138\n",
            "Iteration 126, loss = 0.43788062\n",
            "Iteration 127, loss = 0.43788349\n",
            "Iteration 128, loss = 0.43788088\n",
            "Iteration 129, loss = 0.43788351\n",
            "Iteration 130, loss = 0.43788001\n",
            "Iteration 131, loss = 0.43788049\n",
            "Iteration 132, loss = 0.43788121\n",
            "Iteration 133, loss = 0.43788190\n",
            "Iteration 134, loss = 0.43787988\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 135, loss = 0.43787413\n",
            "Iteration 136, loss = 0.43787427\n",
            "Iteration 137, loss = 0.43787420\n",
            "Iteration 138, loss = 0.43787408\n",
            "Iteration 139, loss = 0.43787375\n",
            "Iteration 140, loss = 0.43787391\n",
            "Iteration 141, loss = 0.43787381\n",
            "Iteration 142, loss = 0.43787373\n",
            "Iteration 143, loss = 0.43787391\n",
            "Iteration 144, loss = 0.43787371\n",
            "Iteration 145, loss = 0.43787380\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 146, loss = 0.43787239\n",
            "Iteration 147, loss = 0.43787235\n",
            "Iteration 148, loss = 0.43787234\n",
            "Iteration 149, loss = 0.43787229\n",
            "Iteration 150, loss = 0.43787230\n",
            "Iteration 151, loss = 0.43787230\n",
            "Iteration 152, loss = 0.43787236\n",
            "Iteration 153, loss = 0.43787227\n",
            "Iteration 154, loss = 0.43787233\n",
            "Iteration 155, loss = 0.43787223\n",
            "Iteration 156, loss = 0.43787226\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.69714008\n",
            "Iteration 2, loss = 0.69157082\n",
            "Iteration 3, loss = 0.65863104\n",
            "Iteration 4, loss = 0.57212365\n",
            "Iteration 5, loss = 0.53772367\n",
            "Iteration 6, loss = 0.51386019\n",
            "Iteration 7, loss = 0.49286046\n",
            "Iteration 8, loss = 0.47822400\n",
            "Iteration 9, loss = 0.46983349\n",
            "Iteration 10, loss = 0.46477525\n",
            "Iteration 11, loss = 0.46142972\n",
            "Iteration 12, loss = 0.45901963\n",
            "Iteration 13, loss = 0.45692667\n",
            "Iteration 14, loss = 0.45543225\n",
            "Iteration 15, loss = 0.45412331\n",
            "Iteration 16, loss = 0.45302135\n",
            "Iteration 17, loss = 0.45213241\n",
            "Iteration 18, loss = 0.45132548\n",
            "Iteration 19, loss = 0.45065207\n",
            "Iteration 20, loss = 0.45010350\n",
            "Iteration 21, loss = 0.44953132\n",
            "Iteration 22, loss = 0.44904728\n",
            "Iteration 23, loss = 0.44862348\n",
            "Iteration 24, loss = 0.44820072\n",
            "Iteration 25, loss = 0.44784428\n",
            "Iteration 26, loss = 0.44757214\n",
            "Iteration 27, loss = 0.44723584\n",
            "Iteration 28, loss = 0.44689602\n",
            "Iteration 29, loss = 0.44669984\n",
            "Iteration 30, loss = 0.44635963\n",
            "Iteration 31, loss = 0.44610360\n",
            "Iteration 32, loss = 0.44590322\n",
            "Iteration 33, loss = 0.44583099\n",
            "Iteration 34, loss = 0.44555464\n",
            "Iteration 35, loss = 0.44534078\n",
            "Iteration 36, loss = 0.44517712\n",
            "Iteration 37, loss = 0.44495301\n",
            "Iteration 38, loss = 0.44476379\n",
            "Iteration 39, loss = 0.44471431\n",
            "Iteration 40, loss = 0.44445496\n",
            "Iteration 41, loss = 0.44434671\n",
            "Iteration 42, loss = 0.44411299\n",
            "Iteration 43, loss = 0.44400051\n",
            "Iteration 44, loss = 0.44383614\n",
            "Iteration 45, loss = 0.44376386\n",
            "Iteration 46, loss = 0.44352660\n",
            "Iteration 47, loss = 0.44343957\n",
            "Iteration 48, loss = 0.44323898\n",
            "Iteration 49, loss = 0.44310069\n",
            "Iteration 50, loss = 0.44300884\n",
            "Iteration 51, loss = 0.44291342\n",
            "Iteration 52, loss = 0.44273617\n",
            "Iteration 53, loss = 0.44270001\n",
            "Iteration 54, loss = 0.44247959\n",
            "Iteration 55, loss = 0.44240630\n",
            "Iteration 56, loss = 0.44218306\n",
            "Iteration 57, loss = 0.44225924\n",
            "Iteration 58, loss = 0.44205539\n",
            "Iteration 59, loss = 0.44196328\n",
            "Iteration 60, loss = 0.44190040\n",
            "Iteration 61, loss = 0.44191870\n",
            "Iteration 62, loss = 0.44172135\n",
            "Iteration 63, loss = 0.44164440\n",
            "Iteration 64, loss = 0.44160563\n",
            "Iteration 65, loss = 0.44161008\n",
            "Iteration 66, loss = 0.44137729\n",
            "Iteration 67, loss = 0.44126700\n",
            "Iteration 68, loss = 0.44120746\n",
            "Iteration 69, loss = 0.44114186\n",
            "Iteration 70, loss = 0.44112564\n",
            "Iteration 71, loss = 0.44126002\n",
            "Iteration 72, loss = 0.44095836\n",
            "Iteration 73, loss = 0.44099540\n",
            "Iteration 74, loss = 0.44089151\n",
            "Iteration 75, loss = 0.44076153\n",
            "Iteration 76, loss = 0.44069203\n",
            "Iteration 77, loss = 0.44070592\n",
            "Iteration 78, loss = 0.44060004\n",
            "Iteration 79, loss = 0.44062211\n",
            "Iteration 80, loss = 0.44051298\n",
            "Iteration 81, loss = 0.44063404\n",
            "Iteration 82, loss = 0.44052912\n",
            "Iteration 83, loss = 0.44043754\n",
            "Iteration 84, loss = 0.44039725\n",
            "Iteration 85, loss = 0.44036655\n",
            "Iteration 86, loss = 0.44039277\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 87, loss = 0.43992025\n",
            "Iteration 88, loss = 0.43987154\n",
            "Iteration 89, loss = 0.43989247\n",
            "Iteration 90, loss = 0.43986570\n",
            "Iteration 91, loss = 0.43985980\n",
            "Iteration 92, loss = 0.43986886\n",
            "Iteration 93, loss = 0.43981302\n",
            "Iteration 94, loss = 0.43984102\n",
            "Iteration 95, loss = 0.43980270\n",
            "Iteration 96, loss = 0.43982693\n",
            "Iteration 97, loss = 0.43978895\n",
            "Iteration 98, loss = 0.43981888\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 99, loss = 0.43972008\n",
            "Iteration 100, loss = 0.43970133\n",
            "Iteration 101, loss = 0.43968664\n",
            "Iteration 102, loss = 0.43969639\n",
            "Iteration 103, loss = 0.43969427\n",
            "Iteration 104, loss = 0.43968452\n",
            "Iteration 105, loss = 0.43969551\n",
            "Iteration 106, loss = 0.43967849\n",
            "Iteration 107, loss = 0.43967643\n",
            "Iteration 108, loss = 0.43968094\n",
            "Iteration 109, loss = 0.43968810\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 110, loss = 0.43965916\n",
            "Iteration 111, loss = 0.43965858\n",
            "Iteration 112, loss = 0.43965678\n",
            "Iteration 113, loss = 0.43965537\n",
            "Iteration 114, loss = 0.43965693\n",
            "Iteration 115, loss = 0.43965601\n",
            "Iteration 116, loss = 0.43965714\n",
            "Iteration 117, loss = 0.43965673\n",
            "Iteration 118, loss = 0.43965519\n",
            "Iteration 119, loss = 0.43965522\n",
            "Iteration 120, loss = 0.43965320\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 121, loss = 0.43965091\n",
            "Iteration 122, loss = 0.43964988\n",
            "Iteration 123, loss = 0.43964916\n",
            "Iteration 124, loss = 0.43964872\n",
            "Iteration 125, loss = 0.43964840\n",
            "Iteration 126, loss = 0.43964858\n",
            "Iteration 127, loss = 0.43964823\n",
            "Iteration 128, loss = 0.43964844\n",
            "Iteration 129, loss = 0.43964798\n",
            "Iteration 130, loss = 0.43964818\n",
            "Iteration 131, loss = 0.43964784\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 132, loss = 0.43964664\n",
            "Iteration 133, loss = 0.43964660\n",
            "Iteration 134, loss = 0.43964660\n",
            "Iteration 135, loss = 0.43964656\n",
            "Iteration 136, loss = 0.43964661\n",
            "Iteration 137, loss = 0.43964658\n",
            "Iteration 138, loss = 0.43964655\n",
            "Iteration 139, loss = 0.43964651\n",
            "Iteration 140, loss = 0.43964650\n",
            "Iteration 141, loss = 0.43964659\n",
            "Iteration 142, loss = 0.43964651\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.69713998\n",
            "Iteration 2, loss = 0.69140356\n",
            "Iteration 3, loss = 0.65563318\n",
            "Iteration 4, loss = 0.56869657\n",
            "Iteration 5, loss = 0.53448477\n",
            "Iteration 6, loss = 0.51061879\n",
            "Iteration 7, loss = 0.49004456\n",
            "Iteration 8, loss = 0.47592441\n",
            "Iteration 9, loss = 0.46773945\n",
            "Iteration 10, loss = 0.46278680\n",
            "Iteration 11, loss = 0.45938066\n",
            "Iteration 12, loss = 0.45692120\n",
            "Iteration 13, loss = 0.45491241\n",
            "Iteration 14, loss = 0.45335338\n",
            "Iteration 15, loss = 0.45204675\n",
            "Iteration 16, loss = 0.45100489\n",
            "Iteration 17, loss = 0.45012508\n",
            "Iteration 18, loss = 0.44934343\n",
            "Iteration 19, loss = 0.44859127\n",
            "Iteration 20, loss = 0.44801456\n",
            "Iteration 21, loss = 0.44750978\n",
            "Iteration 22, loss = 0.44695958\n",
            "Iteration 23, loss = 0.44654756\n",
            "Iteration 24, loss = 0.44613952\n",
            "Iteration 25, loss = 0.44582457\n",
            "Iteration 26, loss = 0.44551135\n",
            "Iteration 27, loss = 0.44519342\n",
            "Iteration 28, loss = 0.44487647\n",
            "Iteration 29, loss = 0.44453695\n",
            "Iteration 30, loss = 0.44427026\n",
            "Iteration 31, loss = 0.44402622\n",
            "Iteration 32, loss = 0.44381550\n",
            "Iteration 33, loss = 0.44364265\n",
            "Iteration 34, loss = 0.44331730\n",
            "Iteration 35, loss = 0.44318690\n",
            "Iteration 36, loss = 0.44294480\n",
            "Iteration 37, loss = 0.44275930\n",
            "Iteration 38, loss = 0.44263045\n",
            "Iteration 39, loss = 0.44237996\n",
            "Iteration 40, loss = 0.44226061\n",
            "Iteration 41, loss = 0.44197619\n",
            "Iteration 42, loss = 0.44192642\n",
            "Iteration 43, loss = 0.44174102\n",
            "Iteration 44, loss = 0.44158136\n",
            "Iteration 45, loss = 0.44144273\n",
            "Iteration 46, loss = 0.44129362\n",
            "Iteration 47, loss = 0.44113464\n",
            "Iteration 48, loss = 0.44100615\n",
            "Iteration 49, loss = 0.44093136\n",
            "Iteration 50, loss = 0.44074147\n",
            "Iteration 51, loss = 0.44059658\n",
            "Iteration 52, loss = 0.44049129\n",
            "Iteration 53, loss = 0.44033849\n",
            "Iteration 54, loss = 0.44028782\n",
            "Iteration 55, loss = 0.44016449\n",
            "Iteration 56, loss = 0.44005898\n",
            "Iteration 57, loss = 0.44004982\n",
            "Iteration 58, loss = 0.43986746\n",
            "Iteration 59, loss = 0.43981705\n",
            "Iteration 60, loss = 0.43978974\n",
            "Iteration 61, loss = 0.43962954\n",
            "Iteration 62, loss = 0.43950662\n",
            "Iteration 63, loss = 0.43950494\n",
            "Iteration 64, loss = 0.43944236\n",
            "Iteration 65, loss = 0.43931583\n",
            "Iteration 66, loss = 0.43921904\n",
            "Iteration 67, loss = 0.43921596\n",
            "Iteration 68, loss = 0.43909640\n",
            "Iteration 69, loss = 0.43901186\n",
            "Iteration 70, loss = 0.43901006\n",
            "Iteration 71, loss = 0.43904673\n",
            "Iteration 72, loss = 0.43899164\n",
            "Iteration 73, loss = 0.43888768\n",
            "Iteration 74, loss = 0.43878162\n",
            "Iteration 75, loss = 0.43870010\n",
            "Iteration 76, loss = 0.43863725\n",
            "Iteration 77, loss = 0.43863552\n",
            "Iteration 78, loss = 0.43865141\n",
            "Iteration 79, loss = 0.43857129\n",
            "Iteration 80, loss = 0.43848227\n",
            "Iteration 81, loss = 0.43850017\n",
            "Iteration 82, loss = 0.43841370\n",
            "Iteration 83, loss = 0.43841586\n",
            "Iteration 84, loss = 0.43837264\n",
            "Iteration 85, loss = 0.43834837\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 86, loss = 0.43795449\n",
            "Iteration 87, loss = 0.43790775\n",
            "Iteration 88, loss = 0.43788763\n",
            "Iteration 89, loss = 0.43785383\n",
            "Iteration 90, loss = 0.43784649\n",
            "Iteration 91, loss = 0.43784576\n",
            "Iteration 92, loss = 0.43781008\n",
            "Iteration 93, loss = 0.43782593\n",
            "Iteration 94, loss = 0.43782700\n",
            "Iteration 95, loss = 0.43781054\n",
            "Iteration 96, loss = 0.43782493\n",
            "Iteration 97, loss = 0.43780483\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 98, loss = 0.43771135\n",
            "Iteration 99, loss = 0.43769823\n",
            "Iteration 100, loss = 0.43770159\n",
            "Iteration 101, loss = 0.43768804\n",
            "Iteration 102, loss = 0.43769156\n",
            "Iteration 103, loss = 0.43768631\n",
            "Iteration 104, loss = 0.43768526\n",
            "Iteration 105, loss = 0.43770046\n",
            "Iteration 106, loss = 0.43769224\n",
            "Iteration 107, loss = 0.43768497\n",
            "Iteration 108, loss = 0.43769838\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 109, loss = 0.43766284\n",
            "Iteration 110, loss = 0.43766033\n",
            "Iteration 111, loss = 0.43766139\n",
            "Iteration 112, loss = 0.43765939\n",
            "Iteration 113, loss = 0.43766234\n",
            "Iteration 114, loss = 0.43766019\n",
            "Iteration 115, loss = 0.43766164\n",
            "Iteration 116, loss = 0.43765968\n",
            "Iteration 117, loss = 0.43765752\n",
            "Iteration 118, loss = 0.43765846\n",
            "Iteration 119, loss = 0.43765834\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 120, loss = 0.43765311\n",
            "Iteration 121, loss = 0.43765322\n",
            "Iteration 122, loss = 0.43765257\n",
            "Iteration 123, loss = 0.43765259\n",
            "Iteration 124, loss = 0.43765264\n",
            "Iteration 125, loss = 0.43765226\n",
            "Iteration 126, loss = 0.43765243\n",
            "Iteration 127, loss = 0.43765223\n",
            "Iteration 128, loss = 0.43765208\n",
            "Iteration 129, loss = 0.43765201\n",
            "Iteration 130, loss = 0.43765218\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 131, loss = 0.43765085\n",
            "Iteration 132, loss = 0.43765081\n",
            "Iteration 133, loss = 0.43765079\n",
            "Iteration 134, loss = 0.43765075\n",
            "Iteration 135, loss = 0.43765083\n",
            "Iteration 136, loss = 0.43765075\n",
            "Iteration 137, loss = 0.43765074\n",
            "Iteration 138, loss = 0.43765071\n",
            "Iteration 139, loss = 0.43765073\n",
            "Iteration 140, loss = 0.43765066\n",
            "Iteration 141, loss = 0.43765072\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.69708404\n",
            "Iteration 2, loss = 0.69154191\n",
            "Iteration 3, loss = 0.65816697\n",
            "Iteration 4, loss = 0.57133640\n",
            "Iteration 5, loss = 0.53661013\n",
            "Iteration 6, loss = 0.51269290\n",
            "Iteration 7, loss = 0.49179785\n",
            "Iteration 8, loss = 0.47734880\n",
            "Iteration 9, loss = 0.46892132\n",
            "Iteration 10, loss = 0.46398245\n",
            "Iteration 11, loss = 0.46062606\n",
            "Iteration 12, loss = 0.45818177\n",
            "Iteration 13, loss = 0.45626490\n",
            "Iteration 14, loss = 0.45478492\n",
            "Iteration 15, loss = 0.45343558\n",
            "Iteration 16, loss = 0.45236607\n",
            "Iteration 17, loss = 0.45135084\n",
            "Iteration 18, loss = 0.45057642\n",
            "Iteration 19, loss = 0.44993737\n",
            "Iteration 20, loss = 0.44931367\n",
            "Iteration 21, loss = 0.44873643\n",
            "Iteration 22, loss = 0.44826353\n",
            "Iteration 23, loss = 0.44789811\n",
            "Iteration 24, loss = 0.44740744\n",
            "Iteration 25, loss = 0.44698629\n",
            "Iteration 26, loss = 0.44677732\n",
            "Iteration 27, loss = 0.44644706\n",
            "Iteration 28, loss = 0.44607468\n",
            "Iteration 29, loss = 0.44585770\n",
            "Iteration 30, loss = 0.44561272\n",
            "Iteration 31, loss = 0.44530969\n",
            "Iteration 32, loss = 0.44508831\n",
            "Iteration 33, loss = 0.44485957\n",
            "Iteration 34, loss = 0.44466041\n",
            "Iteration 35, loss = 0.44448900\n",
            "Iteration 36, loss = 0.44427459\n",
            "Iteration 37, loss = 0.44410904\n",
            "Iteration 38, loss = 0.44389541\n",
            "Iteration 39, loss = 0.44371236\n",
            "Iteration 40, loss = 0.44352618\n",
            "Iteration 41, loss = 0.44331372\n",
            "Iteration 42, loss = 0.44317696\n",
            "Iteration 43, loss = 0.44306215\n",
            "Iteration 44, loss = 0.44281957\n",
            "Iteration 45, loss = 0.44277457\n",
            "Iteration 46, loss = 0.44263297\n",
            "Iteration 47, loss = 0.44253732\n",
            "Iteration 48, loss = 0.44240854\n",
            "Iteration 49, loss = 0.44224407\n",
            "Iteration 50, loss = 0.44199234\n",
            "Iteration 51, loss = 0.44203049\n",
            "Iteration 52, loss = 0.44191672\n",
            "Iteration 53, loss = 0.44176282\n",
            "Iteration 54, loss = 0.44173987\n",
            "Iteration 55, loss = 0.44163846\n",
            "Iteration 56, loss = 0.44156131\n",
            "Iteration 57, loss = 0.44132826\n",
            "Iteration 58, loss = 0.44134383\n",
            "Iteration 59, loss = 0.44120917\n",
            "Iteration 60, loss = 0.44117367\n",
            "Iteration 61, loss = 0.44099578\n",
            "Iteration 62, loss = 0.44089499\n",
            "Iteration 63, loss = 0.44093485\n",
            "Iteration 64, loss = 0.44081084\n",
            "Iteration 65, loss = 0.44079347\n",
            "Iteration 66, loss = 0.44072526\n",
            "Iteration 67, loss = 0.44071655\n",
            "Iteration 68, loss = 0.44072241\n",
            "Iteration 69, loss = 0.44043033\n",
            "Iteration 70, loss = 0.44047690\n",
            "Iteration 71, loss = 0.44039963\n",
            "Iteration 72, loss = 0.44028535\n",
            "Iteration 73, loss = 0.44035012\n",
            "Iteration 74, loss = 0.44024210\n",
            "Iteration 75, loss = 0.44019537\n",
            "Iteration 76, loss = 0.44004661\n",
            "Iteration 77, loss = 0.44002308\n",
            "Iteration 78, loss = 0.44005875\n",
            "Iteration 79, loss = 0.43995394\n",
            "Iteration 80, loss = 0.44000562\n",
            "Iteration 81, loss = 0.43978528\n",
            "Iteration 82, loss = 0.43993409\n",
            "Iteration 83, loss = 0.43983564\n",
            "Iteration 84, loss = 0.43987905\n",
            "Iteration 85, loss = 0.43976803\n",
            "Iteration 86, loss = 0.43976009\n",
            "Iteration 87, loss = 0.43969314\n",
            "Iteration 88, loss = 0.43964792\n",
            "Iteration 89, loss = 0.43968557\n",
            "Iteration 90, loss = 0.43958023\n",
            "Iteration 91, loss = 0.43957382\n",
            "Iteration 92, loss = 0.43955667\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 93, loss = 0.43921458\n",
            "Iteration 94, loss = 0.43911976\n",
            "Iteration 95, loss = 0.43910160\n",
            "Iteration 96, loss = 0.43908751\n",
            "Iteration 97, loss = 0.43906315\n",
            "Iteration 98, loss = 0.43908558\n",
            "Iteration 99, loss = 0.43908597\n",
            "Iteration 100, loss = 0.43907082\n",
            "Iteration 101, loss = 0.43903499\n",
            "Iteration 102, loss = 0.43904200\n",
            "Iteration 103, loss = 0.43906355\n",
            "Iteration 104, loss = 0.43905135\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 105, loss = 0.43893506\n",
            "Iteration 106, loss = 0.43894189\n",
            "Iteration 107, loss = 0.43893631\n",
            "Iteration 108, loss = 0.43894654\n",
            "Iteration 109, loss = 0.43892668\n",
            "Iteration 110, loss = 0.43894769\n",
            "Iteration 111, loss = 0.43892994\n",
            "Iteration 112, loss = 0.43892937\n",
            "Iteration 113, loss = 0.43893418\n",
            "Iteration 114, loss = 0.43892888\n",
            "Iteration 115, loss = 0.43893744\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 116, loss = 0.43890491\n",
            "Iteration 117, loss = 0.43890684\n",
            "Iteration 118, loss = 0.43890530\n",
            "Iteration 119, loss = 0.43890420\n",
            "Iteration 120, loss = 0.43890435\n",
            "Iteration 121, loss = 0.43890370\n",
            "Iteration 122, loss = 0.43890567\n",
            "Iteration 123, loss = 0.43890415\n",
            "Iteration 124, loss = 0.43890512\n",
            "Iteration 125, loss = 0.43890427\n",
            "Iteration 126, loss = 0.43890347\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 127, loss = 0.43889741\n",
            "Iteration 128, loss = 0.43889692\n",
            "Iteration 129, loss = 0.43889710\n",
            "Iteration 130, loss = 0.43889694\n",
            "Iteration 131, loss = 0.43889700\n",
            "Iteration 132, loss = 0.43889665\n",
            "Iteration 133, loss = 0.43889662\n",
            "Iteration 134, loss = 0.43889663\n",
            "Iteration 135, loss = 0.43889666\n",
            "Iteration 136, loss = 0.43889626\n",
            "Iteration 137, loss = 0.43889644\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 138, loss = 0.43889516\n",
            "Iteration 139, loss = 0.43889516\n",
            "Iteration 140, loss = 0.43889518\n",
            "Iteration 141, loss = 0.43889516\n",
            "Iteration 142, loss = 0.43889524\n",
            "Iteration 143, loss = 0.43889522\n",
            "Iteration 144, loss = 0.43889513\n",
            "Iteration 145, loss = 0.43889510\n",
            "Iteration 146, loss = 0.43889510\n",
            "Iteration 147, loss = 0.43889510\n",
            "Iteration 148, loss = 0.43889514\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.69714053\n",
            "Iteration 2, loss = 0.69141832\n",
            "Iteration 3, loss = 0.65614599\n",
            "Iteration 4, loss = 0.56919469\n",
            "Iteration 5, loss = 0.53471913\n",
            "Iteration 6, loss = 0.51087357\n",
            "Iteration 7, loss = 0.49013793\n",
            "Iteration 8, loss = 0.47589290\n",
            "Iteration 9, loss = 0.46762869\n",
            "Iteration 10, loss = 0.46271985\n",
            "Iteration 11, loss = 0.45939929\n",
            "Iteration 12, loss = 0.45700565\n",
            "Iteration 13, loss = 0.45508549\n",
            "Iteration 14, loss = 0.45352042\n",
            "Iteration 15, loss = 0.45217980\n",
            "Iteration 16, loss = 0.45108685\n",
            "Iteration 17, loss = 0.45021769\n",
            "Iteration 18, loss = 0.44946940\n",
            "Iteration 19, loss = 0.44876716\n",
            "Iteration 20, loss = 0.44813223\n",
            "Iteration 21, loss = 0.44758475\n",
            "Iteration 22, loss = 0.44710727\n",
            "Iteration 23, loss = 0.44665859\n",
            "Iteration 24, loss = 0.44629451\n",
            "Iteration 25, loss = 0.44582850\n",
            "Iteration 26, loss = 0.44550404\n",
            "Iteration 27, loss = 0.44518606\n",
            "Iteration 28, loss = 0.44492539\n",
            "Iteration 29, loss = 0.44464946\n",
            "Iteration 30, loss = 0.44430950\n",
            "Iteration 31, loss = 0.44406342\n",
            "Iteration 32, loss = 0.44385465\n",
            "Iteration 33, loss = 0.44363474\n",
            "Iteration 34, loss = 0.44341406\n",
            "Iteration 35, loss = 0.44319616\n",
            "Iteration 36, loss = 0.44298059\n",
            "Iteration 37, loss = 0.44272750\n",
            "Iteration 38, loss = 0.44252288\n",
            "Iteration 39, loss = 0.44243717\n",
            "Iteration 40, loss = 0.44223170\n",
            "Iteration 41, loss = 0.44196832\n",
            "Iteration 42, loss = 0.44186067\n",
            "Iteration 43, loss = 0.44175191\n",
            "Iteration 44, loss = 0.44158591\n",
            "Iteration 45, loss = 0.44138870\n",
            "Iteration 46, loss = 0.44132660\n",
            "Iteration 47, loss = 0.44120946\n",
            "Iteration 48, loss = 0.44106609\n",
            "Iteration 49, loss = 0.44095056\n",
            "Iteration 50, loss = 0.44082919\n",
            "Iteration 51, loss = 0.44067381\n",
            "Iteration 52, loss = 0.44061678\n",
            "Iteration 53, loss = 0.44051211\n",
            "Iteration 54, loss = 0.44043804\n",
            "Iteration 55, loss = 0.44030618\n",
            "Iteration 56, loss = 0.44021406\n",
            "Iteration 57, loss = 0.44013044\n",
            "Iteration 58, loss = 0.44002221\n",
            "Iteration 59, loss = 0.43994689\n",
            "Iteration 60, loss = 0.43984050\n",
            "Iteration 61, loss = 0.43981240\n",
            "Iteration 62, loss = 0.43980364\n",
            "Iteration 63, loss = 0.43967887\n",
            "Iteration 64, loss = 0.43955730\n",
            "Iteration 65, loss = 0.43949141\n",
            "Iteration 66, loss = 0.43941597\n",
            "Iteration 67, loss = 0.43937268\n",
            "Iteration 68, loss = 0.43930152\n",
            "Iteration 69, loss = 0.43921972\n",
            "Iteration 70, loss = 0.43920178\n",
            "Iteration 71, loss = 0.43918459\n",
            "Iteration 72, loss = 0.43911324\n",
            "Iteration 73, loss = 0.43902233\n",
            "Iteration 74, loss = 0.43898751\n",
            "Iteration 75, loss = 0.43893064\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 76, loss = 0.43854967\n",
            "Iteration 77, loss = 0.43851370\n",
            "Iteration 78, loss = 0.43853520\n",
            "Iteration 79, loss = 0.43845230\n",
            "Iteration 80, loss = 0.43845269\n",
            "Iteration 81, loss = 0.43846479\n",
            "Iteration 82, loss = 0.43844909\n",
            "Iteration 83, loss = 0.43841588\n",
            "Iteration 84, loss = 0.43840962\n",
            "Iteration 85, loss = 0.43843034\n",
            "Iteration 86, loss = 0.43840098\n",
            "Iteration 87, loss = 0.43839840\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 88, loss = 0.43830533\n",
            "Iteration 89, loss = 0.43829820\n",
            "Iteration 90, loss = 0.43829397\n",
            "Iteration 91, loss = 0.43829361\n",
            "Iteration 92, loss = 0.43829003\n",
            "Iteration 93, loss = 0.43827405\n",
            "Iteration 94, loss = 0.43828787\n",
            "Iteration 95, loss = 0.43828111\n",
            "Iteration 96, loss = 0.43828168\n",
            "Iteration 97, loss = 0.43828050\n",
            "Iteration 98, loss = 0.43829227\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 99, loss = 0.43825605\n",
            "Iteration 100, loss = 0.43825446\n",
            "Iteration 101, loss = 0.43825290\n",
            "Iteration 102, loss = 0.43825415\n",
            "Iteration 103, loss = 0.43825347\n",
            "Iteration 104, loss = 0.43825262\n",
            "Iteration 105, loss = 0.43825065\n",
            "Iteration 106, loss = 0.43825230\n",
            "Iteration 107, loss = 0.43825178\n",
            "Iteration 108, loss = 0.43825200\n",
            "Iteration 109, loss = 0.43825120\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 110, loss = 0.43824542\n",
            "Iteration 111, loss = 0.43824510\n",
            "Iteration 112, loss = 0.43824499\n",
            "Iteration 113, loss = 0.43824521\n",
            "Iteration 114, loss = 0.43824512\n",
            "Iteration 115, loss = 0.43824472\n",
            "Iteration 116, loss = 0.43824485\n",
            "Iteration 117, loss = 0.43824461\n",
            "Iteration 118, loss = 0.43824470\n",
            "Iteration 119, loss = 0.43824455\n",
            "Iteration 120, loss = 0.43824443\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 121, loss = 0.43824321\n",
            "Iteration 122, loss = 0.43824320\n",
            "Iteration 123, loss = 0.43824319\n",
            "Iteration 124, loss = 0.43824320\n",
            "Iteration 125, loss = 0.43824324\n",
            "Iteration 126, loss = 0.43824319\n",
            "Iteration 127, loss = 0.43824319\n",
            "Iteration 128, loss = 0.43824312\n",
            "Iteration 129, loss = 0.43824316\n",
            "Iteration 130, loss = 0.43824308\n",
            "Iteration 131, loss = 0.43824309\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "[-0.83316881 -0.85587364 -0.85735439 -0.84698914 -0.86056269 -0.8571076\n",
            " -0.80700888 -0.85735439 -0.83168806 -0.84896347]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 19.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.70309445\n",
            "Iteration 2, loss = 0.69719594\n",
            "Iteration 3, loss = 0.69482624\n",
            "Iteration 4, loss = 0.69373604\n",
            "Iteration 5, loss = 0.69304578\n",
            "Iteration 6, loss = 0.69237380\n",
            "Iteration 7, loss = 0.69132068\n",
            "Iteration 8, loss = 0.68936854\n",
            "Iteration 9, loss = 0.68555787\n",
            "Iteration 10, loss = 0.67787178\n",
            "Iteration 11, loss = 0.66264500\n",
            "Iteration 12, loss = 0.63634840\n",
            "Iteration 13, loss = 0.60613206\n",
            "Iteration 14, loss = 0.58398666\n",
            "Iteration 15, loss = 0.56947879\n",
            "Iteration 16, loss = 0.55891029\n",
            "Iteration 17, loss = 0.55058478\n",
            "Iteration 18, loss = 0.54351992\n",
            "Iteration 19, loss = 0.53706841\n",
            "Iteration 20, loss = 0.53112219\n",
            "Iteration 21, loss = 0.52553838\n",
            "Iteration 22, loss = 0.52017809\n",
            "Iteration 23, loss = 0.51497390\n",
            "Iteration 24, loss = 0.50983375\n",
            "Iteration 25, loss = 0.50493420\n",
            "Iteration 26, loss = 0.50011049\n",
            "Iteration 27, loss = 0.49551006\n",
            "Iteration 28, loss = 0.49140303\n",
            "Iteration 29, loss = 0.48747918\n",
            "Iteration 30, loss = 0.48403555\n",
            "Iteration 31, loss = 0.48084097\n",
            "Iteration 32, loss = 0.47815162\n",
            "Iteration 33, loss = 0.47562618\n",
            "Iteration 34, loss = 0.47347785\n",
            "Iteration 35, loss = 0.47153532\n",
            "Iteration 36, loss = 0.46999554\n",
            "Iteration 37, loss = 0.46862729\n",
            "Iteration 38, loss = 0.46729107\n",
            "Iteration 39, loss = 0.46618842\n",
            "Iteration 40, loss = 0.46512620\n",
            "Iteration 41, loss = 0.46445031\n",
            "Iteration 42, loss = 0.46329883\n",
            "Iteration 43, loss = 0.46249461\n",
            "Iteration 44, loss = 0.46187587\n",
            "Iteration 45, loss = 0.46120803\n",
            "Iteration 46, loss = 0.46046934\n",
            "Iteration 47, loss = 0.45982607\n",
            "Iteration 48, loss = 0.45928110\n",
            "Iteration 49, loss = 0.45870718\n",
            "Iteration 50, loss = 0.45818125\n",
            "Iteration 51, loss = 0.45776464\n",
            "Iteration 52, loss = 0.45741327\n",
            "Iteration 53, loss = 0.45674973\n",
            "Iteration 54, loss = 0.45626490\n",
            "Iteration 55, loss = 0.45596428\n",
            "Iteration 56, loss = 0.45571860\n",
            "Iteration 57, loss = 0.45521377\n",
            "Iteration 58, loss = 0.45484503\n",
            "Iteration 59, loss = 0.45455674\n",
            "Iteration 60, loss = 0.45410334\n",
            "Iteration 61, loss = 0.45381203\n",
            "Iteration 62, loss = 0.45341620\n",
            "Iteration 63, loss = 0.45309430\n",
            "Iteration 64, loss = 0.45289785\n",
            "Iteration 65, loss = 0.45270880\n",
            "Iteration 66, loss = 0.45231748\n",
            "Iteration 67, loss = 0.45219541\n",
            "Iteration 68, loss = 0.45193189\n",
            "Iteration 69, loss = 0.45145382\n",
            "Iteration 70, loss = 0.45147344\n",
            "Iteration 71, loss = 0.45115252\n",
            "Iteration 72, loss = 0.45103748\n",
            "Iteration 73, loss = 0.45070594\n",
            "Iteration 74, loss = 0.45049787\n",
            "Iteration 75, loss = 0.45029753\n",
            "Iteration 76, loss = 0.45009175\n",
            "Iteration 77, loss = 0.45007035\n",
            "Iteration 78, loss = 0.44982201\n",
            "Iteration 79, loss = 0.44963723\n",
            "Iteration 80, loss = 0.44951721\n",
            "Iteration 81, loss = 0.44929828\n",
            "Iteration 82, loss = 0.44918844\n",
            "Iteration 83, loss = 0.44889210\n",
            "Iteration 84, loss = 0.44883309\n",
            "Iteration 85, loss = 0.44867791\n",
            "Iteration 86, loss = 0.44858829\n",
            "Iteration 87, loss = 0.44834118\n",
            "Iteration 88, loss = 0.44827960\n",
            "Iteration 89, loss = 0.44805091\n",
            "Iteration 90, loss = 0.44790395\n",
            "Iteration 91, loss = 0.44773977\n",
            "Iteration 92, loss = 0.44772713\n",
            "Iteration 93, loss = 0.44751715\n",
            "Iteration 94, loss = 0.44733979\n",
            "Iteration 95, loss = 0.44730597\n",
            "Iteration 96, loss = 0.44718590\n",
            "Iteration 97, loss = 0.44699349\n",
            "Iteration 98, loss = 0.44693332\n",
            "Iteration 99, loss = 0.44681114\n",
            "Iteration 100, loss = 0.44680993\n",
            "Iteration 101, loss = 0.44654100\n",
            "Iteration 102, loss = 0.44652548\n",
            "Iteration 103, loss = 0.44643530\n",
            "Iteration 104, loss = 0.44638631\n",
            "Iteration 105, loss = 0.44624795\n",
            "Iteration 106, loss = 0.44614670\n",
            "Iteration 107, loss = 0.44604352\n",
            "Iteration 108, loss = 0.44600869\n",
            "Iteration 109, loss = 0.44583671\n",
            "Iteration 110, loss = 0.44574376\n",
            "Iteration 111, loss = 0.44546812\n",
            "Iteration 112, loss = 0.44573031\n",
            "Iteration 113, loss = 0.44548053\n",
            "Iteration 114, loss = 0.44530873\n",
            "Iteration 115, loss = 0.44520197\n",
            "Iteration 116, loss = 0.44521468\n",
            "Iteration 117, loss = 0.44509449\n",
            "Iteration 118, loss = 0.44515928\n",
            "Iteration 119, loss = 0.44497731\n",
            "Iteration 120, loss = 0.44481441\n",
            "Iteration 121, loss = 0.44494810\n",
            "Iteration 122, loss = 0.44475259\n",
            "Iteration 123, loss = 0.44470644\n",
            "Iteration 124, loss = 0.44460307\n",
            "Iteration 125, loss = 0.44461160\n",
            "Iteration 126, loss = 0.44440691\n",
            "Iteration 127, loss = 0.44456238\n",
            "Iteration 128, loss = 0.44437530\n",
            "Iteration 129, loss = 0.44436535\n",
            "Iteration 130, loss = 0.44409721\n",
            "Iteration 131, loss = 0.44431526\n",
            "Iteration 132, loss = 0.44399665\n",
            "Iteration 133, loss = 0.44399436\n",
            "Iteration 134, loss = 0.44408299\n",
            "Iteration 135, loss = 0.44385925\n",
            "Iteration 136, loss = 0.44379574\n",
            "Iteration 137, loss = 0.44374863\n",
            "Iteration 138, loss = 0.44363641\n",
            "Iteration 139, loss = 0.44370486\n",
            "Iteration 140, loss = 0.44358121\n",
            "Iteration 141, loss = 0.44365945\n",
            "Iteration 142, loss = 0.44341356\n",
            "Iteration 143, loss = 0.44324903\n",
            "Iteration 144, loss = 0.44308469\n",
            "Iteration 145, loss = 0.44337223\n",
            "Iteration 146, loss = 0.44310170\n",
            "Iteration 147, loss = 0.44321715\n",
            "Iteration 148, loss = 0.44310402\n",
            "Iteration 149, loss = 0.44300316\n",
            "Iteration 150, loss = 0.44298588\n",
            "Iteration 151, loss = 0.44299762\n",
            "Iteration 152, loss = 0.44289224\n",
            "Iteration 153, loss = 0.44275015\n",
            "Iteration 154, loss = 0.44266414\n",
            "Iteration 155, loss = 0.44275633\n",
            "Iteration 156, loss = 0.44266850\n",
            "Iteration 157, loss = 0.44266419\n",
            "Iteration 158, loss = 0.44258486\n",
            "Iteration 159, loss = 0.44246302\n",
            "Iteration 160, loss = 0.44265659\n",
            "Iteration 161, loss = 0.44230717\n",
            "Iteration 162, loss = 0.44246603\n",
            "Iteration 163, loss = 0.44224996\n",
            "Iteration 164, loss = 0.44211222\n",
            "Iteration 165, loss = 0.44232805\n",
            "Iteration 166, loss = 0.44224660\n",
            "Iteration 167, loss = 0.44203033\n",
            "Iteration 168, loss = 0.44197756\n",
            "Iteration 169, loss = 0.44216250\n",
            "Iteration 170, loss = 0.44204756\n",
            "Iteration 171, loss = 0.44199564\n",
            "Iteration 172, loss = 0.44170912\n",
            "Iteration 173, loss = 0.44174004\n",
            "Iteration 174, loss = 0.44176033\n",
            "Iteration 175, loss = 0.44165479\n",
            "Iteration 176, loss = 0.44149757\n",
            "Iteration 177, loss = 0.44151499\n",
            "Iteration 178, loss = 0.44150176\n",
            "Iteration 179, loss = 0.44142727\n",
            "Iteration 180, loss = 0.44147238\n",
            "Iteration 181, loss = 0.44135228\n",
            "Iteration 182, loss = 0.44152923\n",
            "Iteration 183, loss = 0.44138203\n",
            "Iteration 184, loss = 0.44138846\n",
            "Iteration 185, loss = 0.44112304\n",
            "Iteration 186, loss = 0.44120057\n",
            "Iteration 187, loss = 0.44115037\n",
            "Iteration 188, loss = 0.44109729\n",
            "Iteration 189, loss = 0.44122579\n",
            "Iteration 190, loss = 0.44102517\n",
            "Iteration 191, loss = 0.44091352\n",
            "Iteration 192, loss = 0.44089706\n",
            "Iteration 193, loss = 0.44096534\n",
            "Iteration 194, loss = 0.44089269\n",
            "Iteration 195, loss = 0.44067284\n",
            "Iteration 196, loss = 0.44061194\n",
            "Iteration 197, loss = 0.44068017\n",
            "Iteration 198, loss = 0.44074094\n",
            "Iteration 199, loss = 0.44080999\n",
            "Iteration 200, loss = 0.44042855\n",
            "Iteration 1, loss = 0.70310892\n",
            "Iteration 2, loss = 0.69722445\n",
            "Iteration 3, loss = 0.69486577\n",
            "Iteration 4, loss = 0.69377428\n",
            "Iteration 5, loss = 0.69312300\n",
            "Iteration 6, loss = 0.69251428\n",
            "Iteration 7, loss = 0.69159849\n",
            "Iteration 8, loss = 0.68989775\n",
            "Iteration 9, loss = 0.68664440\n",
            "Iteration 10, loss = 0.68008533\n",
            "Iteration 11, loss = 0.66715151\n",
            "Iteration 12, loss = 0.64398575\n",
            "Iteration 13, loss = 0.61379121\n",
            "Iteration 14, loss = 0.58979977\n",
            "Iteration 15, loss = 0.57411600\n",
            "Iteration 16, loss = 0.56296184\n",
            "Iteration 17, loss = 0.55443158\n",
            "Iteration 18, loss = 0.54706823\n",
            "Iteration 19, loss = 0.54062595\n",
            "Iteration 20, loss = 0.53480233\n",
            "Iteration 21, loss = 0.52932188\n",
            "Iteration 22, loss = 0.52405485\n",
            "Iteration 23, loss = 0.51889771\n",
            "Iteration 24, loss = 0.51382272\n",
            "Iteration 25, loss = 0.50896648\n",
            "Iteration 26, loss = 0.50415856\n",
            "Iteration 27, loss = 0.49962695\n",
            "Iteration 28, loss = 0.49536591\n",
            "Iteration 29, loss = 0.49138319\n",
            "Iteration 30, loss = 0.48764064\n",
            "Iteration 31, loss = 0.48422704\n",
            "Iteration 32, loss = 0.48121134\n",
            "Iteration 33, loss = 0.47846212\n",
            "Iteration 34, loss = 0.47614484\n",
            "Iteration 35, loss = 0.47403679\n",
            "Iteration 36, loss = 0.47225746\n",
            "Iteration 37, loss = 0.47076983\n",
            "Iteration 38, loss = 0.46934667\n",
            "Iteration 39, loss = 0.46829557\n",
            "Iteration 40, loss = 0.46700103\n",
            "Iteration 41, loss = 0.46624510\n",
            "Iteration 42, loss = 0.46518115\n",
            "Iteration 43, loss = 0.46436792\n",
            "Iteration 44, loss = 0.46352110\n",
            "Iteration 45, loss = 0.46293716\n",
            "Iteration 46, loss = 0.46220125\n",
            "Iteration 47, loss = 0.46161479\n",
            "Iteration 48, loss = 0.46110618\n",
            "Iteration 49, loss = 0.46048555\n",
            "Iteration 50, loss = 0.45990414\n",
            "Iteration 51, loss = 0.45953393\n",
            "Iteration 52, loss = 0.45908292\n",
            "Iteration 53, loss = 0.45862188\n",
            "Iteration 54, loss = 0.45797110\n",
            "Iteration 55, loss = 0.45783646\n",
            "Iteration 56, loss = 0.45749059\n",
            "Iteration 57, loss = 0.45702995\n",
            "Iteration 58, loss = 0.45662636\n",
            "Iteration 59, loss = 0.45632908\n",
            "Iteration 60, loss = 0.45617158\n",
            "Iteration 61, loss = 0.45574058\n",
            "Iteration 62, loss = 0.45530112\n",
            "Iteration 63, loss = 0.45516732\n",
            "Iteration 64, loss = 0.45483709\n",
            "Iteration 65, loss = 0.45451325\n",
            "Iteration 66, loss = 0.45417645\n",
            "Iteration 67, loss = 0.45415971\n",
            "Iteration 68, loss = 0.45373107\n",
            "Iteration 69, loss = 0.45341415\n",
            "Iteration 70, loss = 0.45332975\n",
            "Iteration 71, loss = 0.45310423\n",
            "Iteration 72, loss = 0.45298380\n",
            "Iteration 73, loss = 0.45269932\n",
            "Iteration 74, loss = 0.45245553\n",
            "Iteration 75, loss = 0.45232274\n",
            "Iteration 76, loss = 0.45213090\n",
            "Iteration 77, loss = 0.45201837\n",
            "Iteration 78, loss = 0.45189716\n",
            "Iteration 79, loss = 0.45168957\n",
            "Iteration 80, loss = 0.45146405\n",
            "Iteration 81, loss = 0.45131193\n",
            "Iteration 82, loss = 0.45116768\n",
            "Iteration 83, loss = 0.45098063\n",
            "Iteration 84, loss = 0.45091259\n",
            "Iteration 85, loss = 0.45073550\n",
            "Iteration 86, loss = 0.45064782\n",
            "Iteration 87, loss = 0.45049216\n",
            "Iteration 88, loss = 0.45037248\n",
            "Iteration 89, loss = 0.45017810\n",
            "Iteration 90, loss = 0.45011366\n",
            "Iteration 91, loss = 0.44982803\n",
            "Iteration 92, loss = 0.44978674\n",
            "Iteration 93, loss = 0.44970457\n",
            "Iteration 94, loss = 0.44951755\n",
            "Iteration 95, loss = 0.44967496\n",
            "Iteration 96, loss = 0.44948458\n",
            "Iteration 97, loss = 0.44925397\n",
            "Iteration 98, loss = 0.44916586\n",
            "Iteration 99, loss = 0.44920425\n",
            "Iteration 100, loss = 0.44903024\n",
            "Iteration 101, loss = 0.44887501\n",
            "Iteration 102, loss = 0.44890385\n",
            "Iteration 103, loss = 0.44871188\n",
            "Iteration 104, loss = 0.44863927\n",
            "Iteration 105, loss = 0.44862301\n",
            "Iteration 106, loss = 0.44847272\n",
            "Iteration 107, loss = 0.44838594\n",
            "Iteration 108, loss = 0.44837679\n",
            "Iteration 109, loss = 0.44811430\n",
            "Iteration 110, loss = 0.44810704\n",
            "Iteration 111, loss = 0.44798445\n",
            "Iteration 112, loss = 0.44815137\n",
            "Iteration 113, loss = 0.44796287\n",
            "Iteration 114, loss = 0.44782460\n",
            "Iteration 115, loss = 0.44768064\n",
            "Iteration 116, loss = 0.44752276\n",
            "Iteration 117, loss = 0.44756135\n",
            "Iteration 118, loss = 0.44762913\n",
            "Iteration 119, loss = 0.44748019\n",
            "Iteration 120, loss = 0.44725154\n",
            "Iteration 121, loss = 0.44730870\n",
            "Iteration 122, loss = 0.44715911\n",
            "Iteration 123, loss = 0.44724590\n",
            "Iteration 124, loss = 0.44705427\n",
            "Iteration 125, loss = 0.44696726\n",
            "Iteration 126, loss = 0.44697557\n",
            "Iteration 127, loss = 0.44712608\n",
            "Iteration 128, loss = 0.44692286\n",
            "Iteration 129, loss = 0.44690743\n",
            "Iteration 130, loss = 0.44675626\n",
            "Iteration 131, loss = 0.44678341\n",
            "Iteration 132, loss = 0.44654034\n",
            "Iteration 133, loss = 0.44670734\n",
            "Iteration 134, loss = 0.44644840\n",
            "Iteration 135, loss = 0.44641189\n",
            "Iteration 136, loss = 0.44641205\n",
            "Iteration 137, loss = 0.44640873\n",
            "Iteration 138, loss = 0.44642317\n",
            "Iteration 139, loss = 0.44631936\n",
            "Iteration 140, loss = 0.44623965\n",
            "Iteration 141, loss = 0.44625707\n",
            "Iteration 142, loss = 0.44603171\n",
            "Iteration 143, loss = 0.44603359\n",
            "Iteration 144, loss = 0.44575455\n",
            "Iteration 145, loss = 0.44609022\n",
            "Iteration 146, loss = 0.44605838\n",
            "Iteration 147, loss = 0.44574014\n",
            "Iteration 148, loss = 0.44580322\n",
            "Iteration 149, loss = 0.44555457\n",
            "Iteration 150, loss = 0.44576514\n",
            "Iteration 151, loss = 0.44566010\n",
            "Iteration 152, loss = 0.44565720\n",
            "Iteration 153, loss = 0.44562386\n",
            "Iteration 154, loss = 0.44554980\n",
            "Iteration 155, loss = 0.44548929\n",
            "Iteration 156, loss = 0.44542450\n",
            "Iteration 157, loss = 0.44535812\n",
            "Iteration 158, loss = 0.44536293\n",
            "Iteration 159, loss = 0.44523186\n",
            "Iteration 160, loss = 0.44532076\n",
            "Iteration 161, loss = 0.44510221\n",
            "Iteration 162, loss = 0.44525904\n",
            "Iteration 163, loss = 0.44506654\n",
            "Iteration 164, loss = 0.44497986\n",
            "Iteration 165, loss = 0.44521333\n",
            "Iteration 166, loss = 0.44506732\n",
            "Iteration 167, loss = 0.44483682\n",
            "Iteration 168, loss = 0.44521097\n",
            "Iteration 169, loss = 0.44493661\n",
            "Iteration 170, loss = 0.44502419\n",
            "Iteration 171, loss = 0.44488817\n",
            "Iteration 172, loss = 0.44478411\n",
            "Iteration 173, loss = 0.44469398\n",
            "Iteration 174, loss = 0.44474633\n",
            "Iteration 175, loss = 0.44464150\n",
            "Iteration 176, loss = 0.44454409\n",
            "Iteration 177, loss = 0.44448446\n",
            "Iteration 178, loss = 0.44460391\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 179, loss = 0.44459647\n",
            "Iteration 180, loss = 0.44413827\n",
            "Iteration 181, loss = 0.44413076\n",
            "Iteration 182, loss = 0.44413891\n",
            "Iteration 183, loss = 0.44410280\n",
            "Iteration 184, loss = 0.44416266\n",
            "Iteration 185, loss = 0.44415960\n",
            "Iteration 186, loss = 0.44405566\n",
            "Iteration 187, loss = 0.44414903\n",
            "Iteration 188, loss = 0.44407389\n",
            "Iteration 189, loss = 0.44407226\n",
            "Iteration 190, loss = 0.44412071\n",
            "Iteration 191, loss = 0.44406506\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 192, loss = 0.44401463\n",
            "Iteration 193, loss = 0.44395461\n",
            "Iteration 194, loss = 0.44396205\n",
            "Iteration 195, loss = 0.44395825\n",
            "Iteration 196, loss = 0.44396046\n",
            "Iteration 197, loss = 0.44397645\n",
            "Iteration 198, loss = 0.44396182\n",
            "Iteration 199, loss = 0.44395383\n",
            "Iteration 200, loss = 0.44395100\n",
            "Iteration 1, loss = 0.70308804\n",
            "Iteration 2, loss = 0.69722416\n",
            "Iteration 3, loss = 0.69486918\n",
            "Iteration 4, loss = 0.69377021\n",
            "Iteration 5, loss = 0.69312727\n",
            "Iteration 6, loss = 0.69253960\n",
            "Iteration 7, loss = 0.69165814\n",
            "Iteration 8, loss = 0.69003839\n",
            "Iteration 9, loss = 0.68687419\n",
            "Iteration 10, loss = 0.68054475\n",
            "Iteration 11, loss = 0.66805019\n",
            "Iteration 12, loss = 0.64574241\n",
            "Iteration 13, loss = 0.61603245\n",
            "Iteration 14, loss = 0.59166927\n",
            "Iteration 15, loss = 0.57563116\n",
            "Iteration 16, loss = 0.56438105\n",
            "Iteration 17, loss = 0.55579867\n",
            "Iteration 18, loss = 0.54841753\n",
            "Iteration 19, loss = 0.54204977\n",
            "Iteration 20, loss = 0.53622635\n",
            "Iteration 21, loss = 0.53082038\n",
            "Iteration 22, loss = 0.52569680\n",
            "Iteration 23, loss = 0.52038393\n",
            "Iteration 24, loss = 0.51527542\n",
            "Iteration 25, loss = 0.51031909\n",
            "Iteration 26, loss = 0.50546131\n",
            "Iteration 27, loss = 0.50079407\n",
            "Iteration 28, loss = 0.49643443\n",
            "Iteration 29, loss = 0.49226331\n",
            "Iteration 30, loss = 0.48847090\n",
            "Iteration 31, loss = 0.48502925\n",
            "Iteration 32, loss = 0.48202893\n",
            "Iteration 33, loss = 0.47923702\n",
            "Iteration 34, loss = 0.47685797\n",
            "Iteration 35, loss = 0.47475251\n",
            "Iteration 36, loss = 0.47294099\n",
            "Iteration 37, loss = 0.47141886\n",
            "Iteration 38, loss = 0.47001335\n",
            "Iteration 39, loss = 0.46883957\n",
            "Iteration 40, loss = 0.46760558\n",
            "Iteration 41, loss = 0.46668938\n",
            "Iteration 42, loss = 0.46592088\n",
            "Iteration 43, loss = 0.46489408\n",
            "Iteration 44, loss = 0.46418621\n",
            "Iteration 45, loss = 0.46340371\n",
            "Iteration 46, loss = 0.46277492\n",
            "Iteration 47, loss = 0.46213166\n",
            "Iteration 48, loss = 0.46172323\n",
            "Iteration 49, loss = 0.46099834\n",
            "Iteration 50, loss = 0.46032292\n",
            "Iteration 51, loss = 0.46001239\n",
            "Iteration 52, loss = 0.45946157\n",
            "Iteration 53, loss = 0.45898959\n",
            "Iteration 54, loss = 0.45842132\n",
            "Iteration 55, loss = 0.45811846\n",
            "Iteration 56, loss = 0.45780344\n",
            "Iteration 57, loss = 0.45736122\n",
            "Iteration 58, loss = 0.45692755\n",
            "Iteration 59, loss = 0.45664922\n",
            "Iteration 60, loss = 0.45639034\n",
            "Iteration 61, loss = 0.45601422\n",
            "Iteration 62, loss = 0.45577604\n",
            "Iteration 63, loss = 0.45538476\n",
            "Iteration 64, loss = 0.45494935\n",
            "Iteration 65, loss = 0.45475197\n",
            "Iteration 66, loss = 0.45446946\n",
            "Iteration 67, loss = 0.45433413\n",
            "Iteration 68, loss = 0.45398370\n",
            "Iteration 69, loss = 0.45356034\n",
            "Iteration 70, loss = 0.45351782\n",
            "Iteration 71, loss = 0.45319468\n",
            "Iteration 72, loss = 0.45290733\n",
            "Iteration 73, loss = 0.45290182\n",
            "Iteration 74, loss = 0.45252940\n",
            "Iteration 75, loss = 0.45239181\n",
            "Iteration 76, loss = 0.45209248\n",
            "Iteration 77, loss = 0.45194920\n",
            "Iteration 78, loss = 0.45183431\n",
            "Iteration 79, loss = 0.45161028\n",
            "Iteration 80, loss = 0.45147117\n",
            "Iteration 81, loss = 0.45136911\n",
            "Iteration 82, loss = 0.45119183\n",
            "Iteration 83, loss = 0.45084103\n",
            "Iteration 84, loss = 0.45095385\n",
            "Iteration 85, loss = 0.45071660\n",
            "Iteration 86, loss = 0.45043071\n",
            "Iteration 87, loss = 0.45040922\n",
            "Iteration 88, loss = 0.45028219\n",
            "Iteration 89, loss = 0.45004559\n",
            "Iteration 90, loss = 0.45002710\n",
            "Iteration 91, loss = 0.44966390\n",
            "Iteration 92, loss = 0.44965457\n",
            "Iteration 93, loss = 0.44953517\n",
            "Iteration 94, loss = 0.44937887\n",
            "Iteration 95, loss = 0.44946047\n",
            "Iteration 96, loss = 0.44920225\n",
            "Iteration 97, loss = 0.44910655\n",
            "Iteration 98, loss = 0.44899631\n",
            "Iteration 99, loss = 0.44901418\n",
            "Iteration 100, loss = 0.44872428\n",
            "Iteration 101, loss = 0.44865716\n",
            "Iteration 102, loss = 0.44860795\n",
            "Iteration 103, loss = 0.44847548\n",
            "Iteration 104, loss = 0.44843291\n",
            "Iteration 105, loss = 0.44840466\n",
            "Iteration 106, loss = 0.44827084\n",
            "Iteration 107, loss = 0.44797881\n",
            "Iteration 108, loss = 0.44795137\n",
            "Iteration 109, loss = 0.44777810\n",
            "Iteration 110, loss = 0.44787797\n",
            "Iteration 111, loss = 0.44777027\n",
            "Iteration 112, loss = 0.44779734\n",
            "Iteration 113, loss = 0.44773892\n",
            "Iteration 114, loss = 0.44767638\n",
            "Iteration 115, loss = 0.44730391\n",
            "Iteration 116, loss = 0.44726860\n",
            "Iteration 117, loss = 0.44716213\n",
            "Iteration 118, loss = 0.44712577\n",
            "Iteration 119, loss = 0.44698488\n",
            "Iteration 120, loss = 0.44688393\n",
            "Iteration 121, loss = 0.44692230\n",
            "Iteration 122, loss = 0.44690094\n",
            "Iteration 123, loss = 0.44674385\n",
            "Iteration 124, loss = 0.44661943\n",
            "Iteration 125, loss = 0.44671239\n",
            "Iteration 126, loss = 0.44659013\n",
            "Iteration 127, loss = 0.44662713\n",
            "Iteration 128, loss = 0.44647352\n",
            "Iteration 129, loss = 0.44638602\n",
            "Iteration 130, loss = 0.44618204\n",
            "Iteration 131, loss = 0.44623470\n",
            "Iteration 132, loss = 0.44610584\n",
            "Iteration 133, loss = 0.44608365\n",
            "Iteration 134, loss = 0.44598996\n",
            "Iteration 135, loss = 0.44581139\n",
            "Iteration 136, loss = 0.44583829\n",
            "Iteration 137, loss = 0.44579435\n",
            "Iteration 138, loss = 0.44579457\n",
            "Iteration 139, loss = 0.44576521\n",
            "Iteration 140, loss = 0.44574395\n",
            "Iteration 141, loss = 0.44570804\n",
            "Iteration 142, loss = 0.44538341\n",
            "Iteration 143, loss = 0.44542030\n",
            "Iteration 144, loss = 0.44529018\n",
            "Iteration 145, loss = 0.44527763\n",
            "Iteration 146, loss = 0.44532950\n",
            "Iteration 147, loss = 0.44508448\n",
            "Iteration 148, loss = 0.44508537\n",
            "Iteration 149, loss = 0.44502616\n",
            "Iteration 150, loss = 0.44493857\n",
            "Iteration 151, loss = 0.44486529\n",
            "Iteration 152, loss = 0.44507602\n",
            "Iteration 153, loss = 0.44481306\n",
            "Iteration 154, loss = 0.44480860\n",
            "Iteration 155, loss = 0.44477293\n",
            "Iteration 156, loss = 0.44458455\n",
            "Iteration 157, loss = 0.44466186\n",
            "Iteration 158, loss = 0.44445497\n",
            "Iteration 159, loss = 0.44433288\n",
            "Iteration 160, loss = 0.44453014\n",
            "Iteration 161, loss = 0.44428724\n",
            "Iteration 162, loss = 0.44449169\n",
            "Iteration 163, loss = 0.44429147\n",
            "Iteration 164, loss = 0.44423839\n",
            "Iteration 165, loss = 0.44419881\n",
            "Iteration 166, loss = 0.44417057\n",
            "Iteration 167, loss = 0.44405661\n",
            "Iteration 168, loss = 0.44416697\n",
            "Iteration 169, loss = 0.44407664\n",
            "Iteration 170, loss = 0.44402534\n",
            "Iteration 171, loss = 0.44410624\n",
            "Iteration 172, loss = 0.44390099\n",
            "Iteration 173, loss = 0.44377801\n",
            "Iteration 174, loss = 0.44380084\n",
            "Iteration 175, loss = 0.44377264\n",
            "Iteration 176, loss = 0.44360321\n",
            "Iteration 177, loss = 0.44367470\n",
            "Iteration 178, loss = 0.44375241\n",
            "Iteration 179, loss = 0.44357684\n",
            "Iteration 180, loss = 0.44370236\n",
            "Iteration 181, loss = 0.44355413\n",
            "Iteration 182, loss = 0.44346956\n",
            "Iteration 183, loss = 0.44350512\n",
            "Iteration 184, loss = 0.44355726\n",
            "Iteration 185, loss = 0.44334307\n",
            "Iteration 186, loss = 0.44309380\n",
            "Iteration 187, loss = 0.44322830\n",
            "Iteration 188, loss = 0.44301290\n",
            "Iteration 189, loss = 0.44326860\n",
            "Iteration 190, loss = 0.44297311\n",
            "Iteration 191, loss = 0.44301492\n",
            "Iteration 192, loss = 0.44294570\n",
            "Iteration 193, loss = 0.44286415\n",
            "Iteration 194, loss = 0.44307768\n",
            "Iteration 195, loss = 0.44284183\n",
            "Iteration 196, loss = 0.44285414\n",
            "Iteration 197, loss = 0.44292234\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 198, loss = 0.44258142\n",
            "Iteration 199, loss = 0.44244671\n",
            "Iteration 200, loss = 0.44237593\n",
            "Iteration 1, loss = 0.70307161\n",
            "Iteration 2, loss = 0.69723107\n",
            "Iteration 3, loss = 0.69482570\n",
            "Iteration 4, loss = 0.69371560\n",
            "Iteration 5, loss = 0.69304980\n",
            "Iteration 6, loss = 0.69237518\n",
            "Iteration 7, loss = 0.69130346\n",
            "Iteration 8, loss = 0.68935280\n",
            "Iteration 9, loss = 0.68553583\n",
            "Iteration 10, loss = 0.67786677\n",
            "Iteration 11, loss = 0.66282943\n",
            "Iteration 12, loss = 0.63689005\n",
            "Iteration 13, loss = 0.60664274\n",
            "Iteration 14, loss = 0.58469898\n",
            "Iteration 15, loss = 0.57027991\n",
            "Iteration 16, loss = 0.55980652\n",
            "Iteration 17, loss = 0.55149441\n",
            "Iteration 18, loss = 0.54444429\n",
            "Iteration 19, loss = 0.53830483\n",
            "Iteration 20, loss = 0.53255523\n",
            "Iteration 21, loss = 0.52727965\n",
            "Iteration 22, loss = 0.52205329\n",
            "Iteration 23, loss = 0.51691150\n",
            "Iteration 24, loss = 0.51177079\n",
            "Iteration 25, loss = 0.50694927\n",
            "Iteration 26, loss = 0.50219504\n",
            "Iteration 27, loss = 0.49753085\n",
            "Iteration 28, loss = 0.49318988\n",
            "Iteration 29, loss = 0.48923585\n",
            "Iteration 30, loss = 0.48561812\n",
            "Iteration 31, loss = 0.48225268\n",
            "Iteration 32, loss = 0.47936997\n",
            "Iteration 33, loss = 0.47672215\n",
            "Iteration 34, loss = 0.47452292\n",
            "Iteration 35, loss = 0.47250178\n",
            "Iteration 36, loss = 0.47086896\n",
            "Iteration 37, loss = 0.46934682\n",
            "Iteration 38, loss = 0.46800072\n",
            "Iteration 39, loss = 0.46684045\n",
            "Iteration 40, loss = 0.46573817\n",
            "Iteration 41, loss = 0.46486791\n",
            "Iteration 42, loss = 0.46409982\n",
            "Iteration 43, loss = 0.46315128\n",
            "Iteration 44, loss = 0.46236471\n",
            "Iteration 45, loss = 0.46167508\n",
            "Iteration 46, loss = 0.46095706\n",
            "Iteration 47, loss = 0.46037234\n",
            "Iteration 48, loss = 0.45992159\n",
            "Iteration 49, loss = 0.45924901\n",
            "Iteration 50, loss = 0.45861042\n",
            "Iteration 51, loss = 0.45826475\n",
            "Iteration 52, loss = 0.45780251\n",
            "Iteration 53, loss = 0.45722740\n",
            "Iteration 54, loss = 0.45672380\n",
            "Iteration 55, loss = 0.45631761\n",
            "Iteration 56, loss = 0.45597975\n",
            "Iteration 57, loss = 0.45561262\n",
            "Iteration 58, loss = 0.45515379\n",
            "Iteration 59, loss = 0.45486275\n",
            "Iteration 60, loss = 0.45461832\n",
            "Iteration 61, loss = 0.45426034\n",
            "Iteration 62, loss = 0.45396449\n",
            "Iteration 63, loss = 0.45358286\n",
            "Iteration 64, loss = 0.45324416\n",
            "Iteration 65, loss = 0.45301883\n",
            "Iteration 66, loss = 0.45281363\n",
            "Iteration 67, loss = 0.45265580\n",
            "Iteration 68, loss = 0.45224695\n",
            "Iteration 69, loss = 0.45200696\n",
            "Iteration 70, loss = 0.45187107\n",
            "Iteration 71, loss = 0.45143295\n",
            "Iteration 72, loss = 0.45156935\n",
            "Iteration 73, loss = 0.45123672\n",
            "Iteration 74, loss = 0.45100714\n",
            "Iteration 75, loss = 0.45079548\n",
            "Iteration 76, loss = 0.45053003\n",
            "Iteration 77, loss = 0.45034517\n",
            "Iteration 78, loss = 0.45009327\n",
            "Iteration 79, loss = 0.44997332\n",
            "Iteration 80, loss = 0.44998495\n",
            "Iteration 81, loss = 0.44991289\n",
            "Iteration 82, loss = 0.44956775\n",
            "Iteration 83, loss = 0.44936729\n",
            "Iteration 84, loss = 0.44943567\n",
            "Iteration 85, loss = 0.44918904\n",
            "Iteration 86, loss = 0.44895979\n",
            "Iteration 87, loss = 0.44890055\n",
            "Iteration 88, loss = 0.44880568\n",
            "Iteration 89, loss = 0.44851794\n",
            "Iteration 90, loss = 0.44848082\n",
            "Iteration 91, loss = 0.44837602\n",
            "Iteration 92, loss = 0.44814324\n",
            "Iteration 93, loss = 0.44804017\n",
            "Iteration 94, loss = 0.44799250\n",
            "Iteration 95, loss = 0.44795324\n",
            "Iteration 96, loss = 0.44765359\n",
            "Iteration 97, loss = 0.44759051\n",
            "Iteration 98, loss = 0.44767416\n",
            "Iteration 99, loss = 0.44751148\n",
            "Iteration 100, loss = 0.44722351\n",
            "Iteration 101, loss = 0.44719537\n",
            "Iteration 102, loss = 0.44700532\n",
            "Iteration 103, loss = 0.44704709\n",
            "Iteration 104, loss = 0.44712116\n",
            "Iteration 105, loss = 0.44705193\n",
            "Iteration 106, loss = 0.44679658\n",
            "Iteration 107, loss = 0.44662097\n",
            "Iteration 108, loss = 0.44662968\n",
            "Iteration 109, loss = 0.44647402\n",
            "Iteration 110, loss = 0.44636470\n",
            "Iteration 111, loss = 0.44631862\n",
            "Iteration 112, loss = 0.44646138\n",
            "Iteration 113, loss = 0.44630262\n",
            "Iteration 114, loss = 0.44637713\n",
            "Iteration 115, loss = 0.44606165\n",
            "Iteration 116, loss = 0.44587716\n",
            "Iteration 117, loss = 0.44585690\n",
            "Iteration 118, loss = 0.44572739\n",
            "Iteration 119, loss = 0.44577169\n",
            "Iteration 120, loss = 0.44567589\n",
            "Iteration 121, loss = 0.44568953\n",
            "Iteration 122, loss = 0.44582729\n",
            "Iteration 123, loss = 0.44541664\n",
            "Iteration 124, loss = 0.44540951\n",
            "Iteration 125, loss = 0.44534148\n",
            "Iteration 126, loss = 0.44528507\n",
            "Iteration 127, loss = 0.44518579\n",
            "Iteration 128, loss = 0.44534327\n",
            "Iteration 129, loss = 0.44509101\n",
            "Iteration 130, loss = 0.44503677\n",
            "Iteration 131, loss = 0.44494455\n",
            "Iteration 132, loss = 0.44484514\n",
            "Iteration 133, loss = 0.44479874\n",
            "Iteration 134, loss = 0.44492008\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 135, loss = 0.44526955\n",
            "Iteration 136, loss = 0.44442646\n",
            "Iteration 137, loss = 0.44440874\n",
            "Iteration 138, loss = 0.44441016\n",
            "Iteration 139, loss = 0.44440361\n",
            "Iteration 140, loss = 0.44435870\n",
            "Iteration 141, loss = 0.44433031\n",
            "Iteration 142, loss = 0.44434122\n",
            "Iteration 143, loss = 0.44432128\n",
            "Iteration 144, loss = 0.44430947\n",
            "Iteration 145, loss = 0.44430672\n",
            "Iteration 146, loss = 0.44430955\n",
            "Iteration 147, loss = 0.44426119\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 148, loss = 0.44422099\n",
            "Iteration 149, loss = 0.44417720\n",
            "Iteration 150, loss = 0.44417834\n",
            "Iteration 151, loss = 0.44418318\n",
            "Iteration 152, loss = 0.44417906\n",
            "Iteration 153, loss = 0.44418462\n",
            "Iteration 154, loss = 0.44416871\n",
            "Iteration 155, loss = 0.44416787\n",
            "Iteration 156, loss = 0.44415872\n",
            "Iteration 157, loss = 0.44415812\n",
            "Iteration 158, loss = 0.44415432\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 159, loss = 0.44413838\n",
            "Iteration 160, loss = 0.44413917\n",
            "Iteration 161, loss = 0.44413612\n",
            "Iteration 162, loss = 0.44413851\n",
            "Iteration 163, loss = 0.44413625\n",
            "Iteration 164, loss = 0.44413490\n",
            "Iteration 165, loss = 0.44413472\n",
            "Iteration 166, loss = 0.44413586\n",
            "Iteration 167, loss = 0.44413606\n",
            "Iteration 168, loss = 0.44413245\n",
            "Iteration 169, loss = 0.44413618\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 170, loss = 0.44412885\n",
            "Iteration 171, loss = 0.44412876\n",
            "Iteration 172, loss = 0.44412911\n",
            "Iteration 173, loss = 0.44412855\n",
            "Iteration 174, loss = 0.44412883\n",
            "Iteration 175, loss = 0.44412830\n",
            "Iteration 176, loss = 0.44412793\n",
            "Iteration 177, loss = 0.44412800\n",
            "Iteration 178, loss = 0.44412856\n",
            "Iteration 179, loss = 0.44412843\n",
            "Iteration 180, loss = 0.44412800\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 181, loss = 0.44412695\n",
            "Iteration 182, loss = 0.44412694\n",
            "Iteration 183, loss = 0.44412688\n",
            "Iteration 184, loss = 0.44412687\n",
            "Iteration 185, loss = 0.44412687\n",
            "Iteration 186, loss = 0.44412680\n",
            "Iteration 187, loss = 0.44412681\n",
            "Iteration 188, loss = 0.44412680\n",
            "Iteration 189, loss = 0.44412681\n",
            "Iteration 190, loss = 0.44412674\n",
            "Iteration 191, loss = 0.44412675\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.70308014\n",
            "Iteration 2, loss = 0.69721162\n",
            "Iteration 3, loss = 0.69481839\n",
            "Iteration 4, loss = 0.69371715\n",
            "Iteration 5, loss = 0.69306669\n",
            "Iteration 6, loss = 0.69242174\n",
            "Iteration 7, loss = 0.69141645\n",
            "Iteration 8, loss = 0.68956521\n",
            "Iteration 9, loss = 0.68597405\n",
            "Iteration 10, loss = 0.67886331\n",
            "Iteration 11, loss = 0.66509243\n",
            "Iteration 12, loss = 0.64133898\n",
            "Iteration 13, loss = 0.61211107\n",
            "Iteration 14, loss = 0.58926310\n",
            "Iteration 15, loss = 0.57430796\n",
            "Iteration 16, loss = 0.56360701\n",
            "Iteration 17, loss = 0.55508559\n",
            "Iteration 18, loss = 0.54798360\n",
            "Iteration 19, loss = 0.54180388\n",
            "Iteration 20, loss = 0.53611846\n",
            "Iteration 21, loss = 0.53089075\n",
            "Iteration 22, loss = 0.52578253\n",
            "Iteration 23, loss = 0.52067749\n",
            "Iteration 24, loss = 0.51564482\n",
            "Iteration 25, loss = 0.51090526\n",
            "Iteration 26, loss = 0.50620066\n",
            "Iteration 27, loss = 0.50162311\n",
            "Iteration 28, loss = 0.49714800\n",
            "Iteration 29, loss = 0.49304098\n",
            "Iteration 30, loss = 0.48925392\n",
            "Iteration 31, loss = 0.48570368\n",
            "Iteration 32, loss = 0.48262860\n",
            "Iteration 33, loss = 0.48000666\n",
            "Iteration 34, loss = 0.47758344\n",
            "Iteration 35, loss = 0.47560147\n",
            "Iteration 36, loss = 0.47383932\n",
            "Iteration 37, loss = 0.47233156\n",
            "Iteration 38, loss = 0.47099366\n",
            "Iteration 39, loss = 0.46979804\n",
            "Iteration 40, loss = 0.46876976\n",
            "Iteration 41, loss = 0.46784755\n",
            "Iteration 42, loss = 0.46716792\n",
            "Iteration 43, loss = 0.46618780\n",
            "Iteration 44, loss = 0.46534998\n",
            "Iteration 45, loss = 0.46464052\n",
            "Iteration 46, loss = 0.46397905\n",
            "Iteration 47, loss = 0.46337707\n",
            "Iteration 48, loss = 0.46269288\n",
            "Iteration 49, loss = 0.46209454\n",
            "Iteration 50, loss = 0.46157922\n",
            "Iteration 51, loss = 0.46116840\n",
            "Iteration 52, loss = 0.46063468\n",
            "Iteration 53, loss = 0.46013406\n",
            "Iteration 54, loss = 0.45979274\n",
            "Iteration 55, loss = 0.45920762\n",
            "Iteration 56, loss = 0.45888649\n",
            "Iteration 57, loss = 0.45850224\n",
            "Iteration 58, loss = 0.45806676\n",
            "Iteration 59, loss = 0.45785799\n",
            "Iteration 60, loss = 0.45744956\n",
            "Iteration 61, loss = 0.45710897\n",
            "Iteration 62, loss = 0.45695047\n",
            "Iteration 63, loss = 0.45664600\n",
            "Iteration 64, loss = 0.45616351\n",
            "Iteration 65, loss = 0.45611480\n",
            "Iteration 66, loss = 0.45573762\n",
            "Iteration 67, loss = 0.45546370\n",
            "Iteration 68, loss = 0.45524947\n",
            "Iteration 69, loss = 0.45498531\n",
            "Iteration 70, loss = 0.45471328\n",
            "Iteration 71, loss = 0.45433246\n",
            "Iteration 72, loss = 0.45432607\n",
            "Iteration 73, loss = 0.45411367\n",
            "Iteration 74, loss = 0.45396703\n",
            "Iteration 75, loss = 0.45369523\n",
            "Iteration 76, loss = 0.45350462\n",
            "Iteration 77, loss = 0.45337119\n",
            "Iteration 78, loss = 0.45306857\n",
            "Iteration 79, loss = 0.45283156\n",
            "Iteration 80, loss = 0.45290042\n",
            "Iteration 81, loss = 0.45285398\n",
            "Iteration 82, loss = 0.45258241\n",
            "Iteration 83, loss = 0.45234371\n",
            "Iteration 84, loss = 0.45230132\n",
            "Iteration 85, loss = 0.45209187\n",
            "Iteration 86, loss = 0.45177594\n",
            "Iteration 87, loss = 0.45188069\n",
            "Iteration 88, loss = 0.45169557\n",
            "Iteration 89, loss = 0.45143895\n",
            "Iteration 90, loss = 0.45133479\n",
            "Iteration 91, loss = 0.45125626\n",
            "Iteration 92, loss = 0.45085533\n",
            "Iteration 93, loss = 0.45092466\n",
            "Iteration 94, loss = 0.45088800\n",
            "Iteration 95, loss = 0.45089009\n",
            "Iteration 96, loss = 0.45052614\n",
            "Iteration 97, loss = 0.45044875\n",
            "Iteration 98, loss = 0.45049415\n",
            "Iteration 99, loss = 0.45027399\n",
            "Iteration 100, loss = 0.45003379\n",
            "Iteration 101, loss = 0.45013037\n",
            "Iteration 102, loss = 0.44997951\n",
            "Iteration 103, loss = 0.44992572\n",
            "Iteration 104, loss = 0.44990671\n",
            "Iteration 105, loss = 0.44968890\n",
            "Iteration 106, loss = 0.44953691\n",
            "Iteration 107, loss = 0.44934635\n",
            "Iteration 108, loss = 0.44920031\n",
            "Iteration 109, loss = 0.44926442\n",
            "Iteration 110, loss = 0.44908378\n",
            "Iteration 111, loss = 0.44911760\n",
            "Iteration 112, loss = 0.44914325\n",
            "Iteration 113, loss = 0.44885057\n",
            "Iteration 114, loss = 0.44884944\n",
            "Iteration 115, loss = 0.44864587\n",
            "Iteration 116, loss = 0.44862620\n",
            "Iteration 117, loss = 0.44846078\n",
            "Iteration 118, loss = 0.44836736\n",
            "Iteration 119, loss = 0.44847468\n",
            "Iteration 120, loss = 0.44843011\n",
            "Iteration 121, loss = 0.44832133\n",
            "Iteration 122, loss = 0.44844343\n",
            "Iteration 123, loss = 0.44796058\n",
            "Iteration 124, loss = 0.44817862\n",
            "Iteration 125, loss = 0.44802702\n",
            "Iteration 126, loss = 0.44795794\n",
            "Iteration 127, loss = 0.44785052\n",
            "Iteration 128, loss = 0.44793039\n",
            "Iteration 129, loss = 0.44769725\n",
            "Iteration 130, loss = 0.44768688\n",
            "Iteration 131, loss = 0.44742662\n",
            "Iteration 132, loss = 0.44756803\n",
            "Iteration 133, loss = 0.44745043\n",
            "Iteration 134, loss = 0.44745792\n",
            "Iteration 135, loss = 0.44731339\n",
            "Iteration 136, loss = 0.44722205\n",
            "Iteration 137, loss = 0.44715367\n",
            "Iteration 138, loss = 0.44701068\n",
            "Iteration 139, loss = 0.44711256\n",
            "Iteration 140, loss = 0.44701498\n",
            "Iteration 141, loss = 0.44693316\n",
            "Iteration 142, loss = 0.44693856\n",
            "Iteration 143, loss = 0.44690419\n",
            "Iteration 144, loss = 0.44678373\n",
            "Iteration 145, loss = 0.44670932\n",
            "Iteration 146, loss = 0.44666394\n",
            "Iteration 147, loss = 0.44657425\n",
            "Iteration 148, loss = 0.44651137\n",
            "Iteration 149, loss = 0.44643791\n",
            "Iteration 150, loss = 0.44641562\n",
            "Iteration 151, loss = 0.44636889\n",
            "Iteration 152, loss = 0.44640738\n",
            "Iteration 153, loss = 0.44626149\n",
            "Iteration 154, loss = 0.44635973\n",
            "Iteration 155, loss = 0.44614236\n",
            "Iteration 156, loss = 0.44614582\n",
            "Iteration 157, loss = 0.44624452\n",
            "Iteration 158, loss = 0.44599557\n",
            "Iteration 159, loss = 0.44603038\n",
            "Iteration 160, loss = 0.44600778\n",
            "Iteration 161, loss = 0.44579125\n",
            "Iteration 162, loss = 0.44580589\n",
            "Iteration 163, loss = 0.44583720\n",
            "Iteration 164, loss = 0.44566616\n",
            "Iteration 165, loss = 0.44551746\n",
            "Iteration 166, loss = 0.44546827\n",
            "Iteration 167, loss = 0.44555745\n",
            "Iteration 168, loss = 0.44548410\n",
            "Iteration 169, loss = 0.44530188\n",
            "Iteration 170, loss = 0.44543028\n",
            "Iteration 171, loss = 0.44547250\n",
            "Iteration 172, loss = 0.44527671\n",
            "Iteration 173, loss = 0.44520602\n",
            "Iteration 174, loss = 0.44531995\n",
            "Iteration 175, loss = 0.44556077\n",
            "Iteration 176, loss = 0.44519538\n",
            "Iteration 177, loss = 0.44518802\n",
            "Iteration 178, loss = 0.44494012\n",
            "Iteration 179, loss = 0.44516838\n",
            "Iteration 180, loss = 0.44502106\n",
            "Iteration 181, loss = 0.44499851\n",
            "Iteration 182, loss = 0.44484866\n",
            "Iteration 183, loss = 0.44479626\n",
            "Iteration 184, loss = 0.44477422\n",
            "Iteration 185, loss = 0.44473474\n",
            "Iteration 186, loss = 0.44467076\n",
            "Iteration 187, loss = 0.44458572\n",
            "Iteration 188, loss = 0.44459022\n",
            "Iteration 189, loss = 0.44447555\n",
            "Iteration 190, loss = 0.44451221\n",
            "Iteration 191, loss = 0.44452018\n",
            "Iteration 192, loss = 0.44442463\n",
            "Iteration 193, loss = 0.44445400\n",
            "Iteration 194, loss = 0.44438565\n",
            "Iteration 195, loss = 0.44436938\n",
            "Iteration 196, loss = 0.44425138\n",
            "Iteration 197, loss = 0.44450319\n",
            "Iteration 198, loss = 0.44425560\n",
            "Iteration 199, loss = 0.44410375\n",
            "Iteration 200, loss = 0.44411741\n",
            "Iteration 1, loss = 0.70259997\n",
            "Iteration 2, loss = 0.69655096\n",
            "Iteration 3, loss = 0.69440360\n",
            "Iteration 4, loss = 0.69346387\n",
            "Iteration 5, loss = 0.69279568\n",
            "Iteration 6, loss = 0.69191850\n",
            "Iteration 7, loss = 0.69026929\n",
            "Iteration 8, loss = 0.68684875\n",
            "Iteration 9, loss = 0.67940987\n",
            "Iteration 10, loss = 0.66333257\n",
            "Iteration 11, loss = 0.63404188\n",
            "Iteration 12, loss = 0.60127699\n",
            "Iteration 13, loss = 0.57937607\n",
            "Iteration 14, loss = 0.56524754\n",
            "Iteration 15, loss = 0.55478102\n",
            "Iteration 16, loss = 0.54629719\n",
            "Iteration 17, loss = 0.53909901\n",
            "Iteration 18, loss = 0.53253797\n",
            "Iteration 19, loss = 0.52635836\n",
            "Iteration 20, loss = 0.52037209\n",
            "Iteration 21, loss = 0.51461532\n",
            "Iteration 22, loss = 0.50909639\n",
            "Iteration 23, loss = 0.50361636\n",
            "Iteration 24, loss = 0.49838317\n",
            "Iteration 25, loss = 0.49346204\n",
            "Iteration 26, loss = 0.48909870\n",
            "Iteration 27, loss = 0.48506404\n",
            "Iteration 28, loss = 0.48147796\n",
            "Iteration 29, loss = 0.47841955\n",
            "Iteration 30, loss = 0.47579333\n",
            "Iteration 31, loss = 0.47347203\n",
            "Iteration 32, loss = 0.47152202\n",
            "Iteration 33, loss = 0.46984025\n",
            "Iteration 34, loss = 0.46825599\n",
            "Iteration 35, loss = 0.46694426\n",
            "Iteration 36, loss = 0.46576820\n",
            "Iteration 37, loss = 0.46482925\n",
            "Iteration 38, loss = 0.46383695\n",
            "Iteration 39, loss = 0.46293314\n",
            "Iteration 40, loss = 0.46208210\n",
            "Iteration 41, loss = 0.46146400\n",
            "Iteration 42, loss = 0.46063846\n",
            "Iteration 43, loss = 0.45992111\n",
            "Iteration 44, loss = 0.45936185\n",
            "Iteration 45, loss = 0.45888763\n",
            "Iteration 46, loss = 0.45824937\n",
            "Iteration 47, loss = 0.45764360\n",
            "Iteration 48, loss = 0.45712125\n",
            "Iteration 49, loss = 0.45659132\n",
            "Iteration 50, loss = 0.45646964\n",
            "Iteration 51, loss = 0.45588369\n",
            "Iteration 52, loss = 0.45541350\n",
            "Iteration 53, loss = 0.45509679\n",
            "Iteration 54, loss = 0.45467067\n",
            "Iteration 55, loss = 0.45433096\n",
            "Iteration 56, loss = 0.45387873\n",
            "Iteration 57, loss = 0.45354005\n",
            "Iteration 58, loss = 0.45333278\n",
            "Iteration 59, loss = 0.45299922\n",
            "Iteration 60, loss = 0.45270820\n",
            "Iteration 61, loss = 0.45236072\n",
            "Iteration 62, loss = 0.45219706\n",
            "Iteration 63, loss = 0.45185892\n",
            "Iteration 64, loss = 0.45185632\n",
            "Iteration 65, loss = 0.45145005\n",
            "Iteration 66, loss = 0.45129929\n",
            "Iteration 67, loss = 0.45099598\n",
            "Iteration 68, loss = 0.45080475\n",
            "Iteration 69, loss = 0.45059561\n",
            "Iteration 70, loss = 0.45024479\n",
            "Iteration 71, loss = 0.45019558\n",
            "Iteration 72, loss = 0.44998081\n",
            "Iteration 73, loss = 0.44973673\n",
            "Iteration 74, loss = 0.44986439\n",
            "Iteration 75, loss = 0.44942073\n",
            "Iteration 76, loss = 0.44938605\n",
            "Iteration 77, loss = 0.44917162\n",
            "Iteration 78, loss = 0.44895615\n",
            "Iteration 79, loss = 0.44879715\n",
            "Iteration 80, loss = 0.44871476\n",
            "Iteration 81, loss = 0.44852888\n",
            "Iteration 82, loss = 0.44836031\n",
            "Iteration 83, loss = 0.44831043\n",
            "Iteration 84, loss = 0.44811022\n",
            "Iteration 85, loss = 0.44810723\n",
            "Iteration 86, loss = 0.44794050\n",
            "Iteration 87, loss = 0.44783754\n",
            "Iteration 88, loss = 0.44762865\n",
            "Iteration 89, loss = 0.44759920\n",
            "Iteration 90, loss = 0.44732821\n",
            "Iteration 91, loss = 0.44723480\n",
            "Iteration 92, loss = 0.44735766\n",
            "Iteration 93, loss = 0.44705905\n",
            "Iteration 94, loss = 0.44693148\n",
            "Iteration 95, loss = 0.44687465\n",
            "Iteration 96, loss = 0.44699099\n",
            "Iteration 97, loss = 0.44655646\n",
            "Iteration 98, loss = 0.44651082\n",
            "Iteration 99, loss = 0.44664441\n",
            "Iteration 100, loss = 0.44636985\n",
            "Iteration 101, loss = 0.44621081\n",
            "Iteration 102, loss = 0.44633669\n",
            "Iteration 103, loss = 0.44602072\n",
            "Iteration 104, loss = 0.44618017\n",
            "Iteration 105, loss = 0.44601013\n",
            "Iteration 106, loss = 0.44586473\n",
            "Iteration 107, loss = 0.44584398\n",
            "Iteration 108, loss = 0.44561486\n",
            "Iteration 109, loss = 0.44559221\n",
            "Iteration 110, loss = 0.44555415\n",
            "Iteration 111, loss = 0.44549246\n",
            "Iteration 112, loss = 0.44531010\n",
            "Iteration 113, loss = 0.44521438\n",
            "Iteration 114, loss = 0.44519805\n",
            "Iteration 115, loss = 0.44509304\n",
            "Iteration 116, loss = 0.44492614\n",
            "Iteration 117, loss = 0.44499441\n",
            "Iteration 118, loss = 0.44504368\n",
            "Iteration 119, loss = 0.44489135\n",
            "Iteration 120, loss = 0.44485385\n",
            "Iteration 121, loss = 0.44495586\n",
            "Iteration 122, loss = 0.44457217\n",
            "Iteration 123, loss = 0.44449104\n",
            "Iteration 124, loss = 0.44473506\n",
            "Iteration 125, loss = 0.44438940\n",
            "Iteration 126, loss = 0.44448095\n",
            "Iteration 127, loss = 0.44433365\n",
            "Iteration 128, loss = 0.44422569\n",
            "Iteration 129, loss = 0.44427134\n",
            "Iteration 130, loss = 0.44423406\n",
            "Iteration 131, loss = 0.44411181\n",
            "Iteration 132, loss = 0.44403570\n",
            "Iteration 133, loss = 0.44389028\n",
            "Iteration 134, loss = 0.44391387\n",
            "Iteration 135, loss = 0.44387239\n",
            "Iteration 136, loss = 0.44379907\n",
            "Iteration 137, loss = 0.44372457\n",
            "Iteration 138, loss = 0.44362389\n",
            "Iteration 139, loss = 0.44363004\n",
            "Iteration 140, loss = 0.44350148\n",
            "Iteration 141, loss = 0.44348443\n",
            "Iteration 142, loss = 0.44331523\n",
            "Iteration 143, loss = 0.44336878\n",
            "Iteration 144, loss = 0.44340696\n",
            "Iteration 145, loss = 0.44328201\n",
            "Iteration 146, loss = 0.44322141\n",
            "Iteration 147, loss = 0.44314253\n",
            "Iteration 148, loss = 0.44313739\n",
            "Iteration 149, loss = 0.44299620\n",
            "Iteration 150, loss = 0.44291706\n",
            "Iteration 151, loss = 0.44287278\n",
            "Iteration 152, loss = 0.44279324\n",
            "Iteration 153, loss = 0.44275463\n",
            "Iteration 154, loss = 0.44279216\n",
            "Iteration 155, loss = 0.44271507\n",
            "Iteration 156, loss = 0.44280437\n",
            "Iteration 157, loss = 0.44280543\n",
            "Iteration 158, loss = 0.44259882\n",
            "Iteration 159, loss = 0.44257293\n",
            "Iteration 160, loss = 0.44257577\n",
            "Iteration 161, loss = 0.44243549\n",
            "Iteration 162, loss = 0.44243985\n",
            "Iteration 163, loss = 0.44216908\n",
            "Iteration 164, loss = 0.44225209\n",
            "Iteration 165, loss = 0.44200839\n",
            "Iteration 166, loss = 0.44203339\n",
            "Iteration 167, loss = 0.44195463\n",
            "Iteration 168, loss = 0.44204967\n",
            "Iteration 169, loss = 0.44182236\n",
            "Iteration 170, loss = 0.44205626\n",
            "Iteration 171, loss = 0.44181570\n",
            "Iteration 172, loss = 0.44175056\n",
            "Iteration 173, loss = 0.44192111\n",
            "Iteration 174, loss = 0.44167067\n",
            "Iteration 175, loss = 0.44170387\n",
            "Iteration 176, loss = 0.44174066\n",
            "Iteration 177, loss = 0.44152228\n",
            "Iteration 178, loss = 0.44144123\n",
            "Iteration 179, loss = 0.44138326\n",
            "Iteration 180, loss = 0.44151282\n",
            "Iteration 181, loss = 0.44138477\n",
            "Iteration 182, loss = 0.44151234\n",
            "Iteration 183, loss = 0.44137359\n",
            "Iteration 184, loss = 0.44117170\n",
            "Iteration 185, loss = 0.44128088\n",
            "Iteration 186, loss = 0.44111865\n",
            "Iteration 187, loss = 0.44128730\n",
            "Iteration 188, loss = 0.44118585\n",
            "Iteration 189, loss = 0.44091338\n",
            "Iteration 190, loss = 0.44097503\n",
            "Iteration 191, loss = 0.44100914\n",
            "Iteration 192, loss = 0.44087265\n",
            "Iteration 193, loss = 0.44099142\n",
            "Iteration 194, loss = 0.44096768\n",
            "Iteration 195, loss = 0.44081288\n",
            "Iteration 196, loss = 0.44080979\n",
            "Iteration 197, loss = 0.44067458\n",
            "Iteration 198, loss = 0.44067792\n",
            "Iteration 199, loss = 0.44059702\n",
            "Iteration 200, loss = 0.44072083\n",
            "Iteration 1, loss = 0.70258587\n",
            "Iteration 2, loss = 0.69656777\n",
            "Iteration 3, loss = 0.69439877\n",
            "Iteration 4, loss = 0.69344030\n",
            "Iteration 5, loss = 0.69274156\n",
            "Iteration 6, loss = 0.69179367\n",
            "Iteration 7, loss = 0.69000680\n",
            "Iteration 8, loss = 0.68629079\n",
            "Iteration 9, loss = 0.67822522\n",
            "Iteration 10, loss = 0.66099214\n",
            "Iteration 11, loss = 0.63067730\n",
            "Iteration 12, loss = 0.59885702\n",
            "Iteration 13, loss = 0.57811449\n",
            "Iteration 14, loss = 0.56457580\n",
            "Iteration 15, loss = 0.55442569\n",
            "Iteration 16, loss = 0.54614926\n",
            "Iteration 17, loss = 0.53896394\n",
            "Iteration 18, loss = 0.53245266\n",
            "Iteration 19, loss = 0.52626910\n",
            "Iteration 20, loss = 0.52032536\n",
            "Iteration 21, loss = 0.51466780\n",
            "Iteration 22, loss = 0.50910415\n",
            "Iteration 23, loss = 0.50363490\n",
            "Iteration 24, loss = 0.49843769\n",
            "Iteration 25, loss = 0.49354520\n",
            "Iteration 26, loss = 0.48921508\n",
            "Iteration 27, loss = 0.48515395\n",
            "Iteration 28, loss = 0.48154305\n",
            "Iteration 29, loss = 0.47851316\n",
            "Iteration 30, loss = 0.47588588\n",
            "Iteration 31, loss = 0.47354074\n",
            "Iteration 32, loss = 0.47166020\n",
            "Iteration 33, loss = 0.46997129\n",
            "Iteration 34, loss = 0.46843495\n",
            "Iteration 35, loss = 0.46715415\n",
            "Iteration 36, loss = 0.46597946\n",
            "Iteration 37, loss = 0.46511725\n",
            "Iteration 38, loss = 0.46402970\n",
            "Iteration 39, loss = 0.46323580\n",
            "Iteration 40, loss = 0.46236517\n",
            "Iteration 41, loss = 0.46176224\n",
            "Iteration 42, loss = 0.46091325\n",
            "Iteration 43, loss = 0.46026116\n",
            "Iteration 44, loss = 0.45968954\n",
            "Iteration 45, loss = 0.45920037\n",
            "Iteration 46, loss = 0.45863178\n",
            "Iteration 47, loss = 0.45805680\n",
            "Iteration 48, loss = 0.45758673\n",
            "Iteration 49, loss = 0.45707088\n",
            "Iteration 50, loss = 0.45692441\n",
            "Iteration 51, loss = 0.45635113\n",
            "Iteration 52, loss = 0.45590492\n",
            "Iteration 53, loss = 0.45568550\n",
            "Iteration 54, loss = 0.45522689\n",
            "Iteration 55, loss = 0.45487974\n",
            "Iteration 56, loss = 0.45450967\n",
            "Iteration 57, loss = 0.45415973\n",
            "Iteration 58, loss = 0.45408399\n",
            "Iteration 59, loss = 0.45366497\n",
            "Iteration 60, loss = 0.45337978\n",
            "Iteration 61, loss = 0.45310307\n",
            "Iteration 62, loss = 0.45287380\n",
            "Iteration 63, loss = 0.45264974\n",
            "Iteration 64, loss = 0.45251180\n",
            "Iteration 65, loss = 0.45219709\n",
            "Iteration 66, loss = 0.45196501\n",
            "Iteration 67, loss = 0.45174427\n",
            "Iteration 68, loss = 0.45159900\n",
            "Iteration 69, loss = 0.45134391\n",
            "Iteration 70, loss = 0.45105552\n",
            "Iteration 71, loss = 0.45099111\n",
            "Iteration 72, loss = 0.45074892\n",
            "Iteration 73, loss = 0.45047507\n",
            "Iteration 74, loss = 0.45059037\n",
            "Iteration 75, loss = 0.45026349\n",
            "Iteration 76, loss = 0.45014963\n",
            "Iteration 77, loss = 0.44987981\n",
            "Iteration 78, loss = 0.44969976\n",
            "Iteration 79, loss = 0.44959154\n",
            "Iteration 80, loss = 0.44956470\n",
            "Iteration 81, loss = 0.44926870\n",
            "Iteration 82, loss = 0.44915814\n",
            "Iteration 83, loss = 0.44908578\n",
            "Iteration 84, loss = 0.44891422\n",
            "Iteration 85, loss = 0.44887331\n",
            "Iteration 86, loss = 0.44866900\n",
            "Iteration 87, loss = 0.44861421\n",
            "Iteration 88, loss = 0.44840317\n",
            "Iteration 89, loss = 0.44834854\n",
            "Iteration 90, loss = 0.44814906\n",
            "Iteration 91, loss = 0.44802301\n",
            "Iteration 92, loss = 0.44813737\n",
            "Iteration 93, loss = 0.44791236\n",
            "Iteration 94, loss = 0.44776085\n",
            "Iteration 95, loss = 0.44759222\n",
            "Iteration 96, loss = 0.44767869\n",
            "Iteration 97, loss = 0.44735558\n",
            "Iteration 98, loss = 0.44731433\n",
            "Iteration 99, loss = 0.44742357\n",
            "Iteration 100, loss = 0.44711633\n",
            "Iteration 101, loss = 0.44697876\n",
            "Iteration 102, loss = 0.44706636\n",
            "Iteration 103, loss = 0.44681478\n",
            "Iteration 104, loss = 0.44683711\n",
            "Iteration 105, loss = 0.44684678\n",
            "Iteration 106, loss = 0.44663008\n",
            "Iteration 107, loss = 0.44662042\n",
            "Iteration 108, loss = 0.44641961\n",
            "Iteration 109, loss = 0.44639276\n",
            "Iteration 110, loss = 0.44640739\n",
            "Iteration 111, loss = 0.44626568\n",
            "Iteration 112, loss = 0.44612316\n",
            "Iteration 113, loss = 0.44610241\n",
            "Iteration 114, loss = 0.44609472\n",
            "Iteration 115, loss = 0.44591965\n",
            "Iteration 116, loss = 0.44576632\n",
            "Iteration 117, loss = 0.44582138\n",
            "Iteration 118, loss = 0.44583639\n",
            "Iteration 119, loss = 0.44564851\n",
            "Iteration 120, loss = 0.44570724\n",
            "Iteration 121, loss = 0.44578510\n",
            "Iteration 122, loss = 0.44544441\n",
            "Iteration 123, loss = 0.44535172\n",
            "Iteration 124, loss = 0.44553335\n",
            "Iteration 125, loss = 0.44523972\n",
            "Iteration 126, loss = 0.44527750\n",
            "Iteration 127, loss = 0.44517580\n",
            "Iteration 128, loss = 0.44508737\n",
            "Iteration 129, loss = 0.44534297\n",
            "Iteration 130, loss = 0.44513967\n",
            "Iteration 131, loss = 0.44490228\n",
            "Iteration 132, loss = 0.44491554\n",
            "Iteration 133, loss = 0.44478958\n",
            "Iteration 134, loss = 0.44482724\n",
            "Iteration 135, loss = 0.44476698\n",
            "Iteration 136, loss = 0.44470128\n",
            "Iteration 137, loss = 0.44478029\n",
            "Iteration 138, loss = 0.44459638\n",
            "Iteration 139, loss = 0.44457935\n",
            "Iteration 140, loss = 0.44440895\n",
            "Iteration 141, loss = 0.44443651\n",
            "Iteration 142, loss = 0.44436011\n",
            "Iteration 143, loss = 0.44431242\n",
            "Iteration 144, loss = 0.44432338\n",
            "Iteration 145, loss = 0.44420816\n",
            "Iteration 146, loss = 0.44421139\n",
            "Iteration 147, loss = 0.44425560\n",
            "Iteration 148, loss = 0.44400318\n",
            "Iteration 149, loss = 0.44401085\n",
            "Iteration 150, loss = 0.44396823\n",
            "Iteration 151, loss = 0.44398796\n",
            "Iteration 152, loss = 0.44384974\n",
            "Iteration 153, loss = 0.44384896\n",
            "Iteration 154, loss = 0.44386640\n",
            "Iteration 155, loss = 0.44370893\n",
            "Iteration 156, loss = 0.44369108\n",
            "Iteration 157, loss = 0.44378321\n",
            "Iteration 158, loss = 0.44355459\n",
            "Iteration 159, loss = 0.44365416\n",
            "Iteration 160, loss = 0.44374105\n",
            "Iteration 161, loss = 0.44351632\n",
            "Iteration 162, loss = 0.44348258\n",
            "Iteration 163, loss = 0.44333949\n",
            "Iteration 164, loss = 0.44336683\n",
            "Iteration 165, loss = 0.44310463\n",
            "Iteration 166, loss = 0.44319653\n",
            "Iteration 167, loss = 0.44309049\n",
            "Iteration 168, loss = 0.44312492\n",
            "Iteration 169, loss = 0.44299629\n",
            "Iteration 170, loss = 0.44317444\n",
            "Iteration 171, loss = 0.44304577\n",
            "Iteration 172, loss = 0.44293719\n",
            "Iteration 173, loss = 0.44308671\n",
            "Iteration 174, loss = 0.44288438\n",
            "Iteration 175, loss = 0.44297357\n",
            "Iteration 176, loss = 0.44295826\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 177, loss = 0.44255012\n",
            "Iteration 178, loss = 0.44245590\n",
            "Iteration 179, loss = 0.44233968\n",
            "Iteration 180, loss = 0.44236432\n",
            "Iteration 181, loss = 0.44233823\n",
            "Iteration 182, loss = 0.44235955\n",
            "Iteration 183, loss = 0.44231405\n",
            "Iteration 184, loss = 0.44231817\n",
            "Iteration 185, loss = 0.44232062\n",
            "Iteration 186, loss = 0.44233049\n",
            "Iteration 187, loss = 0.44229129\n",
            "Iteration 188, loss = 0.44227387\n",
            "Iteration 189, loss = 0.44224626\n",
            "Iteration 190, loss = 0.44227636\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 191, loss = 0.44217801\n",
            "Iteration 192, loss = 0.44217186\n",
            "Iteration 193, loss = 0.44216676\n",
            "Iteration 194, loss = 0.44217016\n",
            "Iteration 195, loss = 0.44216103\n",
            "Iteration 196, loss = 0.44215664\n",
            "Iteration 197, loss = 0.44217184\n",
            "Iteration 198, loss = 0.44216754\n",
            "Iteration 199, loss = 0.44215002\n",
            "Iteration 200, loss = 0.44215617\n",
            "Iteration 1, loss = 0.70259292\n",
            "Iteration 2, loss = 0.69656659\n",
            "Iteration 3, loss = 0.69440423\n",
            "Iteration 4, loss = 0.69345932\n",
            "Iteration 5, loss = 0.69278537\n",
            "Iteration 6, loss = 0.69189536\n",
            "Iteration 7, loss = 0.69021859\n",
            "Iteration 8, loss = 0.68673273\n",
            "Iteration 9, loss = 0.67919402\n",
            "Iteration 10, loss = 0.66311907\n",
            "Iteration 11, loss = 0.63415370\n",
            "Iteration 12, loss = 0.60197250\n",
            "Iteration 13, loss = 0.58038617\n",
            "Iteration 14, loss = 0.56646288\n",
            "Iteration 15, loss = 0.55613619\n",
            "Iteration 16, loss = 0.54775921\n",
            "Iteration 17, loss = 0.54060525\n",
            "Iteration 18, loss = 0.53413800\n",
            "Iteration 19, loss = 0.52806534\n",
            "Iteration 20, loss = 0.52223310\n",
            "Iteration 21, loss = 0.51661841\n",
            "Iteration 22, loss = 0.51121612\n",
            "Iteration 23, loss = 0.50583862\n",
            "Iteration 24, loss = 0.50078290\n",
            "Iteration 25, loss = 0.49598360\n",
            "Iteration 26, loss = 0.49158239\n",
            "Iteration 27, loss = 0.48751848\n",
            "Iteration 28, loss = 0.48377267\n",
            "Iteration 29, loss = 0.48072879\n",
            "Iteration 30, loss = 0.47794903\n",
            "Iteration 31, loss = 0.47545891\n",
            "Iteration 32, loss = 0.47354261\n",
            "Iteration 33, loss = 0.47182911\n",
            "Iteration 34, loss = 0.47016705\n",
            "Iteration 35, loss = 0.46880281\n",
            "Iteration 36, loss = 0.46761728\n",
            "Iteration 37, loss = 0.46663608\n",
            "Iteration 38, loss = 0.46554012\n",
            "Iteration 39, loss = 0.46471027\n",
            "Iteration 40, loss = 0.46393178\n",
            "Iteration 41, loss = 0.46321644\n",
            "Iteration 42, loss = 0.46236407\n",
            "Iteration 43, loss = 0.46170085\n",
            "Iteration 44, loss = 0.46104120\n",
            "Iteration 45, loss = 0.46063851\n",
            "Iteration 46, loss = 0.45998451\n",
            "Iteration 47, loss = 0.45953877\n",
            "Iteration 48, loss = 0.45896281\n",
            "Iteration 49, loss = 0.45847909\n",
            "Iteration 50, loss = 0.45820639\n",
            "Iteration 51, loss = 0.45775045\n",
            "Iteration 52, loss = 0.45727578\n",
            "Iteration 53, loss = 0.45700754\n",
            "Iteration 54, loss = 0.45657379\n",
            "Iteration 55, loss = 0.45623406\n",
            "Iteration 56, loss = 0.45599128\n",
            "Iteration 57, loss = 0.45557433\n",
            "Iteration 58, loss = 0.45547586\n",
            "Iteration 59, loss = 0.45501565\n",
            "Iteration 60, loss = 0.45479845\n",
            "Iteration 61, loss = 0.45457466\n",
            "Iteration 62, loss = 0.45419602\n",
            "Iteration 63, loss = 0.45396604\n",
            "Iteration 64, loss = 0.45382398\n",
            "Iteration 65, loss = 0.45351664\n",
            "Iteration 66, loss = 0.45336347\n",
            "Iteration 67, loss = 0.45317698\n",
            "Iteration 68, loss = 0.45292635\n",
            "Iteration 69, loss = 0.45276740\n",
            "Iteration 70, loss = 0.45236113\n",
            "Iteration 71, loss = 0.45245616\n",
            "Iteration 72, loss = 0.45206232\n",
            "Iteration 73, loss = 0.45191874\n",
            "Iteration 74, loss = 0.45192453\n",
            "Iteration 75, loss = 0.45169550\n",
            "Iteration 76, loss = 0.45155291\n",
            "Iteration 77, loss = 0.45134580\n",
            "Iteration 78, loss = 0.45104473\n",
            "Iteration 79, loss = 0.45097162\n",
            "Iteration 80, loss = 0.45094955\n",
            "Iteration 81, loss = 0.45069105\n",
            "Iteration 82, loss = 0.45055768\n",
            "Iteration 83, loss = 0.45048866\n",
            "Iteration 84, loss = 0.45027807\n",
            "Iteration 85, loss = 0.45019817\n",
            "Iteration 86, loss = 0.45009935\n",
            "Iteration 87, loss = 0.45002252\n",
            "Iteration 88, loss = 0.44980760\n",
            "Iteration 89, loss = 0.44977308\n",
            "Iteration 90, loss = 0.44955653\n",
            "Iteration 91, loss = 0.44945868\n",
            "Iteration 92, loss = 0.44952070\n",
            "Iteration 93, loss = 0.44934398\n",
            "Iteration 94, loss = 0.44930794\n",
            "Iteration 95, loss = 0.44898363\n",
            "Iteration 96, loss = 0.44917857\n",
            "Iteration 97, loss = 0.44884026\n",
            "Iteration 98, loss = 0.44871750\n",
            "Iteration 99, loss = 0.44876440\n",
            "Iteration 100, loss = 0.44862183\n",
            "Iteration 101, loss = 0.44846359\n",
            "Iteration 102, loss = 0.44843548\n",
            "Iteration 103, loss = 0.44824747\n",
            "Iteration 104, loss = 0.44832305\n",
            "Iteration 105, loss = 0.44828507\n",
            "Iteration 106, loss = 0.44803151\n",
            "Iteration 107, loss = 0.44811719\n",
            "Iteration 108, loss = 0.44786328\n",
            "Iteration 109, loss = 0.44782920\n",
            "Iteration 110, loss = 0.44790160\n",
            "Iteration 111, loss = 0.44765636\n",
            "Iteration 112, loss = 0.44760598\n",
            "Iteration 113, loss = 0.44753739\n",
            "Iteration 114, loss = 0.44754056\n",
            "Iteration 115, loss = 0.44737335\n",
            "Iteration 116, loss = 0.44720166\n",
            "Iteration 117, loss = 0.44735767\n",
            "Iteration 118, loss = 0.44732736\n",
            "Iteration 119, loss = 0.44709255\n",
            "Iteration 120, loss = 0.44721882\n",
            "Iteration 121, loss = 0.44725791\n",
            "Iteration 122, loss = 0.44697123\n",
            "Iteration 123, loss = 0.44683365\n",
            "Iteration 124, loss = 0.44701007\n",
            "Iteration 125, loss = 0.44661076\n",
            "Iteration 126, loss = 0.44673048\n",
            "Iteration 127, loss = 0.44665976\n",
            "Iteration 128, loss = 0.44660433\n",
            "Iteration 129, loss = 0.44678082\n",
            "Iteration 130, loss = 0.44661036\n",
            "Iteration 131, loss = 0.44640641\n",
            "Iteration 132, loss = 0.44633900\n",
            "Iteration 133, loss = 0.44631712\n",
            "Iteration 134, loss = 0.44632824\n",
            "Iteration 135, loss = 0.44623225\n",
            "Iteration 136, loss = 0.44618635\n",
            "Iteration 137, loss = 0.44630634\n",
            "Iteration 138, loss = 0.44621268\n",
            "Iteration 139, loss = 0.44607019\n",
            "Iteration 140, loss = 0.44596612\n",
            "Iteration 141, loss = 0.44600872\n",
            "Iteration 142, loss = 0.44598839\n",
            "Iteration 143, loss = 0.44582320\n",
            "Iteration 144, loss = 0.44589373\n",
            "Iteration 145, loss = 0.44569560\n",
            "Iteration 146, loss = 0.44569500\n",
            "Iteration 147, loss = 0.44575153\n",
            "Iteration 148, loss = 0.44551776\n",
            "Iteration 149, loss = 0.44560731\n",
            "Iteration 150, loss = 0.44552079\n",
            "Iteration 151, loss = 0.44551266\n",
            "Iteration 152, loss = 0.44540294\n",
            "Iteration 153, loss = 0.44530083\n",
            "Iteration 154, loss = 0.44540139\n",
            "Iteration 155, loss = 0.44530120\n",
            "Iteration 156, loss = 0.44520499\n",
            "Iteration 157, loss = 0.44523816\n",
            "Iteration 158, loss = 0.44514645\n",
            "Iteration 159, loss = 0.44519824\n",
            "Iteration 160, loss = 0.44533823\n",
            "Iteration 161, loss = 0.44506755\n",
            "Iteration 162, loss = 0.44496923\n",
            "Iteration 163, loss = 0.44489831\n",
            "Iteration 164, loss = 0.44489980\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 165, loss = 0.44439594\n",
            "Iteration 166, loss = 0.44455055\n",
            "Iteration 167, loss = 0.44444817\n",
            "Iteration 168, loss = 0.44446718\n",
            "Iteration 169, loss = 0.44447467\n",
            "Iteration 170, loss = 0.44446881\n",
            "Iteration 171, loss = 0.44443490\n",
            "Iteration 172, loss = 0.44447029\n",
            "Iteration 173, loss = 0.44446372\n",
            "Iteration 174, loss = 0.44441047\n",
            "Iteration 175, loss = 0.44443726\n",
            "Iteration 176, loss = 0.44442590\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 177, loss = 0.44430579\n",
            "Iteration 178, loss = 0.44432847\n",
            "Iteration 179, loss = 0.44430680\n",
            "Iteration 180, loss = 0.44430593\n",
            "Iteration 181, loss = 0.44429194\n",
            "Iteration 182, loss = 0.44430025\n",
            "Iteration 183, loss = 0.44429926\n",
            "Iteration 184, loss = 0.44429982\n",
            "Iteration 185, loss = 0.44429263\n",
            "Iteration 186, loss = 0.44429484\n",
            "Iteration 187, loss = 0.44428772\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 188, loss = 0.44426940\n",
            "Iteration 189, loss = 0.44426978\n",
            "Iteration 190, loss = 0.44426909\n",
            "Iteration 191, loss = 0.44426820\n",
            "Iteration 192, loss = 0.44426634\n",
            "Iteration 193, loss = 0.44426659\n",
            "Iteration 194, loss = 0.44426777\n",
            "Iteration 195, loss = 0.44426587\n",
            "Iteration 196, loss = 0.44426437\n",
            "Iteration 197, loss = 0.44426694\n",
            "Iteration 198, loss = 0.44426613\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 199, loss = 0.44426029\n",
            "Iteration 200, loss = 0.44425988\n",
            "Iteration 1, loss = 0.70263443\n",
            "Iteration 2, loss = 0.69659657\n",
            "Iteration 3, loss = 0.69442373\n",
            "Iteration 4, loss = 0.69349531\n",
            "Iteration 5, loss = 0.69284479\n",
            "Iteration 6, loss = 0.69200160\n",
            "Iteration 7, loss = 0.69044092\n",
            "Iteration 8, loss = 0.68721568\n",
            "Iteration 9, loss = 0.68025919\n",
            "Iteration 10, loss = 0.66528063\n",
            "Iteration 11, loss = 0.63734987\n",
            "Iteration 12, loss = 0.60436984\n",
            "Iteration 13, loss = 0.58155645\n",
            "Iteration 14, loss = 0.56703699\n",
            "Iteration 15, loss = 0.55640436\n",
            "Iteration 16, loss = 0.54784373\n",
            "Iteration 17, loss = 0.54060393\n",
            "Iteration 18, loss = 0.53396549\n",
            "Iteration 19, loss = 0.52788169\n",
            "Iteration 20, loss = 0.52181064\n",
            "Iteration 21, loss = 0.51604419\n",
            "Iteration 22, loss = 0.51044570\n",
            "Iteration 23, loss = 0.50486428\n",
            "Iteration 24, loss = 0.49964343\n",
            "Iteration 25, loss = 0.49469301\n",
            "Iteration 26, loss = 0.49015741\n",
            "Iteration 27, loss = 0.48592090\n",
            "Iteration 28, loss = 0.48214606\n",
            "Iteration 29, loss = 0.47900878\n",
            "Iteration 30, loss = 0.47619379\n",
            "Iteration 31, loss = 0.47374422\n",
            "Iteration 32, loss = 0.47171033\n",
            "Iteration 33, loss = 0.46999966\n",
            "Iteration 34, loss = 0.46844041\n",
            "Iteration 35, loss = 0.46709514\n",
            "Iteration 36, loss = 0.46598757\n",
            "Iteration 37, loss = 0.46505618\n",
            "Iteration 38, loss = 0.46403818\n",
            "Iteration 39, loss = 0.46303965\n",
            "Iteration 40, loss = 0.46233515\n",
            "Iteration 41, loss = 0.46157839\n",
            "Iteration 42, loss = 0.46078061\n",
            "Iteration 43, loss = 0.46018754\n",
            "Iteration 44, loss = 0.45948187\n",
            "Iteration 45, loss = 0.45901200\n",
            "Iteration 46, loss = 0.45843330\n",
            "Iteration 47, loss = 0.45797376\n",
            "Iteration 48, loss = 0.45743139\n",
            "Iteration 49, loss = 0.45695898\n",
            "Iteration 50, loss = 0.45660609\n",
            "Iteration 51, loss = 0.45615341\n",
            "Iteration 52, loss = 0.45566582\n",
            "Iteration 53, loss = 0.45538165\n",
            "Iteration 54, loss = 0.45495078\n",
            "Iteration 55, loss = 0.45455682\n",
            "Iteration 56, loss = 0.45432938\n",
            "Iteration 57, loss = 0.45394474\n",
            "Iteration 58, loss = 0.45379968\n",
            "Iteration 59, loss = 0.45338126\n",
            "Iteration 60, loss = 0.45314533\n",
            "Iteration 61, loss = 0.45293044\n",
            "Iteration 62, loss = 0.45254304\n",
            "Iteration 63, loss = 0.45234012\n",
            "Iteration 64, loss = 0.45215993\n",
            "Iteration 65, loss = 0.45188402\n",
            "Iteration 66, loss = 0.45165844\n",
            "Iteration 67, loss = 0.45142456\n",
            "Iteration 68, loss = 0.45126678\n",
            "Iteration 69, loss = 0.45113866\n",
            "Iteration 70, loss = 0.45075710\n",
            "Iteration 71, loss = 0.45073974\n",
            "Iteration 72, loss = 0.45045078\n",
            "Iteration 73, loss = 0.45034966\n",
            "Iteration 74, loss = 0.45034704\n",
            "Iteration 75, loss = 0.45002412\n",
            "Iteration 76, loss = 0.44989257\n",
            "Iteration 77, loss = 0.44968864\n",
            "Iteration 78, loss = 0.44942060\n",
            "Iteration 79, loss = 0.44934861\n",
            "Iteration 80, loss = 0.44926540\n",
            "Iteration 81, loss = 0.44913047\n",
            "Iteration 82, loss = 0.44897540\n",
            "Iteration 83, loss = 0.44890757\n",
            "Iteration 84, loss = 0.44862572\n",
            "Iteration 85, loss = 0.44852131\n",
            "Iteration 86, loss = 0.44846700\n",
            "Iteration 87, loss = 0.44840342\n",
            "Iteration 88, loss = 0.44816684\n",
            "Iteration 89, loss = 0.44815263\n",
            "Iteration 90, loss = 0.44799764\n",
            "Iteration 91, loss = 0.44787526\n",
            "Iteration 92, loss = 0.44785135\n",
            "Iteration 93, loss = 0.44775128\n",
            "Iteration 94, loss = 0.44772568\n",
            "Iteration 95, loss = 0.44753669\n",
            "Iteration 96, loss = 0.44757603\n",
            "Iteration 97, loss = 0.44734637\n",
            "Iteration 98, loss = 0.44731663\n",
            "Iteration 99, loss = 0.44714239\n",
            "Iteration 100, loss = 0.44705519\n",
            "Iteration 101, loss = 0.44695823\n",
            "Iteration 102, loss = 0.44685835\n",
            "Iteration 103, loss = 0.44668235\n",
            "Iteration 104, loss = 0.44671046\n",
            "Iteration 105, loss = 0.44671688\n",
            "Iteration 106, loss = 0.44649666\n",
            "Iteration 107, loss = 0.44659689\n",
            "Iteration 108, loss = 0.44645269\n",
            "Iteration 109, loss = 0.44625924\n",
            "Iteration 110, loss = 0.44642273\n",
            "Iteration 111, loss = 0.44616897\n",
            "Iteration 112, loss = 0.44606109\n",
            "Iteration 113, loss = 0.44609364\n",
            "Iteration 114, loss = 0.44598355\n",
            "Iteration 115, loss = 0.44584770\n",
            "Iteration 116, loss = 0.44571993\n",
            "Iteration 117, loss = 0.44585444\n",
            "Iteration 118, loss = 0.44573337\n",
            "Iteration 119, loss = 0.44554881\n",
            "Iteration 120, loss = 0.44556738\n",
            "Iteration 121, loss = 0.44565608\n",
            "Iteration 122, loss = 0.44545661\n",
            "Iteration 123, loss = 0.44533482\n",
            "Iteration 124, loss = 0.44540828\n",
            "Iteration 125, loss = 0.44511417\n",
            "Iteration 126, loss = 0.44518679\n",
            "Iteration 127, loss = 0.44510320\n",
            "Iteration 128, loss = 0.44507554\n",
            "Iteration 129, loss = 0.44514971\n",
            "Iteration 130, loss = 0.44500058\n",
            "Iteration 131, loss = 0.44483013\n",
            "Iteration 132, loss = 0.44481409\n",
            "Iteration 133, loss = 0.44474078\n",
            "Iteration 134, loss = 0.44471036\n",
            "Iteration 135, loss = 0.44470443\n",
            "Iteration 136, loss = 0.44466463\n",
            "Iteration 137, loss = 0.44465930\n",
            "Iteration 138, loss = 0.44455661\n",
            "Iteration 139, loss = 0.44458161\n",
            "Iteration 140, loss = 0.44434078\n",
            "Iteration 141, loss = 0.44429215\n",
            "Iteration 142, loss = 0.44438530\n",
            "Iteration 143, loss = 0.44427887\n",
            "Iteration 144, loss = 0.44435808\n",
            "Iteration 145, loss = 0.44410531\n",
            "Iteration 146, loss = 0.44408271\n",
            "Iteration 147, loss = 0.44420296\n",
            "Iteration 148, loss = 0.44389550\n",
            "Iteration 149, loss = 0.44399207\n",
            "Iteration 150, loss = 0.44390773\n",
            "Iteration 151, loss = 0.44394215\n",
            "Iteration 152, loss = 0.44383899\n",
            "Iteration 153, loss = 0.44371321\n",
            "Iteration 154, loss = 0.44373808\n",
            "Iteration 155, loss = 0.44365180\n",
            "Iteration 156, loss = 0.44365388\n",
            "Iteration 157, loss = 0.44351481\n",
            "Iteration 158, loss = 0.44355450\n",
            "Iteration 159, loss = 0.44358543\n",
            "Iteration 160, loss = 0.44375559\n",
            "Iteration 161, loss = 0.44345849\n",
            "Iteration 162, loss = 0.44333825\n",
            "Iteration 163, loss = 0.44326025\n",
            "Iteration 164, loss = 0.44342347\n",
            "Iteration 165, loss = 0.44320616\n",
            "Iteration 166, loss = 0.44324171\n",
            "Iteration 167, loss = 0.44318548\n",
            "Iteration 168, loss = 0.44315155\n",
            "Iteration 169, loss = 0.44302690\n",
            "Iteration 170, loss = 0.44307459\n",
            "Iteration 171, loss = 0.44304847\n",
            "Iteration 172, loss = 0.44298509\n",
            "Iteration 173, loss = 0.44317939\n",
            "Iteration 174, loss = 0.44285397\n",
            "Iteration 175, loss = 0.44281217\n",
            "Iteration 176, loss = 0.44304029\n",
            "Iteration 177, loss = 0.44291234\n",
            "Iteration 178, loss = 0.44268102\n",
            "Iteration 179, loss = 0.44260189\n",
            "Iteration 180, loss = 0.44271231\n",
            "Iteration 181, loss = 0.44257361\n",
            "Iteration 182, loss = 0.44271935\n",
            "Iteration 183, loss = 0.44267467\n",
            "Iteration 184, loss = 0.44250940\n",
            "Iteration 185, loss = 0.44252108\n",
            "Iteration 186, loss = 0.44248349\n",
            "Iteration 187, loss = 0.44234578\n",
            "Iteration 188, loss = 0.44237585\n",
            "Iteration 189, loss = 0.44223708\n",
            "Iteration 190, loss = 0.44228640\n",
            "Iteration 191, loss = 0.44227171\n",
            "Iteration 192, loss = 0.44219837\n",
            "Iteration 193, loss = 0.44207041\n",
            "Iteration 194, loss = 0.44207749\n",
            "Iteration 195, loss = 0.44198010\n",
            "Iteration 196, loss = 0.44213504\n",
            "Iteration 197, loss = 0.44214067\n",
            "Iteration 198, loss = 0.44198007\n",
            "Iteration 199, loss = 0.44185276\n",
            "Iteration 200, loss = 0.44184896\n",
            "Iteration 1, loss = 0.70260169\n",
            "Iteration 2, loss = 0.69656662\n",
            "Iteration 3, loss = 0.69439767\n",
            "Iteration 4, loss = 0.69344404\n",
            "Iteration 5, loss = 0.69274921\n",
            "Iteration 6, loss = 0.69181810\n",
            "Iteration 7, loss = 0.69006104\n",
            "Iteration 8, loss = 0.68637876\n",
            "Iteration 9, loss = 0.67838160\n",
            "Iteration 10, loss = 0.66149557\n",
            "Iteration 11, loss = 0.63181904\n",
            "Iteration 12, loss = 0.60009574\n",
            "Iteration 13, loss = 0.57916504\n",
            "Iteration 14, loss = 0.56556591\n",
            "Iteration 15, loss = 0.55542493\n",
            "Iteration 16, loss = 0.54724961\n",
            "Iteration 17, loss = 0.54028877\n",
            "Iteration 18, loss = 0.53385316\n",
            "Iteration 19, loss = 0.52800388\n",
            "Iteration 20, loss = 0.52211966\n",
            "Iteration 21, loss = 0.51654511\n",
            "Iteration 22, loss = 0.51109906\n",
            "Iteration 23, loss = 0.50572924\n",
            "Iteration 24, loss = 0.50057143\n",
            "Iteration 25, loss = 0.49581556\n",
            "Iteration 26, loss = 0.49135751\n",
            "Iteration 27, loss = 0.48719642\n",
            "Iteration 28, loss = 0.48344626\n",
            "Iteration 29, loss = 0.48031233\n",
            "Iteration 30, loss = 0.47747797\n",
            "Iteration 31, loss = 0.47501846\n",
            "Iteration 32, loss = 0.47293699\n",
            "Iteration 33, loss = 0.47123152\n",
            "Iteration 34, loss = 0.46958140\n",
            "Iteration 35, loss = 0.46820298\n",
            "Iteration 36, loss = 0.46713943\n",
            "Iteration 37, loss = 0.46620578\n",
            "Iteration 38, loss = 0.46522703\n",
            "Iteration 39, loss = 0.46408607\n",
            "Iteration 40, loss = 0.46337488\n",
            "Iteration 41, loss = 0.46256590\n",
            "Iteration 42, loss = 0.46174737\n",
            "Iteration 43, loss = 0.46110802\n",
            "Iteration 44, loss = 0.46042679\n",
            "Iteration 45, loss = 0.45982319\n",
            "Iteration 46, loss = 0.45928297\n",
            "Iteration 47, loss = 0.45877080\n",
            "Iteration 48, loss = 0.45818795\n",
            "Iteration 49, loss = 0.45777199\n",
            "Iteration 50, loss = 0.45743774\n",
            "Iteration 51, loss = 0.45689256\n",
            "Iteration 52, loss = 0.45650941\n",
            "Iteration 53, loss = 0.45609328\n",
            "Iteration 54, loss = 0.45566995\n",
            "Iteration 55, loss = 0.45526715\n",
            "Iteration 56, loss = 0.45498810\n",
            "Iteration 57, loss = 0.45462653\n",
            "Iteration 58, loss = 0.45447760\n",
            "Iteration 59, loss = 0.45408747\n",
            "Iteration 60, loss = 0.45375222\n",
            "Iteration 61, loss = 0.45356488\n",
            "Iteration 62, loss = 0.45311123\n",
            "Iteration 63, loss = 0.45298666\n",
            "Iteration 64, loss = 0.45279056\n",
            "Iteration 65, loss = 0.45241463\n",
            "Iteration 66, loss = 0.45223390\n",
            "Iteration 67, loss = 0.45206262\n",
            "Iteration 68, loss = 0.45180917\n",
            "Iteration 69, loss = 0.45163365\n",
            "Iteration 70, loss = 0.45139045\n",
            "Iteration 71, loss = 0.45128179\n",
            "Iteration 72, loss = 0.45098443\n",
            "Iteration 73, loss = 0.45094829\n",
            "Iteration 74, loss = 0.45074011\n",
            "Iteration 75, loss = 0.45060212\n",
            "Iteration 76, loss = 0.45046110\n",
            "Iteration 77, loss = 0.45021013\n",
            "Iteration 78, loss = 0.44987298\n",
            "Iteration 79, loss = 0.44981935\n",
            "Iteration 80, loss = 0.44973389\n",
            "Iteration 81, loss = 0.44961962\n",
            "Iteration 82, loss = 0.44953017\n",
            "Iteration 83, loss = 0.44943896\n",
            "Iteration 84, loss = 0.44906433\n",
            "Iteration 85, loss = 0.44905186\n",
            "Iteration 86, loss = 0.44890704\n",
            "Iteration 87, loss = 0.44878415\n",
            "Iteration 88, loss = 0.44860344\n",
            "Iteration 89, loss = 0.44860792\n",
            "Iteration 90, loss = 0.44840273\n",
            "Iteration 91, loss = 0.44827421\n",
            "Iteration 92, loss = 0.44829970\n",
            "Iteration 93, loss = 0.44822829\n",
            "Iteration 94, loss = 0.44809923\n",
            "Iteration 95, loss = 0.44797076\n",
            "Iteration 96, loss = 0.44787992\n",
            "Iteration 97, loss = 0.44778747\n",
            "Iteration 98, loss = 0.44773349\n",
            "Iteration 99, loss = 0.44756448\n",
            "Iteration 100, loss = 0.44745652\n",
            "Iteration 101, loss = 0.44732485\n",
            "Iteration 102, loss = 0.44726296\n",
            "Iteration 103, loss = 0.44711565\n",
            "Iteration 104, loss = 0.44710381\n",
            "Iteration 105, loss = 0.44704962\n",
            "Iteration 106, loss = 0.44687758\n",
            "Iteration 107, loss = 0.44691279\n",
            "Iteration 108, loss = 0.44680374\n",
            "Iteration 109, loss = 0.44656083\n",
            "Iteration 110, loss = 0.44681681\n",
            "Iteration 111, loss = 0.44651941\n",
            "Iteration 112, loss = 0.44638251\n",
            "Iteration 113, loss = 0.44644186\n",
            "Iteration 114, loss = 0.44622797\n",
            "Iteration 115, loss = 0.44620830\n",
            "Iteration 116, loss = 0.44607943\n",
            "Iteration 117, loss = 0.44611162\n",
            "Iteration 118, loss = 0.44598270\n",
            "Iteration 119, loss = 0.44582907\n",
            "Iteration 120, loss = 0.44585069\n",
            "Iteration 121, loss = 0.44587520\n",
            "Iteration 122, loss = 0.44579039\n",
            "Iteration 123, loss = 0.44558734\n",
            "Iteration 124, loss = 0.44573614\n",
            "Iteration 125, loss = 0.44544111\n",
            "Iteration 126, loss = 0.44532857\n",
            "Iteration 127, loss = 0.44536025\n",
            "Iteration 128, loss = 0.44531972\n",
            "Iteration 129, loss = 0.44524273\n",
            "Iteration 130, loss = 0.44534227\n",
            "Iteration 131, loss = 0.44504023\n",
            "Iteration 132, loss = 0.44512289\n",
            "Iteration 133, loss = 0.44502509\n",
            "Iteration 134, loss = 0.44490448\n",
            "Iteration 135, loss = 0.44492342\n",
            "Iteration 136, loss = 0.44487054\n",
            "Iteration 137, loss = 0.44490455\n",
            "Iteration 138, loss = 0.44484616\n",
            "Iteration 139, loss = 0.44468405\n",
            "Iteration 140, loss = 0.44450727\n",
            "Iteration 141, loss = 0.44448800\n",
            "Iteration 142, loss = 0.44448617\n",
            "Iteration 143, loss = 0.44437139\n",
            "Iteration 144, loss = 0.44444421\n",
            "Iteration 145, loss = 0.44434158\n",
            "Iteration 146, loss = 0.44425790\n",
            "Iteration 147, loss = 0.44433570\n",
            "Iteration 148, loss = 0.44409072\n",
            "Iteration 149, loss = 0.44411272\n",
            "Iteration 150, loss = 0.44401655\n",
            "Iteration 151, loss = 0.44406030\n",
            "Iteration 152, loss = 0.44398583\n",
            "Iteration 153, loss = 0.44386018\n",
            "Iteration 154, loss = 0.44386846\n",
            "Iteration 155, loss = 0.44379894\n",
            "Iteration 156, loss = 0.44381402\n",
            "Iteration 157, loss = 0.44354278\n",
            "Iteration 158, loss = 0.44361331\n",
            "Iteration 159, loss = 0.44365268\n",
            "Iteration 160, loss = 0.44370684\n",
            "Iteration 161, loss = 0.44352237\n",
            "Iteration 162, loss = 0.44341658\n",
            "Iteration 163, loss = 0.44325072\n",
            "Iteration 164, loss = 0.44336004\n",
            "Iteration 165, loss = 0.44333933\n",
            "Iteration 166, loss = 0.44330096\n",
            "Iteration 167, loss = 0.44326603\n",
            "Iteration 168, loss = 0.44308572\n",
            "Iteration 169, loss = 0.44307840\n",
            "Iteration 170, loss = 0.44313627\n",
            "Iteration 171, loss = 0.44313646\n",
            "Iteration 172, loss = 0.44290096\n",
            "Iteration 173, loss = 0.44317760\n",
            "Iteration 174, loss = 0.44281430\n",
            "Iteration 175, loss = 0.44266955\n",
            "Iteration 176, loss = 0.44297371\n",
            "Iteration 177, loss = 0.44293803\n",
            "Iteration 178, loss = 0.44260678\n",
            "Iteration 179, loss = 0.44262243\n",
            "Iteration 180, loss = 0.44260396\n",
            "Iteration 181, loss = 0.44250924\n",
            "Iteration 182, loss = 0.44255566\n",
            "Iteration 183, loss = 0.44253261\n",
            "Iteration 184, loss = 0.44236027\n",
            "Iteration 185, loss = 0.44244152\n",
            "Iteration 186, loss = 0.44248932\n",
            "Iteration 187, loss = 0.44226378\n",
            "Iteration 188, loss = 0.44225812\n",
            "Iteration 189, loss = 0.44214821\n",
            "Iteration 190, loss = 0.44213877\n",
            "Iteration 191, loss = 0.44226445\n",
            "Iteration 192, loss = 0.44217944\n",
            "Iteration 193, loss = 0.44200317\n",
            "Iteration 194, loss = 0.44197806\n",
            "Iteration 195, loss = 0.44188745\n",
            "Iteration 196, loss = 0.44208977\n",
            "Iteration 197, loss = 0.44207579\n",
            "Iteration 198, loss = 0.44182012\n",
            "Iteration 199, loss = 0.44174678\n",
            "Iteration 200, loss = 0.44171681\n",
            "Iteration 1, loss = 0.70259547\n",
            "Iteration 2, loss = 0.69660092\n",
            "Iteration 3, loss = 0.69445210\n",
            "Iteration 4, loss = 0.69352481\n",
            "Iteration 5, loss = 0.69288661\n",
            "Iteration 6, loss = 0.69208415\n",
            "Iteration 7, loss = 0.69062384\n",
            "Iteration 8, loss = 0.68762689\n",
            "Iteration 9, loss = 0.68116420\n",
            "Iteration 10, loss = 0.66718962\n",
            "Iteration 11, loss = 0.64075089\n",
            "Iteration 12, loss = 0.60782920\n",
            "Iteration 13, loss = 0.58411408\n",
            "Iteration 14, loss = 0.56899947\n",
            "Iteration 15, loss = 0.55807149\n",
            "Iteration 16, loss = 0.54938432\n",
            "Iteration 17, loss = 0.54192464\n",
            "Iteration 18, loss = 0.53523566\n",
            "Iteration 19, loss = 0.52910423\n",
            "Iteration 20, loss = 0.52316776\n",
            "Iteration 21, loss = 0.51737917\n",
            "Iteration 22, loss = 0.51164242\n",
            "Iteration 23, loss = 0.50612678\n",
            "Iteration 24, loss = 0.50075283\n",
            "Iteration 25, loss = 0.49566065\n",
            "Iteration 26, loss = 0.49106515\n",
            "Iteration 27, loss = 0.48675886\n",
            "Iteration 28, loss = 0.48312922\n",
            "Iteration 29, loss = 0.47985987\n",
            "Iteration 30, loss = 0.47698230\n",
            "Iteration 31, loss = 0.47470479\n",
            "Iteration 32, loss = 0.47259465\n",
            "Iteration 33, loss = 0.47090911\n",
            "Iteration 34, loss = 0.46939948\n",
            "Iteration 35, loss = 0.46793945\n",
            "Iteration 36, loss = 0.46678811\n",
            "Iteration 37, loss = 0.46574686\n",
            "Iteration 38, loss = 0.46476302\n",
            "Iteration 39, loss = 0.46395931\n",
            "Iteration 40, loss = 0.46309661\n",
            "Iteration 41, loss = 0.46235977\n",
            "Iteration 42, loss = 0.46168495\n",
            "Iteration 43, loss = 0.46104540\n",
            "Iteration 44, loss = 0.46043162\n",
            "Iteration 45, loss = 0.45981731\n",
            "Iteration 46, loss = 0.45934917\n",
            "Iteration 47, loss = 0.45885830\n",
            "Iteration 48, loss = 0.45832684\n",
            "Iteration 49, loss = 0.45790904\n",
            "Iteration 50, loss = 0.45744495\n",
            "Iteration 51, loss = 0.45699367\n",
            "Iteration 52, loss = 0.45668381\n",
            "Iteration 53, loss = 0.45613903\n",
            "Iteration 54, loss = 0.45580279\n",
            "Iteration 55, loss = 0.45550502\n",
            "Iteration 56, loss = 0.45514080\n",
            "Iteration 57, loss = 0.45495549\n",
            "Iteration 58, loss = 0.45456382\n",
            "Iteration 59, loss = 0.45422330\n",
            "Iteration 60, loss = 0.45401232\n",
            "Iteration 61, loss = 0.45375870\n",
            "Iteration 62, loss = 0.45355074\n",
            "Iteration 63, loss = 0.45321997\n",
            "Iteration 64, loss = 0.45293964\n",
            "Iteration 65, loss = 0.45265295\n",
            "Iteration 66, loss = 0.45252543\n",
            "Iteration 67, loss = 0.45228047\n",
            "Iteration 68, loss = 0.45203360\n",
            "Iteration 69, loss = 0.45180488\n",
            "Iteration 70, loss = 0.45174667\n",
            "Iteration 71, loss = 0.45149803\n",
            "Iteration 72, loss = 0.45129054\n",
            "Iteration 73, loss = 0.45120208\n",
            "Iteration 74, loss = 0.45094493\n",
            "Iteration 75, loss = 0.45098674\n",
            "Iteration 76, loss = 0.45059934\n",
            "Iteration 77, loss = 0.45026127\n",
            "Iteration 78, loss = 0.45021134\n",
            "Iteration 79, loss = 0.45026303\n",
            "Iteration 80, loss = 0.44998893\n",
            "Iteration 81, loss = 0.44981259\n",
            "Iteration 82, loss = 0.44974924\n",
            "Iteration 83, loss = 0.44959567\n",
            "Iteration 84, loss = 0.44933621\n",
            "Iteration 85, loss = 0.44953298\n",
            "Iteration 86, loss = 0.44902860\n",
            "Iteration 87, loss = 0.44925926\n",
            "Iteration 88, loss = 0.44899350\n",
            "Iteration 89, loss = 0.44874509\n",
            "Iteration 90, loss = 0.44868012\n",
            "Iteration 91, loss = 0.44858752\n",
            "Iteration 92, loss = 0.44844844\n",
            "Iteration 93, loss = 0.44835337\n",
            "Iteration 94, loss = 0.44824827\n",
            "Iteration 95, loss = 0.44821665\n",
            "Iteration 96, loss = 0.44811901\n",
            "Iteration 97, loss = 0.44810677\n",
            "Iteration 98, loss = 0.44777892\n",
            "Iteration 99, loss = 0.44783438\n",
            "Iteration 100, loss = 0.44769755\n",
            "Iteration 101, loss = 0.44759619\n",
            "Iteration 102, loss = 0.44738079\n",
            "Iteration 103, loss = 0.44771489\n",
            "Iteration 104, loss = 0.44716440\n",
            "Iteration 105, loss = 0.44715784\n",
            "Iteration 106, loss = 0.44711063\n",
            "Iteration 107, loss = 0.44709518\n",
            "Iteration 108, loss = 0.44692513\n",
            "Iteration 109, loss = 0.44689608\n",
            "Iteration 110, loss = 0.44686680\n",
            "Iteration 111, loss = 0.44660968\n",
            "Iteration 112, loss = 0.44672719\n",
            "Iteration 113, loss = 0.44668899\n",
            "Iteration 114, loss = 0.44658083\n",
            "Iteration 115, loss = 0.44655254\n",
            "Iteration 116, loss = 0.44648295\n",
            "Iteration 117, loss = 0.44638637\n",
            "Iteration 118, loss = 0.44629419\n",
            "Iteration 119, loss = 0.44608342\n",
            "Iteration 120, loss = 0.44596878\n",
            "Iteration 121, loss = 0.44611492\n",
            "Iteration 122, loss = 0.44602199\n",
            "Iteration 123, loss = 0.44592127\n",
            "Iteration 124, loss = 0.44587783\n",
            "Iteration 125, loss = 0.44588868\n",
            "Iteration 126, loss = 0.44577051\n",
            "Iteration 127, loss = 0.44561580\n",
            "Iteration 128, loss = 0.44562629\n",
            "Iteration 129, loss = 0.44571898\n",
            "Iteration 130, loss = 0.44535180\n",
            "Iteration 131, loss = 0.44538773\n",
            "Iteration 132, loss = 0.44531835\n",
            "Iteration 133, loss = 0.44533837\n",
            "Iteration 134, loss = 0.44512284\n",
            "Iteration 135, loss = 0.44520102\n",
            "Iteration 136, loss = 0.44508307\n",
            "Iteration 137, loss = 0.44502065\n",
            "Iteration 138, loss = 0.44486949\n",
            "Iteration 139, loss = 0.44491426\n",
            "Iteration 140, loss = 0.44486727\n",
            "Iteration 141, loss = 0.44478194\n",
            "Iteration 142, loss = 0.44476138\n",
            "Iteration 143, loss = 0.44464802\n",
            "Iteration 144, loss = 0.44464763\n",
            "Iteration 145, loss = 0.44454391\n",
            "Iteration 146, loss = 0.44456448\n",
            "Iteration 147, loss = 0.44456244\n",
            "Iteration 148, loss = 0.44440297\n",
            "Iteration 149, loss = 0.44427498\n",
            "Iteration 150, loss = 0.44437379\n",
            "Iteration 151, loss = 0.44418626\n",
            "Iteration 152, loss = 0.44426269\n",
            "Iteration 153, loss = 0.44404828\n",
            "Iteration 154, loss = 0.44418274\n",
            "Iteration 155, loss = 0.44411320\n",
            "Iteration 156, loss = 0.44415394\n",
            "Iteration 157, loss = 0.44383293\n",
            "Iteration 158, loss = 0.44383999\n",
            "Iteration 159, loss = 0.44414815\n",
            "Iteration 160, loss = 0.44370525\n",
            "Iteration 161, loss = 0.44372305\n",
            "Iteration 162, loss = 0.44382734\n",
            "Iteration 163, loss = 0.44378624\n",
            "Iteration 164, loss = 0.44375454\n",
            "Iteration 165, loss = 0.44364455\n",
            "Iteration 166, loss = 0.44356606\n",
            "Iteration 167, loss = 0.44349640\n",
            "Iteration 168, loss = 0.44343128\n",
            "Iteration 169, loss = 0.44335893\n",
            "Iteration 170, loss = 0.44339182\n",
            "Iteration 171, loss = 0.44329855\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 172, loss = 0.44291448\n",
            "Iteration 173, loss = 0.44287087\n",
            "Iteration 174, loss = 0.44285902\n",
            "Iteration 175, loss = 0.44287307\n",
            "Iteration 176, loss = 0.44283579\n",
            "Iteration 177, loss = 0.44284507\n",
            "Iteration 178, loss = 0.44286247\n",
            "Iteration 179, loss = 0.44276589\n",
            "Iteration 180, loss = 0.44281772\n",
            "Iteration 181, loss = 0.44277884\n",
            "Iteration 182, loss = 0.44281306\n",
            "Iteration 183, loss = 0.44276933\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 184, loss = 0.44266581\n",
            "Iteration 185, loss = 0.44268276\n",
            "Iteration 186, loss = 0.44266374\n",
            "Iteration 187, loss = 0.44266188\n",
            "Iteration 188, loss = 0.44265681\n",
            "Iteration 189, loss = 0.44266402\n",
            "Iteration 190, loss = 0.44265017\n",
            "Iteration 191, loss = 0.44265829\n",
            "Iteration 192, loss = 0.44264793\n",
            "Iteration 193, loss = 0.44265593\n",
            "Iteration 194, loss = 0.44264044\n",
            "Iteration 195, loss = 0.44264035\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 196, loss = 0.44263114\n",
            "Iteration 197, loss = 0.44262023\n",
            "Iteration 198, loss = 0.44262036\n",
            "Iteration 199, loss = 0.44261879\n",
            "Iteration 200, loss = 0.44262024\n",
            "Iteration 1, loss = 0.70259669\n",
            "Iteration 2, loss = 0.69659120\n",
            "Iteration 3, loss = 0.69444395\n",
            "Iteration 4, loss = 0.69351447\n",
            "Iteration 5, loss = 0.69286029\n",
            "Iteration 6, loss = 0.69202571\n",
            "Iteration 7, loss = 0.69049938\n",
            "Iteration 8, loss = 0.68737357\n",
            "Iteration 9, loss = 0.68061338\n",
            "Iteration 10, loss = 0.66603073\n",
            "Iteration 11, loss = 0.63870361\n",
            "Iteration 12, loss = 0.60587975\n",
            "Iteration 13, loss = 0.58277501\n",
            "Iteration 14, loss = 0.56801785\n",
            "Iteration 15, loss = 0.55728345\n",
            "Iteration 16, loss = 0.54870320\n",
            "Iteration 17, loss = 0.54133593\n",
            "Iteration 18, loss = 0.53464281\n",
            "Iteration 19, loss = 0.52845095\n",
            "Iteration 20, loss = 0.52239361\n",
            "Iteration 21, loss = 0.51655297\n",
            "Iteration 22, loss = 0.51080140\n",
            "Iteration 23, loss = 0.50536284\n",
            "Iteration 24, loss = 0.49999962\n",
            "Iteration 25, loss = 0.49502133\n",
            "Iteration 26, loss = 0.49042580\n",
            "Iteration 27, loss = 0.48619473\n",
            "Iteration 28, loss = 0.48259508\n",
            "Iteration 29, loss = 0.47936956\n",
            "Iteration 30, loss = 0.47646787\n",
            "Iteration 31, loss = 0.47423024\n",
            "Iteration 32, loss = 0.47214747\n",
            "Iteration 33, loss = 0.47051306\n",
            "Iteration 34, loss = 0.46887742\n",
            "Iteration 35, loss = 0.46744885\n",
            "Iteration 36, loss = 0.46625576\n",
            "Iteration 37, loss = 0.46522042\n",
            "Iteration 38, loss = 0.46422707\n",
            "Iteration 39, loss = 0.46334896\n",
            "Iteration 40, loss = 0.46248104\n",
            "Iteration 41, loss = 0.46169139\n",
            "Iteration 42, loss = 0.46105332\n",
            "Iteration 43, loss = 0.46032069\n",
            "Iteration 44, loss = 0.45966204\n",
            "Iteration 45, loss = 0.45905593\n",
            "Iteration 46, loss = 0.45857784\n",
            "Iteration 47, loss = 0.45799623\n",
            "Iteration 48, loss = 0.45750695\n",
            "Iteration 49, loss = 0.45714905\n",
            "Iteration 50, loss = 0.45658470\n",
            "Iteration 51, loss = 0.45610649\n",
            "Iteration 52, loss = 0.45569264\n",
            "Iteration 53, loss = 0.45526714\n",
            "Iteration 54, loss = 0.45494765\n",
            "Iteration 55, loss = 0.45458281\n",
            "Iteration 56, loss = 0.45425562\n",
            "Iteration 57, loss = 0.45395115\n",
            "Iteration 58, loss = 0.45357906\n",
            "Iteration 59, loss = 0.45333358\n",
            "Iteration 60, loss = 0.45311780\n",
            "Iteration 61, loss = 0.45287041\n",
            "Iteration 62, loss = 0.45261960\n",
            "Iteration 63, loss = 0.45233726\n",
            "Iteration 64, loss = 0.45199736\n",
            "Iteration 65, loss = 0.45183597\n",
            "Iteration 66, loss = 0.45167407\n",
            "Iteration 67, loss = 0.45143805\n",
            "Iteration 68, loss = 0.45112685\n",
            "Iteration 69, loss = 0.45097721\n",
            "Iteration 70, loss = 0.45081451\n",
            "Iteration 71, loss = 0.45070005\n",
            "Iteration 72, loss = 0.45047011\n",
            "Iteration 73, loss = 0.45030109\n",
            "Iteration 74, loss = 0.45015333\n",
            "Iteration 75, loss = 0.45006885\n",
            "Iteration 76, loss = 0.44983514\n",
            "Iteration 77, loss = 0.44950413\n",
            "Iteration 78, loss = 0.44945348\n",
            "Iteration 79, loss = 0.44936719\n",
            "Iteration 80, loss = 0.44923114\n",
            "Iteration 81, loss = 0.44899394\n",
            "Iteration 82, loss = 0.44890638\n",
            "Iteration 83, loss = 0.44880754\n",
            "Iteration 84, loss = 0.44858826\n",
            "Iteration 85, loss = 0.44864772\n",
            "Iteration 86, loss = 0.44819996\n",
            "Iteration 87, loss = 0.44828883\n",
            "Iteration 88, loss = 0.44820267\n",
            "Iteration 89, loss = 0.44794465\n",
            "Iteration 90, loss = 0.44788991\n",
            "Iteration 91, loss = 0.44780612\n",
            "Iteration 92, loss = 0.44753151\n",
            "Iteration 93, loss = 0.44755611\n",
            "Iteration 94, loss = 0.44741019\n",
            "Iteration 95, loss = 0.44738559\n",
            "Iteration 96, loss = 0.44728531\n",
            "Iteration 97, loss = 0.44729614\n",
            "Iteration 98, loss = 0.44705912\n",
            "Iteration 99, loss = 0.44708251\n",
            "Iteration 100, loss = 0.44702043\n",
            "Iteration 101, loss = 0.44683666\n",
            "Iteration 102, loss = 0.44659086\n",
            "Iteration 103, loss = 0.44679906\n",
            "Iteration 104, loss = 0.44642129\n",
            "Iteration 105, loss = 0.44639557\n",
            "Iteration 106, loss = 0.44630999\n",
            "Iteration 107, loss = 0.44630613\n",
            "Iteration 108, loss = 0.44621808\n",
            "Iteration 109, loss = 0.44610589\n",
            "Iteration 110, loss = 0.44603034\n",
            "Iteration 111, loss = 0.44578107\n",
            "Iteration 112, loss = 0.44586714\n",
            "Iteration 113, loss = 0.44591426\n",
            "Iteration 114, loss = 0.44580896\n",
            "Iteration 115, loss = 0.44569604\n",
            "Iteration 116, loss = 0.44572201\n",
            "Iteration 117, loss = 0.44555420\n",
            "Iteration 118, loss = 0.44537601\n",
            "Iteration 119, loss = 0.44539534\n",
            "Iteration 120, loss = 0.44522844\n",
            "Iteration 121, loss = 0.44535644\n",
            "Iteration 122, loss = 0.44524576\n",
            "Iteration 123, loss = 0.44522244\n",
            "Iteration 124, loss = 0.44511049\n",
            "Iteration 125, loss = 0.44518292\n",
            "Iteration 126, loss = 0.44503166\n",
            "Iteration 127, loss = 0.44492955\n",
            "Iteration 128, loss = 0.44490405\n",
            "Iteration 129, loss = 0.44493562\n",
            "Iteration 130, loss = 0.44471890\n",
            "Iteration 131, loss = 0.44464619\n",
            "Iteration 132, loss = 0.44465359\n",
            "Iteration 133, loss = 0.44459514\n",
            "Iteration 134, loss = 0.44444547\n",
            "Iteration 135, loss = 0.44449788\n",
            "Iteration 136, loss = 0.44437195\n",
            "Iteration 137, loss = 0.44435412\n",
            "Iteration 138, loss = 0.44417755\n",
            "Iteration 139, loss = 0.44428383\n",
            "Iteration 140, loss = 0.44426748\n",
            "Iteration 141, loss = 0.44420240\n",
            "Iteration 142, loss = 0.44415459\n",
            "Iteration 143, loss = 0.44401690\n",
            "Iteration 144, loss = 0.44403355\n",
            "Iteration 145, loss = 0.44388958\n",
            "Iteration 146, loss = 0.44398615\n",
            "Iteration 147, loss = 0.44389902\n",
            "Iteration 148, loss = 0.44384231\n",
            "Iteration 149, loss = 0.44374323\n",
            "Iteration 150, loss = 0.44374475\n",
            "Iteration 151, loss = 0.44361650\n",
            "Iteration 152, loss = 0.44363829\n",
            "Iteration 153, loss = 0.44356413\n",
            "Iteration 154, loss = 0.44375664\n",
            "Iteration 155, loss = 0.44346987\n",
            "Iteration 156, loss = 0.44368129\n",
            "Iteration 157, loss = 0.44327649\n",
            "Iteration 158, loss = 0.44327973\n",
            "Iteration 159, loss = 0.44350492\n",
            "Iteration 160, loss = 0.44316031\n",
            "Iteration 161, loss = 0.44319863\n",
            "Iteration 162, loss = 0.44321547\n",
            "Iteration 163, loss = 0.44322387\n",
            "Iteration 164, loss = 0.44315652\n",
            "Iteration 165, loss = 0.44302891\n",
            "Iteration 166, loss = 0.44304513\n",
            "Iteration 167, loss = 0.44298096\n",
            "Iteration 168, loss = 0.44281013\n",
            "Iteration 169, loss = 0.44283928\n",
            "Iteration 170, loss = 0.44275429\n",
            "Iteration 171, loss = 0.44269475\n",
            "Iteration 172, loss = 0.44265890\n",
            "Iteration 173, loss = 0.44265660\n",
            "Iteration 174, loss = 0.44251980\n",
            "Iteration 175, loss = 0.44252641\n",
            "Iteration 176, loss = 0.44242405\n",
            "Iteration 177, loss = 0.44257334\n",
            "Iteration 178, loss = 0.44268902\n",
            "Iteration 179, loss = 0.44238741\n",
            "Iteration 180, loss = 0.44238241\n",
            "Iteration 181, loss = 0.44226164\n",
            "Iteration 182, loss = 0.44220938\n",
            "Iteration 183, loss = 0.44235925\n",
            "Iteration 184, loss = 0.44233882\n",
            "Iteration 185, loss = 0.44239183\n",
            "Iteration 186, loss = 0.44195285\n",
            "Iteration 187, loss = 0.44196318\n",
            "Iteration 188, loss = 0.44205845\n",
            "Iteration 189, loss = 0.44205231\n",
            "Iteration 190, loss = 0.44204674\n",
            "Iteration 191, loss = 0.44175867\n",
            "Iteration 192, loss = 0.44197535\n",
            "Iteration 193, loss = 0.44181566\n",
            "Iteration 194, loss = 0.44177254\n",
            "Iteration 195, loss = 0.44177240\n",
            "Iteration 196, loss = 0.44159268\n",
            "Iteration 197, loss = 0.44174596\n",
            "Iteration 198, loss = 0.44179764\n",
            "Iteration 199, loss = 0.44144020\n",
            "Iteration 200, loss = 0.44169091\n",
            "Iteration 1, loss = 0.70255541\n",
            "Iteration 2, loss = 0.69655480\n",
            "Iteration 3, loss = 0.69438540\n",
            "Iteration 4, loss = 0.69340537\n",
            "Iteration 5, loss = 0.69265148\n",
            "Iteration 6, loss = 0.69160447\n",
            "Iteration 7, loss = 0.68960911\n",
            "Iteration 8, loss = 0.68542699\n",
            "Iteration 9, loss = 0.67628031\n",
            "Iteration 10, loss = 0.65711106\n",
            "Iteration 11, loss = 0.62519452\n",
            "Iteration 12, loss = 0.59459450\n",
            "Iteration 13, loss = 0.57503521\n",
            "Iteration 14, loss = 0.56201040\n",
            "Iteration 15, loss = 0.55222497\n",
            "Iteration 16, loss = 0.54420553\n",
            "Iteration 17, loss = 0.53729929\n",
            "Iteration 18, loss = 0.53106783\n",
            "Iteration 19, loss = 0.52529845\n",
            "Iteration 20, loss = 0.51962914\n",
            "Iteration 21, loss = 0.51412311\n",
            "Iteration 22, loss = 0.50862917\n",
            "Iteration 23, loss = 0.50333781\n",
            "Iteration 24, loss = 0.49824428\n",
            "Iteration 25, loss = 0.49348027\n",
            "Iteration 26, loss = 0.48912010\n",
            "Iteration 27, loss = 0.48509698\n",
            "Iteration 28, loss = 0.48160870\n",
            "Iteration 29, loss = 0.47853649\n",
            "Iteration 30, loss = 0.47577798\n",
            "Iteration 31, loss = 0.47365503\n",
            "Iteration 32, loss = 0.47163525\n",
            "Iteration 33, loss = 0.47001780\n",
            "Iteration 34, loss = 0.46847282\n",
            "Iteration 35, loss = 0.46706904\n",
            "Iteration 36, loss = 0.46601017\n",
            "Iteration 37, loss = 0.46502561\n",
            "Iteration 38, loss = 0.46410820\n",
            "Iteration 39, loss = 0.46329282\n",
            "Iteration 40, loss = 0.46245977\n",
            "Iteration 41, loss = 0.46175321\n",
            "Iteration 42, loss = 0.46113943\n",
            "Iteration 43, loss = 0.46042220\n",
            "Iteration 44, loss = 0.45976669\n",
            "Iteration 45, loss = 0.45922681\n",
            "Iteration 46, loss = 0.45872693\n",
            "Iteration 47, loss = 0.45813235\n",
            "Iteration 48, loss = 0.45772424\n",
            "Iteration 49, loss = 0.45736252\n",
            "Iteration 50, loss = 0.45680753\n",
            "Iteration 51, loss = 0.45633241\n",
            "Iteration 52, loss = 0.45603311\n",
            "Iteration 53, loss = 0.45563403\n",
            "Iteration 54, loss = 0.45526789\n",
            "Iteration 55, loss = 0.45497410\n",
            "Iteration 56, loss = 0.45468978\n",
            "Iteration 57, loss = 0.45435446\n",
            "Iteration 58, loss = 0.45391398\n",
            "Iteration 59, loss = 0.45372859\n",
            "Iteration 60, loss = 0.45348177\n",
            "Iteration 61, loss = 0.45323291\n",
            "Iteration 62, loss = 0.45302308\n",
            "Iteration 63, loss = 0.45275447\n",
            "Iteration 64, loss = 0.45240273\n",
            "Iteration 65, loss = 0.45227818\n",
            "Iteration 66, loss = 0.45206634\n",
            "Iteration 67, loss = 0.45183732\n",
            "Iteration 68, loss = 0.45150641\n",
            "Iteration 69, loss = 0.45136856\n",
            "Iteration 70, loss = 0.45122705\n",
            "Iteration 71, loss = 0.45103687\n",
            "Iteration 72, loss = 0.45086054\n",
            "Iteration 73, loss = 0.45068681\n",
            "Iteration 74, loss = 0.45048898\n",
            "Iteration 75, loss = 0.45034783\n",
            "Iteration 76, loss = 0.45032600\n",
            "Iteration 77, loss = 0.44993951\n",
            "Iteration 78, loss = 0.44991340\n",
            "Iteration 79, loss = 0.44972775\n",
            "Iteration 80, loss = 0.44971858\n",
            "Iteration 81, loss = 0.44937432\n",
            "Iteration 82, loss = 0.44933238\n",
            "Iteration 83, loss = 0.44918328\n",
            "Iteration 84, loss = 0.44898878\n",
            "Iteration 85, loss = 0.44891622\n",
            "Iteration 86, loss = 0.44866128\n",
            "Iteration 87, loss = 0.44860726\n",
            "Iteration 88, loss = 0.44864374\n",
            "Iteration 89, loss = 0.44845620\n",
            "Iteration 90, loss = 0.44828066\n",
            "Iteration 91, loss = 0.44823850\n",
            "Iteration 92, loss = 0.44797671\n",
            "Iteration 93, loss = 0.44796724\n",
            "Iteration 94, loss = 0.44781667\n",
            "Iteration 95, loss = 0.44779263\n",
            "Iteration 96, loss = 0.44774933\n",
            "Iteration 97, loss = 0.44773524\n",
            "Iteration 98, loss = 0.44755459\n",
            "Iteration 99, loss = 0.44747465\n",
            "Iteration 100, loss = 0.44745238\n",
            "Iteration 101, loss = 0.44723972\n",
            "Iteration 102, loss = 0.44710610\n",
            "Iteration 103, loss = 0.44718569\n",
            "Iteration 104, loss = 0.44681739\n",
            "Iteration 105, loss = 0.44691403\n",
            "Iteration 106, loss = 0.44679746\n",
            "Iteration 107, loss = 0.44677553\n",
            "Iteration 108, loss = 0.44674392\n",
            "Iteration 109, loss = 0.44660011\n",
            "Iteration 110, loss = 0.44651002\n",
            "Iteration 111, loss = 0.44639263\n",
            "Iteration 112, loss = 0.44642138\n",
            "Iteration 113, loss = 0.44630062\n",
            "Iteration 114, loss = 0.44627041\n",
            "Iteration 115, loss = 0.44616547\n",
            "Iteration 116, loss = 0.44620206\n",
            "Iteration 117, loss = 0.44600495\n",
            "Iteration 118, loss = 0.44594489\n",
            "Iteration 119, loss = 0.44577429\n",
            "Iteration 120, loss = 0.44572837\n",
            "Iteration 121, loss = 0.44583909\n",
            "Iteration 122, loss = 0.44565455\n",
            "Iteration 123, loss = 0.44585249\n",
            "Iteration 124, loss = 0.44553605\n",
            "Iteration 125, loss = 0.44556071\n",
            "Iteration 126, loss = 0.44547580\n",
            "Iteration 127, loss = 0.44541877\n",
            "Iteration 128, loss = 0.44540121\n",
            "Iteration 129, loss = 0.44540189\n",
            "Iteration 130, loss = 0.44519113\n",
            "Iteration 131, loss = 0.44512580\n",
            "Iteration 132, loss = 0.44517892\n",
            "Iteration 133, loss = 0.44507855\n",
            "Iteration 134, loss = 0.44506214\n",
            "Iteration 135, loss = 0.44503237\n",
            "Iteration 136, loss = 0.44502140\n",
            "Iteration 137, loss = 0.44487250\n",
            "Iteration 138, loss = 0.44475807\n",
            "Iteration 139, loss = 0.44482823\n",
            "Iteration 140, loss = 0.44479572\n",
            "Iteration 141, loss = 0.44468706\n",
            "Iteration 142, loss = 0.44463740\n",
            "Iteration 143, loss = 0.44465351\n",
            "Iteration 144, loss = 0.44461548\n",
            "Iteration 145, loss = 0.44437650\n",
            "Iteration 146, loss = 0.44451913\n",
            "Iteration 147, loss = 0.44435375\n",
            "Iteration 148, loss = 0.44431809\n",
            "Iteration 149, loss = 0.44433361\n",
            "Iteration 150, loss = 0.44421699\n",
            "Iteration 151, loss = 0.44411160\n",
            "Iteration 152, loss = 0.44406011\n",
            "Iteration 153, loss = 0.44404302\n",
            "Iteration 154, loss = 0.44423983\n",
            "Iteration 155, loss = 0.44394875\n",
            "Iteration 156, loss = 0.44410792\n",
            "Iteration 157, loss = 0.44384436\n",
            "Iteration 158, loss = 0.44376039\n",
            "Iteration 159, loss = 0.44390681\n",
            "Iteration 160, loss = 0.44373451\n",
            "Iteration 161, loss = 0.44360295\n",
            "Iteration 162, loss = 0.44360707\n",
            "Iteration 163, loss = 0.44362274\n",
            "Iteration 164, loss = 0.44356474\n",
            "Iteration 165, loss = 0.44347006\n",
            "Iteration 166, loss = 0.44351467\n",
            "Iteration 167, loss = 0.44341221\n",
            "Iteration 168, loss = 0.44325776\n",
            "Iteration 169, loss = 0.44324783\n",
            "Iteration 170, loss = 0.44314627\n",
            "Iteration 171, loss = 0.44322088\n",
            "Iteration 172, loss = 0.44310252\n",
            "Iteration 173, loss = 0.44314934\n",
            "Iteration 174, loss = 0.44297217\n",
            "Iteration 175, loss = 0.44303271\n",
            "Iteration 176, loss = 0.44297100\n",
            "Iteration 177, loss = 0.44302092\n",
            "Iteration 178, loss = 0.44314303\n",
            "Iteration 179, loss = 0.44279458\n",
            "Iteration 180, loss = 0.44283522\n",
            "Iteration 181, loss = 0.44262588\n",
            "Iteration 182, loss = 0.44269090\n",
            "Iteration 183, loss = 0.44271581\n",
            "Iteration 184, loss = 0.44265807\n",
            "Iteration 185, loss = 0.44279813\n",
            "Iteration 186, loss = 0.44247997\n",
            "Iteration 187, loss = 0.44243361\n",
            "Iteration 188, loss = 0.44249667\n",
            "Iteration 189, loss = 0.44244956\n",
            "Iteration 190, loss = 0.44242144\n",
            "Iteration 191, loss = 0.44215307\n",
            "Iteration 192, loss = 0.44236064\n",
            "Iteration 193, loss = 0.44225436\n",
            "Iteration 194, loss = 0.44229465\n",
            "Iteration 195, loss = 0.44225359\n",
            "Iteration 196, loss = 0.44204104\n",
            "Iteration 197, loss = 0.44219807\n",
            "Iteration 198, loss = 0.44223768\n",
            "Iteration 199, loss = 0.44200991\n",
            "Iteration 200, loss = 0.44214198\n",
            "Iteration 1, loss = 0.70261631\n",
            "Iteration 2, loss = 0.69659636\n",
            "Iteration 3, loss = 0.69443750\n",
            "Iteration 4, loss = 0.69349413\n",
            "Iteration 5, loss = 0.69282655\n",
            "Iteration 6, loss = 0.69196597\n",
            "Iteration 7, loss = 0.69037981\n",
            "Iteration 8, loss = 0.68711518\n",
            "Iteration 9, loss = 0.68011348\n",
            "Iteration 10, loss = 0.66526151\n",
            "Iteration 11, loss = 0.63800049\n",
            "Iteration 12, loss = 0.60592743\n",
            "Iteration 13, loss = 0.58335029\n",
            "Iteration 14, loss = 0.56890891\n",
            "Iteration 15, loss = 0.55837351\n",
            "Iteration 16, loss = 0.54989153\n",
            "Iteration 17, loss = 0.54274406\n",
            "Iteration 18, loss = 0.53633861\n",
            "Iteration 19, loss = 0.53037696\n",
            "Iteration 20, loss = 0.52462634\n",
            "Iteration 21, loss = 0.51901851\n",
            "Iteration 22, loss = 0.51350803\n",
            "Iteration 23, loss = 0.50814353\n",
            "Iteration 24, loss = 0.50290280\n",
            "Iteration 25, loss = 0.49802061\n",
            "Iteration 26, loss = 0.49342950\n",
            "Iteration 27, loss = 0.48914900\n",
            "Iteration 28, loss = 0.48546509\n",
            "Iteration 29, loss = 0.48214806\n",
            "Iteration 30, loss = 0.47921833\n",
            "Iteration 31, loss = 0.47686551\n",
            "Iteration 32, loss = 0.47475648\n",
            "Iteration 33, loss = 0.47310150\n",
            "Iteration 34, loss = 0.47146390\n",
            "Iteration 35, loss = 0.47003321\n",
            "Iteration 36, loss = 0.46888888\n",
            "Iteration 37, loss = 0.46788839\n",
            "Iteration 38, loss = 0.46692122\n",
            "Iteration 39, loss = 0.46611637\n",
            "Iteration 40, loss = 0.46527197\n",
            "Iteration 41, loss = 0.46441376\n",
            "Iteration 42, loss = 0.46381111\n",
            "Iteration 43, loss = 0.46303122\n",
            "Iteration 44, loss = 0.46240300\n",
            "Iteration 45, loss = 0.46186156\n",
            "Iteration 46, loss = 0.46134661\n",
            "Iteration 47, loss = 0.46078406\n",
            "Iteration 48, loss = 0.46025213\n",
            "Iteration 49, loss = 0.45986322\n",
            "Iteration 50, loss = 0.45935364\n",
            "Iteration 51, loss = 0.45887125\n",
            "Iteration 52, loss = 0.45853188\n",
            "Iteration 53, loss = 0.45816711\n",
            "Iteration 54, loss = 0.45780238\n",
            "Iteration 55, loss = 0.45745038\n",
            "Iteration 56, loss = 0.45715544\n",
            "Iteration 57, loss = 0.45676571\n",
            "Iteration 58, loss = 0.45647055\n",
            "Iteration 59, loss = 0.45611095\n",
            "Iteration 60, loss = 0.45596264\n",
            "Iteration 61, loss = 0.45561107\n",
            "Iteration 62, loss = 0.45548556\n",
            "Iteration 63, loss = 0.45517194\n",
            "Iteration 64, loss = 0.45483333\n",
            "Iteration 65, loss = 0.45484793\n",
            "Iteration 66, loss = 0.45453475\n",
            "Iteration 67, loss = 0.45425934\n",
            "Iteration 68, loss = 0.45395556\n",
            "Iteration 69, loss = 0.45374275\n",
            "Iteration 70, loss = 0.45360615\n",
            "Iteration 71, loss = 0.45347845\n",
            "Iteration 72, loss = 0.45323717\n",
            "Iteration 73, loss = 0.45308599\n",
            "Iteration 74, loss = 0.45290358\n",
            "Iteration 75, loss = 0.45272084\n",
            "Iteration 76, loss = 0.45265843\n",
            "Iteration 77, loss = 0.45231046\n",
            "Iteration 78, loss = 0.45215959\n",
            "Iteration 79, loss = 0.45212384\n",
            "Iteration 80, loss = 0.45207568\n",
            "Iteration 81, loss = 0.45177219\n",
            "Iteration 82, loss = 0.45161631\n",
            "Iteration 83, loss = 0.45144816\n",
            "Iteration 84, loss = 0.45138398\n",
            "Iteration 85, loss = 0.45119773\n",
            "Iteration 86, loss = 0.45093706\n",
            "Iteration 87, loss = 0.45089033\n",
            "Iteration 88, loss = 0.45095574\n",
            "Iteration 89, loss = 0.45068650\n",
            "Iteration 90, loss = 0.45067189\n",
            "Iteration 91, loss = 0.45059086\n",
            "Iteration 92, loss = 0.45025332\n",
            "Iteration 93, loss = 0.45026173\n",
            "Iteration 94, loss = 0.45009927\n",
            "Iteration 95, loss = 0.45010736\n",
            "Iteration 96, loss = 0.44996010\n",
            "Iteration 97, loss = 0.44991032\n",
            "Iteration 98, loss = 0.44973327\n",
            "Iteration 99, loss = 0.44967754\n",
            "Iteration 100, loss = 0.44971568\n",
            "Iteration 101, loss = 0.44946514\n",
            "Iteration 102, loss = 0.44947193\n",
            "Iteration 103, loss = 0.44945871\n",
            "Iteration 104, loss = 0.44906824\n",
            "Iteration 105, loss = 0.44921268\n",
            "Iteration 106, loss = 0.44905296\n",
            "Iteration 107, loss = 0.44903508\n",
            "Iteration 108, loss = 0.44903679\n",
            "Iteration 109, loss = 0.44888174\n",
            "Iteration 110, loss = 0.44874619\n",
            "Iteration 111, loss = 0.44860057\n",
            "Iteration 112, loss = 0.44857326\n",
            "Iteration 113, loss = 0.44858415\n",
            "Iteration 114, loss = 0.44854276\n",
            "Iteration 115, loss = 0.44846761\n",
            "Iteration 116, loss = 0.44846065\n",
            "Iteration 117, loss = 0.44831948\n",
            "Iteration 118, loss = 0.44815682\n",
            "Iteration 119, loss = 0.44793895\n",
            "Iteration 120, loss = 0.44798233\n",
            "Iteration 121, loss = 0.44805212\n",
            "Iteration 122, loss = 0.44789715\n",
            "Iteration 123, loss = 0.44801035\n",
            "Iteration 124, loss = 0.44781023\n",
            "Iteration 125, loss = 0.44768238\n",
            "Iteration 126, loss = 0.44768716\n",
            "Iteration 127, loss = 0.44758168\n",
            "Iteration 128, loss = 0.44756332\n",
            "Iteration 129, loss = 0.44756285\n",
            "Iteration 130, loss = 0.44744188\n",
            "Iteration 131, loss = 0.44733672\n",
            "Iteration 132, loss = 0.44734161\n",
            "Iteration 133, loss = 0.44730072\n",
            "Iteration 134, loss = 0.44739190\n",
            "Iteration 135, loss = 0.44716541\n",
            "Iteration 136, loss = 0.44726907\n",
            "Iteration 137, loss = 0.44707546\n",
            "Iteration 138, loss = 0.44694922\n",
            "Iteration 139, loss = 0.44696888\n",
            "Iteration 140, loss = 0.44696925\n",
            "Iteration 141, loss = 0.44681784\n",
            "Iteration 142, loss = 0.44683156\n",
            "Iteration 143, loss = 0.44681608\n",
            "Iteration 144, loss = 0.44675636\n",
            "Iteration 145, loss = 0.44664155\n",
            "Iteration 146, loss = 0.44667784\n",
            "Iteration 147, loss = 0.44646235\n",
            "Iteration 148, loss = 0.44655124\n",
            "Iteration 149, loss = 0.44649509\n",
            "Iteration 150, loss = 0.44641771\n",
            "Iteration 151, loss = 0.44642654\n",
            "Iteration 152, loss = 0.44625404\n",
            "Iteration 153, loss = 0.44624262\n",
            "Iteration 154, loss = 0.44636393\n",
            "Iteration 155, loss = 0.44620493\n",
            "Iteration 156, loss = 0.44626595\n",
            "Iteration 157, loss = 0.44619416\n",
            "Iteration 158, loss = 0.44604059\n",
            "Iteration 159, loss = 0.44617709\n",
            "Iteration 160, loss = 0.44610749\n",
            "Iteration 161, loss = 0.44592966\n",
            "Iteration 162, loss = 0.44580088\n",
            "Iteration 163, loss = 0.44584006\n",
            "Iteration 164, loss = 0.44584336\n",
            "Iteration 165, loss = 0.44580943\n",
            "Iteration 166, loss = 0.44578210\n",
            "Iteration 167, loss = 0.44569398\n",
            "Iteration 168, loss = 0.44558855\n",
            "Iteration 169, loss = 0.44554172\n",
            "Iteration 170, loss = 0.44545321\n",
            "Iteration 171, loss = 0.44547802\n",
            "Iteration 172, loss = 0.44538576\n",
            "Iteration 173, loss = 0.44549834\n",
            "Iteration 174, loss = 0.44525914\n",
            "Iteration 175, loss = 0.44531209\n",
            "Iteration 176, loss = 0.44529760\n",
            "Iteration 177, loss = 0.44527971\n",
            "Iteration 178, loss = 0.44540940\n",
            "Iteration 179, loss = 0.44511977\n",
            "Iteration 180, loss = 0.44522855\n",
            "Iteration 181, loss = 0.44501600\n",
            "Iteration 182, loss = 0.44499265\n",
            "Iteration 183, loss = 0.44504799\n",
            "Iteration 184, loss = 0.44513027\n",
            "Iteration 185, loss = 0.44506834\n",
            "Iteration 186, loss = 0.44480216\n",
            "Iteration 187, loss = 0.44473905\n",
            "Iteration 188, loss = 0.44487474\n",
            "Iteration 189, loss = 0.44472332\n",
            "Iteration 190, loss = 0.44469492\n",
            "Iteration 191, loss = 0.44455997\n",
            "Iteration 192, loss = 0.44470146\n",
            "Iteration 193, loss = 0.44470494\n",
            "Iteration 194, loss = 0.44466732\n",
            "Iteration 195, loss = 0.44454200\n",
            "Iteration 196, loss = 0.44429784\n",
            "Iteration 197, loss = 0.44447813\n",
            "Iteration 198, loss = 0.44457555\n",
            "Iteration 199, loss = 0.44445644\n",
            "Iteration 200, loss = 0.44443245\n",
            "Iteration 1, loss = 0.70259134\n",
            "Iteration 2, loss = 0.69659052\n",
            "Iteration 3, loss = 0.69440932\n",
            "Iteration 4, loss = 0.69343013\n",
            "Iteration 5, loss = 0.69271171\n",
            "Iteration 6, loss = 0.69173228\n",
            "Iteration 7, loss = 0.68988532\n",
            "Iteration 8, loss = 0.68600702\n",
            "Iteration 9, loss = 0.67763506\n",
            "Iteration 10, loss = 0.65995410\n",
            "Iteration 11, loss = 0.62941323\n",
            "Iteration 12, loss = 0.59815381\n",
            "Iteration 13, loss = 0.57766718\n",
            "Iteration 14, loss = 0.56423381\n",
            "Iteration 15, loss = 0.55423841\n",
            "Iteration 16, loss = 0.54600529\n",
            "Iteration 17, loss = 0.53892523\n",
            "Iteration 18, loss = 0.53247620\n",
            "Iteration 19, loss = 0.52652923\n",
            "Iteration 20, loss = 0.52071906\n",
            "Iteration 21, loss = 0.51504210\n",
            "Iteration 22, loss = 0.50958310\n",
            "Iteration 23, loss = 0.50423160\n",
            "Iteration 24, loss = 0.49895897\n",
            "Iteration 25, loss = 0.49413310\n",
            "Iteration 26, loss = 0.48960785\n",
            "Iteration 27, loss = 0.48546297\n",
            "Iteration 28, loss = 0.48188319\n",
            "Iteration 29, loss = 0.47871474\n",
            "Iteration 30, loss = 0.47589584\n",
            "Iteration 31, loss = 0.47362098\n",
            "Iteration 32, loss = 0.47159140\n",
            "Iteration 33, loss = 0.47015418\n",
            "Iteration 34, loss = 0.46844508\n",
            "Iteration 35, loss = 0.46709062\n",
            "Iteration 36, loss = 0.46596388\n",
            "Iteration 37, loss = 0.46498813\n",
            "Iteration 38, loss = 0.46399487\n",
            "Iteration 39, loss = 0.46319626\n",
            "Iteration 40, loss = 0.46236274\n",
            "Iteration 41, loss = 0.46156147\n",
            "Iteration 42, loss = 0.46092869\n",
            "Iteration 43, loss = 0.46018740\n",
            "Iteration 44, loss = 0.45956731\n",
            "Iteration 45, loss = 0.45902221\n",
            "Iteration 46, loss = 0.45852848\n",
            "Iteration 47, loss = 0.45803023\n",
            "Iteration 48, loss = 0.45744046\n",
            "Iteration 49, loss = 0.45702385\n",
            "Iteration 50, loss = 0.45653693\n",
            "Iteration 51, loss = 0.45607301\n",
            "Iteration 52, loss = 0.45573871\n",
            "Iteration 53, loss = 0.45537153\n",
            "Iteration 54, loss = 0.45498979\n",
            "Iteration 55, loss = 0.45459661\n",
            "Iteration 56, loss = 0.45435335\n",
            "Iteration 57, loss = 0.45404755\n",
            "Iteration 58, loss = 0.45378597\n",
            "Iteration 59, loss = 0.45335336\n",
            "Iteration 60, loss = 0.45320898\n",
            "Iteration 61, loss = 0.45288158\n",
            "Iteration 62, loss = 0.45267301\n",
            "Iteration 63, loss = 0.45241132\n",
            "Iteration 64, loss = 0.45213876\n",
            "Iteration 65, loss = 0.45205945\n",
            "Iteration 66, loss = 0.45188814\n",
            "Iteration 67, loss = 0.45159356\n",
            "Iteration 68, loss = 0.45132022\n",
            "Iteration 69, loss = 0.45108678\n",
            "Iteration 70, loss = 0.45083231\n",
            "Iteration 71, loss = 0.45076013\n",
            "Iteration 72, loss = 0.45050761\n",
            "Iteration 73, loss = 0.45043535\n",
            "Iteration 74, loss = 0.45023725\n",
            "Iteration 75, loss = 0.45003363\n",
            "Iteration 76, loss = 0.44998514\n",
            "Iteration 77, loss = 0.44958677\n",
            "Iteration 78, loss = 0.44951815\n",
            "Iteration 79, loss = 0.44939071\n",
            "Iteration 80, loss = 0.44935606\n",
            "Iteration 81, loss = 0.44913896\n",
            "Iteration 82, loss = 0.44893652\n",
            "Iteration 83, loss = 0.44874255\n",
            "Iteration 84, loss = 0.44874463\n",
            "Iteration 85, loss = 0.44855586\n",
            "Iteration 86, loss = 0.44831586\n",
            "Iteration 87, loss = 0.44813615\n",
            "Iteration 88, loss = 0.44820120\n",
            "Iteration 89, loss = 0.44807208\n",
            "Iteration 90, loss = 0.44794616\n",
            "Iteration 91, loss = 0.44790049\n",
            "Iteration 92, loss = 0.44760595\n",
            "Iteration 93, loss = 0.44758255\n",
            "Iteration 94, loss = 0.44751594\n",
            "Iteration 95, loss = 0.44744307\n",
            "Iteration 96, loss = 0.44737692\n",
            "Iteration 97, loss = 0.44716219\n",
            "Iteration 98, loss = 0.44706316\n",
            "Iteration 99, loss = 0.44702784\n",
            "Iteration 100, loss = 0.44701430\n",
            "Iteration 101, loss = 0.44680794\n",
            "Iteration 102, loss = 0.44671017\n",
            "Iteration 103, loss = 0.44666980\n",
            "Iteration 104, loss = 0.44647604\n",
            "Iteration 105, loss = 0.44650044\n",
            "Iteration 106, loss = 0.44632481\n",
            "Iteration 107, loss = 0.44627980\n",
            "Iteration 108, loss = 0.44630549\n",
            "Iteration 109, loss = 0.44618461\n",
            "Iteration 110, loss = 0.44604987\n",
            "Iteration 111, loss = 0.44604963\n",
            "Iteration 112, loss = 0.44582449\n",
            "Iteration 113, loss = 0.44584961\n",
            "Iteration 114, loss = 0.44580168\n",
            "Iteration 115, loss = 0.44578367\n",
            "Iteration 116, loss = 0.44565414\n",
            "Iteration 117, loss = 0.44560326\n",
            "Iteration 118, loss = 0.44544165\n",
            "Iteration 119, loss = 0.44525345\n",
            "Iteration 120, loss = 0.44522771\n",
            "Iteration 121, loss = 0.44538962\n",
            "Iteration 122, loss = 0.44512783\n",
            "Iteration 123, loss = 0.44533298\n",
            "Iteration 124, loss = 0.44512151\n",
            "Iteration 125, loss = 0.44493961\n",
            "Iteration 126, loss = 0.44497309\n",
            "Iteration 127, loss = 0.44485999\n",
            "Iteration 128, loss = 0.44475469\n",
            "Iteration 129, loss = 0.44471053\n",
            "Iteration 130, loss = 0.44462256\n",
            "Iteration 131, loss = 0.44453622\n",
            "Iteration 132, loss = 0.44454850\n",
            "Iteration 133, loss = 0.44455993\n",
            "Iteration 134, loss = 0.44450269\n",
            "Iteration 135, loss = 0.44434915\n",
            "Iteration 136, loss = 0.44442128\n",
            "Iteration 137, loss = 0.44427421\n",
            "Iteration 138, loss = 0.44406059\n",
            "Iteration 139, loss = 0.44417421\n",
            "Iteration 140, loss = 0.44406107\n",
            "Iteration 141, loss = 0.44403820\n",
            "Iteration 142, loss = 0.44386291\n",
            "Iteration 143, loss = 0.44386227\n",
            "Iteration 144, loss = 0.44390758\n",
            "Iteration 145, loss = 0.44384600\n",
            "Iteration 146, loss = 0.44373592\n",
            "Iteration 147, loss = 0.44365163\n",
            "Iteration 148, loss = 0.44367643\n",
            "Iteration 149, loss = 0.44359070\n",
            "Iteration 150, loss = 0.44350859\n",
            "Iteration 151, loss = 0.44345150\n",
            "Iteration 152, loss = 0.44331733\n",
            "Iteration 153, loss = 0.44332133\n",
            "Iteration 154, loss = 0.44342302\n",
            "Iteration 155, loss = 0.44324617\n",
            "Iteration 156, loss = 0.44329421\n",
            "Iteration 157, loss = 0.44323185\n",
            "Iteration 158, loss = 0.44304573\n",
            "Iteration 159, loss = 0.44308226\n",
            "Iteration 160, loss = 0.44318270\n",
            "Iteration 161, loss = 0.44286005\n",
            "Iteration 162, loss = 0.44283563\n",
            "Iteration 163, loss = 0.44284681\n",
            "Iteration 164, loss = 0.44269947\n",
            "Iteration 165, loss = 0.44278577\n",
            "Iteration 166, loss = 0.44278722\n",
            "Iteration 167, loss = 0.44272813\n",
            "Iteration 168, loss = 0.44268113\n",
            "Iteration 169, loss = 0.44253333\n",
            "Iteration 170, loss = 0.44233119\n",
            "Iteration 171, loss = 0.44240122\n",
            "Iteration 172, loss = 0.44228814\n",
            "Iteration 173, loss = 0.44242739\n",
            "Iteration 174, loss = 0.44221761\n",
            "Iteration 175, loss = 0.44231785\n",
            "Iteration 176, loss = 0.44207475\n",
            "Iteration 177, loss = 0.44214443\n",
            "Iteration 178, loss = 0.44227639\n",
            "Iteration 179, loss = 0.44197685\n",
            "Iteration 180, loss = 0.44208836\n",
            "Iteration 181, loss = 0.44196266\n",
            "Iteration 182, loss = 0.44183646\n",
            "Iteration 183, loss = 0.44201977\n",
            "Iteration 184, loss = 0.44197639\n",
            "Iteration 185, loss = 0.44191760\n",
            "Iteration 186, loss = 0.44170389\n",
            "Iteration 187, loss = 0.44162305\n",
            "Iteration 188, loss = 0.44184160\n",
            "Iteration 189, loss = 0.44154843\n",
            "Iteration 190, loss = 0.44151463\n",
            "Iteration 191, loss = 0.44146347\n",
            "Iteration 192, loss = 0.44158446\n",
            "Iteration 193, loss = 0.44153340\n",
            "Iteration 194, loss = 0.44138301\n",
            "Iteration 195, loss = 0.44135216\n",
            "Iteration 196, loss = 0.44113808\n",
            "Iteration 197, loss = 0.44132331\n",
            "Iteration 198, loss = 0.44126131\n",
            "Iteration 199, loss = 0.44130617\n",
            "Iteration 200, loss = 0.44110730\n",
            "Confusion Matrix\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-0a709617ace6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my3_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX3_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my3_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Confusion Matrix'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my3_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my3_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred3_y_sklearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my3_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 205\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [40520, 8104]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9DEd05BOiiO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "beb0b1ed-5699-4d1f-dae4-39781ebd5aa3"
      },
      "source": [
        "print(cross_val_score(mlp,X2_train, y2_train, cv=kfolds, scoring = 'neg_mean_squared_error', verbose = 1))\n",
        "pred2_y_sklearn =cross_val_predict(mlp, X2_test, y2_test, cv=2)\n",
        "y2_true = y_test\n",
        "y2_pred = cross_val_predict(mlp, X2_test, y2_test, cv=10)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(y2_pred, y2_true))\n",
        "print(pd.crosstab(pred2_y_sklearn, y2_test))\n",
        "print(\"\")\n",
        "print(classification_report(y2_pred, y2_true))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.68867077\n",
            "Iteration 2, loss = 0.65757270\n",
            "Iteration 3, loss = 0.60429991\n",
            "Iteration 4, loss = 0.54326951\n",
            "Iteration 5, loss = 0.51003264\n",
            "Iteration 6, loss = 0.49543485\n",
            "Iteration 7, loss = 0.48816258\n",
            "Iteration 8, loss = 0.48369529\n",
            "Iteration 9, loss = 0.48051539\n",
            "Iteration 10, loss = 0.47836426\n",
            "Iteration 11, loss = 0.47648399\n",
            "Iteration 12, loss = 0.47489572\n",
            "Iteration 13, loss = 0.47359533\n",
            "Iteration 14, loss = 0.47241427\n",
            "Iteration 15, loss = 0.47129791\n",
            "Iteration 16, loss = 0.47046764\n",
            "Iteration 17, loss = 0.46946036\n",
            "Iteration 18, loss = 0.46869860\n",
            "Iteration 19, loss = 0.46781178\n",
            "Iteration 20, loss = 0.46703350\n",
            "Iteration 21, loss = 0.46648634\n",
            "Iteration 22, loss = 0.46566916\n",
            "Iteration 23, loss = 0.46496762\n",
            "Iteration 24, loss = 0.46450467\n",
            "Iteration 25, loss = 0.46372691\n",
            "Iteration 26, loss = 0.46342562\n",
            "Iteration 27, loss = 0.46289842\n",
            "Iteration 28, loss = 0.46244153\n",
            "Iteration 29, loss = 0.46201185\n",
            "Iteration 30, loss = 0.46178313\n",
            "Iteration 31, loss = 0.46122100\n",
            "Iteration 32, loss = 0.46084737\n",
            "Iteration 33, loss = 0.46048245\n",
            "Iteration 34, loss = 0.46010606\n",
            "Iteration 35, loss = 0.45992247\n",
            "Iteration 36, loss = 0.45936627\n",
            "Iteration 37, loss = 0.45906304\n",
            "Iteration 38, loss = 0.45876927\n",
            "Iteration 39, loss = 0.45845305\n",
            "Iteration 40, loss = 0.45828512\n",
            "Iteration 41, loss = 0.45792695\n",
            "Iteration 42, loss = 0.45751947\n",
            "Iteration 43, loss = 0.45732896\n",
            "Iteration 44, loss = 0.45716319\n",
            "Iteration 45, loss = 0.45689717\n",
            "Iteration 46, loss = 0.45664350\n",
            "Iteration 47, loss = 0.45629374\n",
            "Iteration 48, loss = 0.45612449\n",
            "Iteration 49, loss = 0.45593870\n",
            "Iteration 50, loss = 0.45579317\n",
            "Iteration 51, loss = 0.45543319\n",
            "Iteration 52, loss = 0.45529496\n",
            "Iteration 53, loss = 0.45513237\n",
            "Iteration 54, loss = 0.45487166\n",
            "Iteration 55, loss = 0.45454185\n",
            "Iteration 56, loss = 0.45441244\n",
            "Iteration 57, loss = 0.45423781\n",
            "Iteration 58, loss = 0.45408315\n",
            "Iteration 59, loss = 0.45397076\n",
            "Iteration 60, loss = 0.45371865\n",
            "Iteration 61, loss = 0.45370759\n",
            "Iteration 62, loss = 0.45347807\n",
            "Iteration 63, loss = 0.45319079\n",
            "Iteration 64, loss = 0.45323055\n",
            "Iteration 65, loss = 0.45294398\n",
            "Iteration 66, loss = 0.45288254\n",
            "Iteration 67, loss = 0.45272731\n",
            "Iteration 68, loss = 0.45264364\n",
            "Iteration 69, loss = 0.45250785\n",
            "Iteration 70, loss = 0.45245903\n",
            "Iteration 71, loss = 0.45238254\n",
            "Iteration 72, loss = 0.45210598\n",
            "Iteration 73, loss = 0.45198033\n",
            "Iteration 74, loss = 0.45201292\n",
            "Iteration 75, loss = 0.45174494\n",
            "Iteration 76, loss = 0.45172334\n",
            "Iteration 77, loss = 0.45173574\n",
            "Iteration 78, loss = 0.45141438\n",
            "Iteration 79, loss = 0.45145604\n",
            "Iteration 80, loss = 0.45124270\n",
            "Iteration 81, loss = 0.45119615\n",
            "Iteration 82, loss = 0.45120800\n",
            "Iteration 83, loss = 0.45103220\n",
            "Iteration 84, loss = 0.45093551\n",
            "Iteration 85, loss = 0.45086252\n",
            "Iteration 86, loss = 0.45074306\n",
            "Iteration 87, loss = 0.45067930\n",
            "Iteration 88, loss = 0.45052641\n",
            "Iteration 89, loss = 0.45055136\n",
            "Iteration 90, loss = 0.45036960\n",
            "Iteration 91, loss = 0.45046138\n",
            "Iteration 92, loss = 0.45037364\n",
            "Iteration 93, loss = 0.45016170\n",
            "Iteration 94, loss = 0.45007664\n",
            "Iteration 95, loss = 0.44987830\n",
            "Iteration 96, loss = 0.45017491\n",
            "Iteration 97, loss = 0.44990602\n",
            "Iteration 98, loss = 0.44996454\n",
            "Iteration 99, loss = 0.44979725\n",
            "Iteration 100, loss = 0.44974882\n",
            "Iteration 101, loss = 0.44963721\n",
            "Iteration 102, loss = 0.44969248\n",
            "Iteration 103, loss = 0.44963458\n",
            "Iteration 104, loss = 0.44936588\n",
            "Iteration 105, loss = 0.44939925\n",
            "Iteration 106, loss = 0.44934357\n",
            "Iteration 107, loss = 0.44940564\n",
            "Iteration 108, loss = 0.44917588\n",
            "Iteration 109, loss = 0.44925537\n",
            "Iteration 110, loss = 0.44910234\n",
            "Iteration 111, loss = 0.44916167\n",
            "Iteration 112, loss = 0.44910987\n",
            "Iteration 113, loss = 0.44886345\n",
            "Iteration 114, loss = 0.44934166\n",
            "Iteration 115, loss = 0.44882264\n",
            "Iteration 116, loss = 0.44896895\n",
            "Iteration 117, loss = 0.44886329\n",
            "Iteration 118, loss = 0.44889453\n",
            "Iteration 119, loss = 0.44888125\n",
            "Iteration 120, loss = 0.44871511\n",
            "Iteration 121, loss = 0.44854472\n",
            "Iteration 122, loss = 0.44877739\n",
            "Iteration 123, loss = 0.44858026\n",
            "Iteration 124, loss = 0.44839687\n",
            "Iteration 125, loss = 0.44863607\n",
            "Iteration 126, loss = 0.44852187\n",
            "Iteration 127, loss = 0.44855430\n",
            "Iteration 128, loss = 0.44827951\n",
            "Iteration 129, loss = 0.44821501\n",
            "Iteration 130, loss = 0.44823110\n",
            "Iteration 131, loss = 0.44827894\n",
            "Iteration 132, loss = 0.44815479\n",
            "Iteration 133, loss = 0.44827775\n",
            "Iteration 134, loss = 0.44813709\n",
            "Iteration 135, loss = 0.44814899\n",
            "Iteration 136, loss = 0.44799179\n",
            "Iteration 137, loss = 0.44799372\n",
            "Iteration 138, loss = 0.44790155\n",
            "Iteration 139, loss = 0.44788589\n",
            "Iteration 140, loss = 0.44786354\n",
            "Iteration 141, loss = 0.44798722\n",
            "Iteration 142, loss = 0.44783028\n",
            "Iteration 143, loss = 0.44795731\n",
            "Iteration 144, loss = 0.44788926\n",
            "Iteration 145, loss = 0.44781782\n",
            "Iteration 146, loss = 0.44769963\n",
            "Iteration 147, loss = 0.44787813\n",
            "Iteration 148, loss = 0.44785784\n",
            "Iteration 149, loss = 0.44776576\n",
            "Iteration 150, loss = 0.44759540\n",
            "Iteration 151, loss = 0.44769439\n",
            "Iteration 152, loss = 0.44760531\n",
            "Iteration 153, loss = 0.44757434\n",
            "Iteration 154, loss = 0.44759002\n",
            "Iteration 155, loss = 0.44749437\n",
            "Iteration 156, loss = 0.44741952\n",
            "Iteration 157, loss = 0.44769663\n",
            "Iteration 158, loss = 0.44746979\n",
            "Iteration 159, loss = 0.44737829\n",
            "Iteration 160, loss = 0.44773435\n",
            "Iteration 161, loss = 0.44737116\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 162, loss = 0.44710854\n",
            "Iteration 163, loss = 0.44709447\n",
            "Iteration 164, loss = 0.44700909\n",
            "Iteration 165, loss = 0.44701522\n",
            "Iteration 166, loss = 0.44700528\n",
            "Iteration 167, loss = 0.44694306\n",
            "Iteration 168, loss = 0.44700490\n",
            "Iteration 169, loss = 0.44699683\n",
            "Iteration 170, loss = 0.44696828\n",
            "Iteration 171, loss = 0.44697410\n",
            "Iteration 172, loss = 0.44695399\n",
            "Iteration 173, loss = 0.44695701\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 174, loss = 0.44691358\n",
            "Iteration 175, loss = 0.44687146\n",
            "Iteration 176, loss = 0.44686661\n",
            "Iteration 177, loss = 0.44686926\n",
            "Iteration 178, loss = 0.44687282\n",
            "Iteration 179, loss = 0.44686731\n",
            "Iteration 180, loss = 0.44687110\n",
            "Iteration 181, loss = 0.44686891\n",
            "Iteration 182, loss = 0.44685875\n",
            "Iteration 183, loss = 0.44686630\n",
            "Iteration 184, loss = 0.44687460\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 185, loss = 0.44684076\n",
            "Iteration 186, loss = 0.44684404\n",
            "Iteration 187, loss = 0.44684442\n",
            "Iteration 188, loss = 0.44684092\n",
            "Iteration 189, loss = 0.44683938\n",
            "Iteration 190, loss = 0.44684045\n",
            "Iteration 191, loss = 0.44683938\n",
            "Iteration 192, loss = 0.44683903\n",
            "Iteration 193, loss = 0.44683819\n",
            "Iteration 194, loss = 0.44683734\n",
            "Iteration 195, loss = 0.44684161\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 196, loss = 0.44683382\n",
            "Iteration 197, loss = 0.44683343\n",
            "Iteration 198, loss = 0.44683376\n",
            "Iteration 199, loss = 0.44683348\n",
            "Iteration 200, loss = 0.44683339\n",
            "Iteration 1, loss = 0.68878148\n",
            "Iteration 2, loss = 0.65756165\n",
            "Iteration 3, loss = 0.60377715\n",
            "Iteration 4, loss = 0.54197953\n",
            "Iteration 5, loss = 0.50882861\n",
            "Iteration 6, loss = 0.49428570\n",
            "Iteration 7, loss = 0.48726442\n",
            "Iteration 8, loss = 0.48275341\n",
            "Iteration 9, loss = 0.47977577\n",
            "Iteration 10, loss = 0.47750898\n",
            "Iteration 11, loss = 0.47576249\n",
            "Iteration 12, loss = 0.47424221\n",
            "Iteration 13, loss = 0.47292481\n",
            "Iteration 14, loss = 0.47168197\n",
            "Iteration 15, loss = 0.47065671\n",
            "Iteration 16, loss = 0.46971280\n",
            "Iteration 17, loss = 0.46874399\n",
            "Iteration 18, loss = 0.46796517\n",
            "Iteration 19, loss = 0.46730812\n",
            "Iteration 20, loss = 0.46651714\n",
            "Iteration 21, loss = 0.46588622\n",
            "Iteration 22, loss = 0.46521002\n",
            "Iteration 23, loss = 0.46446593\n",
            "Iteration 24, loss = 0.46391554\n",
            "Iteration 25, loss = 0.46326834\n",
            "Iteration 26, loss = 0.46275963\n",
            "Iteration 27, loss = 0.46228717\n",
            "Iteration 28, loss = 0.46179462\n",
            "Iteration 29, loss = 0.46132242\n",
            "Iteration 30, loss = 0.46101534\n",
            "Iteration 31, loss = 0.46058945\n",
            "Iteration 32, loss = 0.46024690\n",
            "Iteration 33, loss = 0.45979032\n",
            "Iteration 34, loss = 0.45951197\n",
            "Iteration 35, loss = 0.45915736\n",
            "Iteration 36, loss = 0.45872596\n",
            "Iteration 37, loss = 0.45843332\n",
            "Iteration 38, loss = 0.45811750\n",
            "Iteration 39, loss = 0.45792341\n",
            "Iteration 40, loss = 0.45763684\n",
            "Iteration 41, loss = 0.45727953\n",
            "Iteration 42, loss = 0.45707473\n",
            "Iteration 43, loss = 0.45681217\n",
            "Iteration 44, loss = 0.45660425\n",
            "Iteration 45, loss = 0.45636699\n",
            "Iteration 46, loss = 0.45600208\n",
            "Iteration 47, loss = 0.45584094\n",
            "Iteration 48, loss = 0.45561701\n",
            "Iteration 49, loss = 0.45534109\n",
            "Iteration 50, loss = 0.45516831\n",
            "Iteration 51, loss = 0.45498717\n",
            "Iteration 52, loss = 0.45483774\n",
            "Iteration 53, loss = 0.45452575\n",
            "Iteration 54, loss = 0.45438344\n",
            "Iteration 55, loss = 0.45416659\n",
            "Iteration 56, loss = 0.45408772\n",
            "Iteration 57, loss = 0.45379544\n",
            "Iteration 58, loss = 0.45369593\n",
            "Iteration 59, loss = 0.45360546\n",
            "Iteration 60, loss = 0.45331897\n",
            "Iteration 61, loss = 0.45316294\n",
            "Iteration 62, loss = 0.45304360\n",
            "Iteration 63, loss = 0.45292410\n",
            "Iteration 64, loss = 0.45284954\n",
            "Iteration 65, loss = 0.45261064\n",
            "Iteration 66, loss = 0.45236360\n",
            "Iteration 67, loss = 0.45243811\n",
            "Iteration 68, loss = 0.45200001\n",
            "Iteration 69, loss = 0.45189777\n",
            "Iteration 70, loss = 0.45190216\n",
            "Iteration 71, loss = 0.45164341\n",
            "Iteration 72, loss = 0.45152330\n",
            "Iteration 73, loss = 0.45154613\n",
            "Iteration 74, loss = 0.45146682\n",
            "Iteration 75, loss = 0.45117254\n",
            "Iteration 76, loss = 0.45111227\n",
            "Iteration 77, loss = 0.45090756\n",
            "Iteration 78, loss = 0.45083192\n",
            "Iteration 79, loss = 0.45091162\n",
            "Iteration 80, loss = 0.45065447\n",
            "Iteration 81, loss = 0.45056020\n",
            "Iteration 82, loss = 0.45044139\n",
            "Iteration 83, loss = 0.45037254\n",
            "Iteration 84, loss = 0.45030048\n",
            "Iteration 85, loss = 0.45027747\n",
            "Iteration 86, loss = 0.45026855\n",
            "Iteration 87, loss = 0.44981441\n",
            "Iteration 88, loss = 0.44998192\n",
            "Iteration 89, loss = 0.44987326\n",
            "Iteration 90, loss = 0.44982329\n",
            "Iteration 91, loss = 0.44972270\n",
            "Iteration 92, loss = 0.44966953\n",
            "Iteration 93, loss = 0.44954847\n",
            "Iteration 94, loss = 0.44958894\n",
            "Iteration 95, loss = 0.44931786\n",
            "Iteration 96, loss = 0.44931483\n",
            "Iteration 97, loss = 0.44932067\n",
            "Iteration 98, loss = 0.44918381\n",
            "Iteration 99, loss = 0.44915292\n",
            "Iteration 100, loss = 0.44905343\n",
            "Iteration 101, loss = 0.44909840\n",
            "Iteration 102, loss = 0.44894959\n",
            "Iteration 103, loss = 0.44896317\n",
            "Iteration 104, loss = 0.44873700\n",
            "Iteration 105, loss = 0.44870674\n",
            "Iteration 106, loss = 0.44882387\n",
            "Iteration 107, loss = 0.44870247\n",
            "Iteration 108, loss = 0.44861948\n",
            "Iteration 109, loss = 0.44874245\n",
            "Iteration 110, loss = 0.44857643\n",
            "Iteration 111, loss = 0.44844747\n",
            "Iteration 112, loss = 0.44828631\n",
            "Iteration 113, loss = 0.44849956\n",
            "Iteration 114, loss = 0.44819772\n",
            "Iteration 115, loss = 0.44837576\n",
            "Iteration 116, loss = 0.44820411\n",
            "Iteration 117, loss = 0.44818566\n",
            "Iteration 118, loss = 0.44813659\n",
            "Iteration 119, loss = 0.44805642\n",
            "Iteration 120, loss = 0.44811134\n",
            "Iteration 121, loss = 0.44800823\n",
            "Iteration 122, loss = 0.44799379\n",
            "Iteration 123, loss = 0.44806893\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 124, loss = 0.44758766\n",
            "Iteration 125, loss = 0.44755926\n",
            "Iteration 126, loss = 0.44754293\n",
            "Iteration 127, loss = 0.44756142\n",
            "Iteration 128, loss = 0.44752760\n",
            "Iteration 129, loss = 0.44750919\n",
            "Iteration 130, loss = 0.44753999\n",
            "Iteration 131, loss = 0.44752555\n",
            "Iteration 132, loss = 0.44753431\n",
            "Iteration 133, loss = 0.44748882\n",
            "Iteration 134, loss = 0.44748518\n",
            "Iteration 135, loss = 0.44752538\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 136, loss = 0.44738798\n",
            "Iteration 137, loss = 0.44739933\n",
            "Iteration 138, loss = 0.44739156\n",
            "Iteration 139, loss = 0.44739194\n",
            "Iteration 140, loss = 0.44739459\n",
            "Iteration 141, loss = 0.44738705\n",
            "Iteration 142, loss = 0.44738477\n",
            "Iteration 143, loss = 0.44738759\n",
            "Iteration 144, loss = 0.44737956\n",
            "Iteration 145, loss = 0.44738008\n",
            "Iteration 146, loss = 0.44738780\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 147, loss = 0.44735775\n",
            "Iteration 148, loss = 0.44736150\n",
            "Iteration 149, loss = 0.44735981\n",
            "Iteration 150, loss = 0.44735881\n",
            "Iteration 151, loss = 0.44735953\n",
            "Iteration 152, loss = 0.44735948\n",
            "Iteration 153, loss = 0.44735800\n",
            "Iteration 154, loss = 0.44735952\n",
            "Iteration 155, loss = 0.44735869\n",
            "Iteration 156, loss = 0.44735626\n",
            "Iteration 157, loss = 0.44735598\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 158, loss = 0.44735252\n",
            "Iteration 159, loss = 0.44735252\n",
            "Iteration 160, loss = 0.44735218\n",
            "Iteration 161, loss = 0.44735283\n",
            "Iteration 162, loss = 0.44735238\n",
            "Iteration 163, loss = 0.44735243\n",
            "Iteration 164, loss = 0.44735261\n",
            "Iteration 165, loss = 0.44735207\n",
            "Iteration 166, loss = 0.44735198\n",
            "Iteration 167, loss = 0.44735203\n",
            "Iteration 168, loss = 0.44735160\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 169, loss = 0.44735104\n",
            "Iteration 170, loss = 0.44735103\n",
            "Iteration 171, loss = 0.44735105\n",
            "Iteration 172, loss = 0.44735105\n",
            "Iteration 173, loss = 0.44735099\n",
            "Iteration 174, loss = 0.44735102\n",
            "Iteration 175, loss = 0.44735102\n",
            "Iteration 176, loss = 0.44735095\n",
            "Iteration 177, loss = 0.44735098\n",
            "Iteration 178, loss = 0.44735089\n",
            "Iteration 179, loss = 0.44735092\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.68886254\n",
            "Iteration 2, loss = 0.65776572\n",
            "Iteration 3, loss = 0.60536375\n",
            "Iteration 4, loss = 0.54413571\n",
            "Iteration 5, loss = 0.50970803\n",
            "Iteration 6, loss = 0.49442693\n",
            "Iteration 7, loss = 0.48676765\n",
            "Iteration 8, loss = 0.48213537\n",
            "Iteration 9, loss = 0.47899751\n",
            "Iteration 10, loss = 0.47669254\n",
            "Iteration 11, loss = 0.47484619\n",
            "Iteration 12, loss = 0.47323214\n",
            "Iteration 13, loss = 0.47187250\n",
            "Iteration 14, loss = 0.47063862\n",
            "Iteration 15, loss = 0.46943690\n",
            "Iteration 16, loss = 0.46864170\n",
            "Iteration 17, loss = 0.46748679\n",
            "Iteration 18, loss = 0.46676675\n",
            "Iteration 19, loss = 0.46587816\n",
            "Iteration 20, loss = 0.46522668\n",
            "Iteration 21, loss = 0.46447108\n",
            "Iteration 22, loss = 0.46378983\n",
            "Iteration 23, loss = 0.46300451\n",
            "Iteration 24, loss = 0.46236759\n",
            "Iteration 25, loss = 0.46178314\n",
            "Iteration 26, loss = 0.46133725\n",
            "Iteration 27, loss = 0.46085539\n",
            "Iteration 28, loss = 0.46036785\n",
            "Iteration 29, loss = 0.45998083\n",
            "Iteration 30, loss = 0.45957342\n",
            "Iteration 31, loss = 0.45926008\n",
            "Iteration 32, loss = 0.45893523\n",
            "Iteration 33, loss = 0.45857416\n",
            "Iteration 34, loss = 0.45826099\n",
            "Iteration 35, loss = 0.45789770\n",
            "Iteration 36, loss = 0.45763976\n",
            "Iteration 37, loss = 0.45729647\n",
            "Iteration 38, loss = 0.45701679\n",
            "Iteration 39, loss = 0.45674528\n",
            "Iteration 40, loss = 0.45637432\n",
            "Iteration 41, loss = 0.45608910\n",
            "Iteration 42, loss = 0.45593284\n",
            "Iteration 43, loss = 0.45564755\n",
            "Iteration 44, loss = 0.45536751\n",
            "Iteration 45, loss = 0.45510830\n",
            "Iteration 46, loss = 0.45484056\n",
            "Iteration 47, loss = 0.45464573\n",
            "Iteration 48, loss = 0.45437548\n",
            "Iteration 49, loss = 0.45421006\n",
            "Iteration 50, loss = 0.45394848\n",
            "Iteration 51, loss = 0.45372638\n",
            "Iteration 52, loss = 0.45351706\n",
            "Iteration 53, loss = 0.45333888\n",
            "Iteration 54, loss = 0.45314387\n",
            "Iteration 55, loss = 0.45293741\n",
            "Iteration 56, loss = 0.45276814\n",
            "Iteration 57, loss = 0.45255019\n",
            "Iteration 58, loss = 0.45260995\n",
            "Iteration 59, loss = 0.45219756\n",
            "Iteration 60, loss = 0.45206452\n",
            "Iteration 61, loss = 0.45183029\n",
            "Iteration 62, loss = 0.45181076\n",
            "Iteration 63, loss = 0.45161221\n",
            "Iteration 64, loss = 0.45149113\n",
            "Iteration 65, loss = 0.45138970\n",
            "Iteration 66, loss = 0.45118624\n",
            "Iteration 67, loss = 0.45106435\n",
            "Iteration 68, loss = 0.45093808\n",
            "Iteration 69, loss = 0.45113380\n",
            "Iteration 70, loss = 0.45066073\n",
            "Iteration 71, loss = 0.45063778\n",
            "Iteration 72, loss = 0.45053178\n",
            "Iteration 73, loss = 0.45039122\n",
            "Iteration 74, loss = 0.45038933\n",
            "Iteration 75, loss = 0.45013820\n",
            "Iteration 76, loss = 0.45019838\n",
            "Iteration 77, loss = 0.44996168\n",
            "Iteration 78, loss = 0.44983906\n",
            "Iteration 79, loss = 0.44988246\n",
            "Iteration 80, loss = 0.44972329\n",
            "Iteration 81, loss = 0.44947600\n",
            "Iteration 82, loss = 0.44944559\n",
            "Iteration 83, loss = 0.44940518\n",
            "Iteration 84, loss = 0.44929491\n",
            "Iteration 85, loss = 0.44927018\n",
            "Iteration 86, loss = 0.44904238\n",
            "Iteration 87, loss = 0.44910987\n",
            "Iteration 88, loss = 0.44910922\n",
            "Iteration 89, loss = 0.44887915\n",
            "Iteration 90, loss = 0.44886694\n",
            "Iteration 91, loss = 0.44868602\n",
            "Iteration 92, loss = 0.44875421\n",
            "Iteration 93, loss = 0.44865521\n",
            "Iteration 94, loss = 0.44848981\n",
            "Iteration 95, loss = 0.44860287\n",
            "Iteration 96, loss = 0.44843000\n",
            "Iteration 97, loss = 0.44843316\n",
            "Iteration 98, loss = 0.44813352\n",
            "Iteration 99, loss = 0.44820596\n",
            "Iteration 100, loss = 0.44806031\n",
            "Iteration 101, loss = 0.44818373\n",
            "Iteration 102, loss = 0.44801708\n",
            "Iteration 103, loss = 0.44789321\n",
            "Iteration 104, loss = 0.44791427\n",
            "Iteration 105, loss = 0.44789594\n",
            "Iteration 106, loss = 0.44779937\n",
            "Iteration 107, loss = 0.44772010\n",
            "Iteration 108, loss = 0.44763194\n",
            "Iteration 109, loss = 0.44748161\n",
            "Iteration 110, loss = 0.44778811\n",
            "Iteration 111, loss = 0.44755939\n",
            "Iteration 112, loss = 0.44734794\n",
            "Iteration 113, loss = 0.44735754\n",
            "Iteration 114, loss = 0.44735062\n",
            "Iteration 115, loss = 0.44732296\n",
            "Iteration 116, loss = 0.44727040\n",
            "Iteration 117, loss = 0.44707172\n",
            "Iteration 118, loss = 0.44737735\n",
            "Iteration 119, loss = 0.44704633\n",
            "Iteration 120, loss = 0.44699152\n",
            "Iteration 121, loss = 0.44703875\n",
            "Iteration 122, loss = 0.44700179\n",
            "Iteration 123, loss = 0.44695137\n",
            "Iteration 124, loss = 0.44691943\n",
            "Iteration 125, loss = 0.44692551\n",
            "Iteration 126, loss = 0.44676935\n",
            "Iteration 127, loss = 0.44672189\n",
            "Iteration 128, loss = 0.44679627\n",
            "Iteration 129, loss = 0.44674360\n",
            "Iteration 130, loss = 0.44666736\n",
            "Iteration 131, loss = 0.44671569\n",
            "Iteration 132, loss = 0.44668967\n",
            "Iteration 133, loss = 0.44655561\n",
            "Iteration 134, loss = 0.44639930\n",
            "Iteration 135, loss = 0.44663895\n",
            "Iteration 136, loss = 0.44640520\n",
            "Iteration 137, loss = 0.44658335\n",
            "Iteration 138, loss = 0.44638989\n",
            "Iteration 139, loss = 0.44639857\n",
            "Iteration 140, loss = 0.44619838\n",
            "Iteration 141, loss = 0.44625487\n",
            "Iteration 142, loss = 0.44633690\n",
            "Iteration 143, loss = 0.44631477\n",
            "Iteration 144, loss = 0.44596766\n",
            "Iteration 145, loss = 0.44620000\n",
            "Iteration 146, loss = 0.44632963\n",
            "Iteration 147, loss = 0.44618680\n",
            "Iteration 148, loss = 0.44608576\n",
            "Iteration 149, loss = 0.44595187\n",
            "Iteration 150, loss = 0.44600579\n",
            "Iteration 151, loss = 0.44590575\n",
            "Iteration 152, loss = 0.44599074\n",
            "Iteration 153, loss = 0.44581307\n",
            "Iteration 154, loss = 0.44580222\n",
            "Iteration 155, loss = 0.44579221\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 156, loss = 0.44558158\n",
            "Iteration 157, loss = 0.44548274\n",
            "Iteration 158, loss = 0.44549139\n",
            "Iteration 159, loss = 0.44551619\n",
            "Iteration 160, loss = 0.44545742\n",
            "Iteration 161, loss = 0.44548718\n",
            "Iteration 162, loss = 0.44544140\n",
            "Iteration 163, loss = 0.44546722\n",
            "Iteration 164, loss = 0.44547819\n",
            "Iteration 165, loss = 0.44545391\n",
            "Iteration 166, loss = 0.44546620\n",
            "Iteration 167, loss = 0.44542704\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 168, loss = 0.44535461\n",
            "Iteration 169, loss = 0.44534713\n",
            "Iteration 170, loss = 0.44535183\n",
            "Iteration 171, loss = 0.44534994\n",
            "Iteration 172, loss = 0.44535443\n",
            "Iteration 173, loss = 0.44535336\n",
            "Iteration 174, loss = 0.44534912\n",
            "Iteration 175, loss = 0.44534091\n",
            "Iteration 176, loss = 0.44533599\n",
            "Iteration 177, loss = 0.44534470\n",
            "Iteration 178, loss = 0.44533981\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 179, loss = 0.44531678\n",
            "Iteration 180, loss = 0.44531931\n",
            "Iteration 181, loss = 0.44531834\n",
            "Iteration 182, loss = 0.44531754\n",
            "Iteration 183, loss = 0.44531772\n",
            "Iteration 184, loss = 0.44531656\n",
            "Iteration 185, loss = 0.44531566\n",
            "Iteration 186, loss = 0.44531636\n",
            "Iteration 187, loss = 0.44531501\n",
            "Iteration 188, loss = 0.44531792\n",
            "Iteration 189, loss = 0.44531507\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 190, loss = 0.44531134\n",
            "Iteration 191, loss = 0.44531110\n",
            "Iteration 192, loss = 0.44531135\n",
            "Iteration 193, loss = 0.44531128\n",
            "Iteration 194, loss = 0.44531131\n",
            "Iteration 195, loss = 0.44531110\n",
            "Iteration 196, loss = 0.44531147\n",
            "Iteration 197, loss = 0.44531130\n",
            "Iteration 198, loss = 0.44531120\n",
            "Iteration 199, loss = 0.44531092\n",
            "Iteration 200, loss = 0.44531072\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 1, loss = 0.68884278\n",
            "Iteration 2, loss = 0.65787433\n",
            "Iteration 3, loss = 0.60439038\n",
            "Iteration 4, loss = 0.54306690\n",
            "Iteration 5, loss = 0.51003856\n",
            "Iteration 6, loss = 0.49541587\n",
            "Iteration 7, loss = 0.48792956\n",
            "Iteration 8, loss = 0.48342131\n",
            "Iteration 9, loss = 0.48034539\n",
            "Iteration 10, loss = 0.47801750\n",
            "Iteration 11, loss = 0.47606689\n",
            "Iteration 12, loss = 0.47437670\n",
            "Iteration 13, loss = 0.47330518\n",
            "Iteration 14, loss = 0.47180931\n",
            "Iteration 15, loss = 0.47082114\n",
            "Iteration 16, loss = 0.46984582\n",
            "Iteration 17, loss = 0.46877060\n",
            "Iteration 18, loss = 0.46787347\n",
            "Iteration 19, loss = 0.46709591\n",
            "Iteration 20, loss = 0.46636314\n",
            "Iteration 21, loss = 0.46555347\n",
            "Iteration 22, loss = 0.46473442\n",
            "Iteration 23, loss = 0.46400894\n",
            "Iteration 24, loss = 0.46338675\n",
            "Iteration 25, loss = 0.46279748\n",
            "Iteration 26, loss = 0.46227097\n",
            "Iteration 27, loss = 0.46186489\n",
            "Iteration 28, loss = 0.46118141\n",
            "Iteration 29, loss = 0.46083481\n",
            "Iteration 30, loss = 0.46042779\n",
            "Iteration 31, loss = 0.45994526\n",
            "Iteration 32, loss = 0.45960159\n",
            "Iteration 33, loss = 0.45930184\n",
            "Iteration 34, loss = 0.45884867\n",
            "Iteration 35, loss = 0.45856981\n",
            "Iteration 36, loss = 0.45800305\n",
            "Iteration 37, loss = 0.45808668\n",
            "Iteration 38, loss = 0.45765878\n",
            "Iteration 39, loss = 0.45738640\n",
            "Iteration 40, loss = 0.45716399\n",
            "Iteration 41, loss = 0.45691237\n",
            "Iteration 42, loss = 0.45653287\n",
            "Iteration 43, loss = 0.45630999\n",
            "Iteration 44, loss = 0.45611977\n",
            "Iteration 45, loss = 0.45578506\n",
            "Iteration 46, loss = 0.45567282\n",
            "Iteration 47, loss = 0.45537282\n",
            "Iteration 48, loss = 0.45513053\n",
            "Iteration 49, loss = 0.45490531\n",
            "Iteration 50, loss = 0.45479198\n",
            "Iteration 51, loss = 0.45446666\n",
            "Iteration 52, loss = 0.45426360\n",
            "Iteration 53, loss = 0.45417266\n",
            "Iteration 54, loss = 0.45384367\n",
            "Iteration 55, loss = 0.45373766\n",
            "Iteration 56, loss = 0.45355208\n",
            "Iteration 57, loss = 0.45342447\n",
            "Iteration 58, loss = 0.45335974\n",
            "Iteration 59, loss = 0.45311285\n",
            "Iteration 60, loss = 0.45297221\n",
            "Iteration 61, loss = 0.45267699\n",
            "Iteration 62, loss = 0.45257850\n",
            "Iteration 63, loss = 0.45234750\n",
            "Iteration 64, loss = 0.45223929\n",
            "Iteration 65, loss = 0.45216363\n",
            "Iteration 66, loss = 0.45195689\n",
            "Iteration 67, loss = 0.45181796\n",
            "Iteration 68, loss = 0.45175297\n",
            "Iteration 69, loss = 0.45155478\n",
            "Iteration 70, loss = 0.45135913\n",
            "Iteration 71, loss = 0.45115512\n",
            "Iteration 72, loss = 0.45115665\n",
            "Iteration 73, loss = 0.45107668\n",
            "Iteration 74, loss = 0.45069281\n",
            "Iteration 75, loss = 0.45073996\n",
            "Iteration 76, loss = 0.45062417\n",
            "Iteration 77, loss = 0.45046486\n",
            "Iteration 78, loss = 0.45042898\n",
            "Iteration 79, loss = 0.45027366\n",
            "Iteration 80, loss = 0.45019063\n",
            "Iteration 81, loss = 0.45014005\n",
            "Iteration 82, loss = 0.45019514\n",
            "Iteration 83, loss = 0.45001835\n",
            "Iteration 84, loss = 0.44961432\n",
            "Iteration 85, loss = 0.44979687\n",
            "Iteration 86, loss = 0.44970590\n",
            "Iteration 87, loss = 0.44948553\n",
            "Iteration 88, loss = 0.44947657\n",
            "Iteration 89, loss = 0.44931496\n",
            "Iteration 90, loss = 0.44919468\n",
            "Iteration 91, loss = 0.44932181\n",
            "Iteration 92, loss = 0.44906928\n",
            "Iteration 93, loss = 0.44904746\n",
            "Iteration 94, loss = 0.44900509\n",
            "Iteration 95, loss = 0.44886793\n",
            "Iteration 96, loss = 0.44892241\n",
            "Iteration 97, loss = 0.44874615\n",
            "Iteration 98, loss = 0.44866200\n",
            "Iteration 99, loss = 0.44861774\n",
            "Iteration 100, loss = 0.44861241\n",
            "Iteration 101, loss = 0.44849253\n",
            "Iteration 102, loss = 0.44840619\n",
            "Iteration 103, loss = 0.44836513\n",
            "Iteration 104, loss = 0.44823270\n",
            "Iteration 105, loss = 0.44825821\n",
            "Iteration 106, loss = 0.44825207\n",
            "Iteration 107, loss = 0.44805202\n",
            "Iteration 108, loss = 0.44817728\n",
            "Iteration 109, loss = 0.44803600\n",
            "Iteration 110, loss = 0.44795345\n",
            "Iteration 111, loss = 0.44785849\n",
            "Iteration 112, loss = 0.44784061\n",
            "Iteration 113, loss = 0.44788125\n",
            "Iteration 114, loss = 0.44770987\n",
            "Iteration 115, loss = 0.44766139\n",
            "Iteration 116, loss = 0.44762302\n",
            "Iteration 117, loss = 0.44761519\n",
            "Iteration 118, loss = 0.44758365\n",
            "Iteration 119, loss = 0.44744331\n",
            "Iteration 120, loss = 0.44753824\n",
            "Iteration 121, loss = 0.44745846\n",
            "Iteration 122, loss = 0.44743439\n",
            "Iteration 123, loss = 0.44744095\n",
            "Iteration 124, loss = 0.44740596\n",
            "Iteration 125, loss = 0.44710836\n",
            "Iteration 126, loss = 0.44725258\n",
            "Iteration 127, loss = 0.44718191\n",
            "Iteration 128, loss = 0.44709049\n",
            "Iteration 129, loss = 0.44711125\n",
            "Iteration 130, loss = 0.44699643\n",
            "Iteration 131, loss = 0.44708438\n",
            "Iteration 132, loss = 0.44691743\n",
            "Iteration 133, loss = 0.44691770\n",
            "Iteration 134, loss = 0.44685043\n",
            "Iteration 135, loss = 0.44700656\n",
            "Iteration 136, loss = 0.44684302\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 137, loss = 0.44654836\n",
            "Iteration 138, loss = 0.44652574\n",
            "Iteration 139, loss = 0.44650545\n",
            "Iteration 140, loss = 0.44647174\n",
            "Iteration 141, loss = 0.44648918\n",
            "Iteration 142, loss = 0.44648418\n",
            "Iteration 143, loss = 0.44648371\n",
            "Iteration 144, loss = 0.44643719\n",
            "Iteration 145, loss = 0.44643521\n",
            "Iteration 146, loss = 0.44644969\n",
            "Iteration 147, loss = 0.44651517\n",
            "Iteration 148, loss = 0.44647687\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 149, loss = 0.44635530\n",
            "Iteration 150, loss = 0.44634500\n",
            "Iteration 151, loss = 0.44634603\n",
            "Iteration 152, loss = 0.44635546\n",
            "Iteration 153, loss = 0.44634155\n",
            "Iteration 154, loss = 0.44634549\n",
            "Iteration 155, loss = 0.44633783\n",
            "Iteration 156, loss = 0.44634058\n",
            "Iteration 157, loss = 0.44632978\n",
            "Iteration 158, loss = 0.44634770\n",
            "Iteration 159, loss = 0.44633114\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 160, loss = 0.44631749\n",
            "Iteration 161, loss = 0.44631473\n",
            "Iteration 162, loss = 0.44631320\n",
            "Iteration 163, loss = 0.44631195\n",
            "Iteration 164, loss = 0.44631174\n",
            "Iteration 165, loss = 0.44631175\n",
            "Iteration 166, loss = 0.44631252\n",
            "Iteration 167, loss = 0.44631192\n",
            "Iteration 168, loss = 0.44631147\n",
            "Iteration 169, loss = 0.44631259\n",
            "Iteration 170, loss = 0.44631172\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 171, loss = 0.44630580\n",
            "Iteration 172, loss = 0.44630605\n",
            "Iteration 173, loss = 0.44630572\n",
            "Iteration 174, loss = 0.44630587\n",
            "Iteration 175, loss = 0.44630643\n",
            "Iteration 176, loss = 0.44630576\n",
            "Iteration 177, loss = 0.44630567\n",
            "Iteration 178, loss = 0.44630547\n",
            "Iteration 179, loss = 0.44630562\n",
            "Iteration 180, loss = 0.44630541\n",
            "Iteration 181, loss = 0.44630556\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 182, loss = 0.44630449\n",
            "Iteration 183, loss = 0.44630463\n",
            "Iteration 184, loss = 0.44630453\n",
            "Iteration 185, loss = 0.44630449\n",
            "Iteration 186, loss = 0.44630443\n",
            "Iteration 187, loss = 0.44630446\n",
            "Iteration 188, loss = 0.44630447\n",
            "Iteration 189, loss = 0.44630443\n",
            "Iteration 190, loss = 0.44630446\n",
            "Iteration 191, loss = 0.44630446\n",
            "Iteration 192, loss = 0.44630447\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.68875060\n",
            "Iteration 2, loss = 0.65773535\n",
            "Iteration 3, loss = 0.60519319\n",
            "Iteration 4, loss = 0.54411542\n",
            "Iteration 5, loss = 0.51067286\n",
            "Iteration 6, loss = 0.49575088\n",
            "Iteration 7, loss = 0.48820090\n",
            "Iteration 8, loss = 0.48364914\n",
            "Iteration 9, loss = 0.48048636\n",
            "Iteration 10, loss = 0.47817237\n",
            "Iteration 11, loss = 0.47630273\n",
            "Iteration 12, loss = 0.47474104\n",
            "Iteration 13, loss = 0.47341704\n",
            "Iteration 14, loss = 0.47223302\n",
            "Iteration 15, loss = 0.47101891\n",
            "Iteration 16, loss = 0.47013178\n",
            "Iteration 17, loss = 0.46913923\n",
            "Iteration 18, loss = 0.46822870\n",
            "Iteration 19, loss = 0.46743718\n",
            "Iteration 20, loss = 0.46666823\n",
            "Iteration 21, loss = 0.46593405\n",
            "Iteration 22, loss = 0.46511585\n",
            "Iteration 23, loss = 0.46446091\n",
            "Iteration 24, loss = 0.46391394\n",
            "Iteration 25, loss = 0.46328499\n",
            "Iteration 26, loss = 0.46262976\n",
            "Iteration 27, loss = 0.46236784\n",
            "Iteration 28, loss = 0.46171251\n",
            "Iteration 29, loss = 0.46134908\n",
            "Iteration 30, loss = 0.46094568\n",
            "Iteration 31, loss = 0.46047208\n",
            "Iteration 32, loss = 0.46013819\n",
            "Iteration 33, loss = 0.45971345\n",
            "Iteration 34, loss = 0.45947088\n",
            "Iteration 35, loss = 0.45910957\n",
            "Iteration 36, loss = 0.45884869\n",
            "Iteration 37, loss = 0.45850659\n",
            "Iteration 38, loss = 0.45831505\n",
            "Iteration 39, loss = 0.45794925\n",
            "Iteration 40, loss = 0.45757455\n",
            "Iteration 41, loss = 0.45737343\n",
            "Iteration 42, loss = 0.45725854\n",
            "Iteration 43, loss = 0.45687044\n",
            "Iteration 44, loss = 0.45652799\n",
            "Iteration 45, loss = 0.45632582\n",
            "Iteration 46, loss = 0.45607288\n",
            "Iteration 47, loss = 0.45598949\n",
            "Iteration 48, loss = 0.45565916\n",
            "Iteration 49, loss = 0.45565121\n",
            "Iteration 50, loss = 0.45531052\n",
            "Iteration 51, loss = 0.45507353\n",
            "Iteration 52, loss = 0.45485659\n",
            "Iteration 53, loss = 0.45462885\n",
            "Iteration 54, loss = 0.45449616\n",
            "Iteration 55, loss = 0.45433363\n",
            "Iteration 56, loss = 0.45409074\n",
            "Iteration 57, loss = 0.45400534\n",
            "Iteration 58, loss = 0.45379261\n",
            "Iteration 59, loss = 0.45362423\n",
            "Iteration 60, loss = 0.45342000\n",
            "Iteration 61, loss = 0.45331741\n",
            "Iteration 62, loss = 0.45312145\n",
            "Iteration 63, loss = 0.45307509\n",
            "Iteration 64, loss = 0.45278847\n",
            "Iteration 65, loss = 0.45275515\n",
            "Iteration 66, loss = 0.45252365\n",
            "Iteration 67, loss = 0.45235167\n",
            "Iteration 68, loss = 0.45224901\n",
            "Iteration 69, loss = 0.45213717\n",
            "Iteration 70, loss = 0.45205425\n",
            "Iteration 71, loss = 0.45180977\n",
            "Iteration 72, loss = 0.45171073\n",
            "Iteration 73, loss = 0.45173447\n",
            "Iteration 74, loss = 0.45159958\n",
            "Iteration 75, loss = 0.45144976\n",
            "Iteration 76, loss = 0.45134328\n",
            "Iteration 77, loss = 0.45120018\n",
            "Iteration 78, loss = 0.45102549\n",
            "Iteration 79, loss = 0.45103177\n",
            "Iteration 80, loss = 0.45100088\n",
            "Iteration 81, loss = 0.45068465\n",
            "Iteration 82, loss = 0.45077165\n",
            "Iteration 83, loss = 0.45057304\n",
            "Iteration 84, loss = 0.45053312\n",
            "Iteration 85, loss = 0.45049585\n",
            "Iteration 86, loss = 0.45034325\n",
            "Iteration 87, loss = 0.45030921\n",
            "Iteration 88, loss = 0.45024817\n",
            "Iteration 89, loss = 0.45005358\n",
            "Iteration 90, loss = 0.44998789\n",
            "Iteration 91, loss = 0.44989162\n",
            "Iteration 92, loss = 0.44986130\n",
            "Iteration 93, loss = 0.44981075\n",
            "Iteration 94, loss = 0.44960410\n",
            "Iteration 95, loss = 0.44968421\n",
            "Iteration 96, loss = 0.44955762\n",
            "Iteration 97, loss = 0.44944566\n",
            "Iteration 98, loss = 0.44930251\n",
            "Iteration 99, loss = 0.44934790\n",
            "Iteration 100, loss = 0.44927796\n",
            "Iteration 101, loss = 0.44915078\n",
            "Iteration 102, loss = 0.44908908\n",
            "Iteration 103, loss = 0.44910259\n",
            "Iteration 104, loss = 0.44887989\n",
            "Iteration 105, loss = 0.44895317\n",
            "Iteration 106, loss = 0.44892693\n",
            "Iteration 107, loss = 0.44885479\n",
            "Iteration 108, loss = 0.44879281\n",
            "Iteration 109, loss = 0.44880664\n",
            "Iteration 110, loss = 0.44882402\n",
            "Iteration 111, loss = 0.44876743\n",
            "Iteration 112, loss = 0.44861542\n",
            "Iteration 113, loss = 0.44846053\n",
            "Iteration 114, loss = 0.44857069\n",
            "Iteration 115, loss = 0.44851108\n",
            "Iteration 116, loss = 0.44840783\n",
            "Iteration 117, loss = 0.44827982\n",
            "Iteration 118, loss = 0.44831135\n",
            "Iteration 119, loss = 0.44830494\n",
            "Iteration 120, loss = 0.44823730\n",
            "Iteration 121, loss = 0.44828650\n",
            "Iteration 122, loss = 0.44816410\n",
            "Iteration 123, loss = 0.44812565\n",
            "Iteration 124, loss = 0.44828506\n",
            "Iteration 125, loss = 0.44809171\n",
            "Iteration 126, loss = 0.44799002\n",
            "Iteration 127, loss = 0.44794571\n",
            "Iteration 128, loss = 0.44794155\n",
            "Iteration 129, loss = 0.44782477\n",
            "Iteration 130, loss = 0.44803696\n",
            "Iteration 131, loss = 0.44797190\n",
            "Iteration 132, loss = 0.44783251\n",
            "Iteration 133, loss = 0.44790674\n",
            "Iteration 134, loss = 0.44788079\n",
            "Iteration 135, loss = 0.44771829\n",
            "Iteration 136, loss = 0.44771292\n",
            "Iteration 137, loss = 0.44766795\n",
            "Iteration 138, loss = 0.44764424\n",
            "Iteration 139, loss = 0.44752273\n",
            "Iteration 140, loss = 0.44763934\n",
            "Iteration 141, loss = 0.44755314\n",
            "Iteration 142, loss = 0.44743095\n",
            "Iteration 143, loss = 0.44757065\n",
            "Iteration 144, loss = 0.44752055\n",
            "Iteration 145, loss = 0.44738521\n",
            "Iteration 146, loss = 0.44754613\n",
            "Iteration 147, loss = 0.44742001\n",
            "Iteration 148, loss = 0.44734867\n",
            "Iteration 149, loss = 0.44730399\n",
            "Iteration 150, loss = 0.44734433\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 151, loss = 0.44702919\n",
            "Iteration 152, loss = 0.44694296\n",
            "Iteration 153, loss = 0.44691653\n",
            "Iteration 154, loss = 0.44691248\n",
            "Iteration 155, loss = 0.44693133\n",
            "Iteration 156, loss = 0.44692679\n",
            "Iteration 157, loss = 0.44694619\n",
            "Iteration 158, loss = 0.44691519\n",
            "Iteration 159, loss = 0.44689235\n",
            "Iteration 160, loss = 0.44688991\n",
            "Iteration 161, loss = 0.44687354\n",
            "Iteration 162, loss = 0.44689307\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 163, loss = 0.44678114\n",
            "Iteration 164, loss = 0.44678263\n",
            "Iteration 165, loss = 0.44678001\n",
            "Iteration 166, loss = 0.44677756\n",
            "Iteration 167, loss = 0.44677499\n",
            "Iteration 168, loss = 0.44679788\n",
            "Iteration 169, loss = 0.44677002\n",
            "Iteration 170, loss = 0.44678119\n",
            "Iteration 171, loss = 0.44676809\n",
            "Iteration 172, loss = 0.44676986\n",
            "Iteration 173, loss = 0.44677033\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 174, loss = 0.44675542\n",
            "Iteration 175, loss = 0.44675160\n",
            "Iteration 176, loss = 0.44675102\n",
            "Iteration 177, loss = 0.44675033\n",
            "Iteration 178, loss = 0.44674975\n",
            "Iteration 179, loss = 0.44675020\n",
            "Iteration 180, loss = 0.44674869\n",
            "Iteration 181, loss = 0.44674777\n",
            "Iteration 182, loss = 0.44674857\n",
            "Iteration 183, loss = 0.44674940\n",
            "Iteration 184, loss = 0.44674839\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 185, loss = 0.44674317\n",
            "Iteration 186, loss = 0.44674321\n",
            "Iteration 187, loss = 0.44674357\n",
            "Iteration 188, loss = 0.44674384\n",
            "Iteration 189, loss = 0.44674329\n",
            "Iteration 190, loss = 0.44674312\n",
            "Iteration 191, loss = 0.44674323\n",
            "Iteration 192, loss = 0.44674302\n",
            "Iteration 193, loss = 0.44674335\n",
            "Iteration 194, loss = 0.44674322\n",
            "Iteration 195, loss = 0.44674310\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 196, loss = 0.44674235\n",
            "Iteration 197, loss = 0.44674220\n",
            "Iteration 198, loss = 0.44674217\n",
            "Iteration 199, loss = 0.44674212\n",
            "Iteration 200, loss = 0.44674211\n",
            "Iteration 1, loss = 0.68878021\n",
            "Iteration 2, loss = 0.65826011\n",
            "Iteration 3, loss = 0.60789218\n",
            "Iteration 4, loss = 0.54832812\n",
            "Iteration 5, loss = 0.51281224\n",
            "Iteration 6, loss = 0.49670473\n",
            "Iteration 7, loss = 0.48852145\n",
            "Iteration 8, loss = 0.48356602\n",
            "Iteration 9, loss = 0.48024091\n",
            "Iteration 10, loss = 0.47778010\n",
            "Iteration 11, loss = 0.47596823\n",
            "Iteration 12, loss = 0.47432213\n",
            "Iteration 13, loss = 0.47286388\n",
            "Iteration 14, loss = 0.47165959\n",
            "Iteration 15, loss = 0.47047463\n",
            "Iteration 16, loss = 0.46946409\n",
            "Iteration 17, loss = 0.46847938\n",
            "Iteration 18, loss = 0.46770129\n",
            "Iteration 19, loss = 0.46685406\n",
            "Iteration 20, loss = 0.46622808\n",
            "Iteration 21, loss = 0.46545727\n",
            "Iteration 22, loss = 0.46482427\n",
            "Iteration 23, loss = 0.46404515\n",
            "Iteration 24, loss = 0.46345533\n",
            "Iteration 25, loss = 0.46285517\n",
            "Iteration 26, loss = 0.46225320\n",
            "Iteration 27, loss = 0.46181833\n",
            "Iteration 28, loss = 0.46131162\n",
            "Iteration 29, loss = 0.46090704\n",
            "Iteration 30, loss = 0.46039299\n",
            "Iteration 31, loss = 0.46008896\n",
            "Iteration 32, loss = 0.45966738\n",
            "Iteration 33, loss = 0.45923994\n",
            "Iteration 34, loss = 0.45893417\n",
            "Iteration 35, loss = 0.45849625\n",
            "Iteration 36, loss = 0.45829977\n",
            "Iteration 37, loss = 0.45793350\n",
            "Iteration 38, loss = 0.45758089\n",
            "Iteration 39, loss = 0.45734659\n",
            "Iteration 40, loss = 0.45705313\n",
            "Iteration 41, loss = 0.45674263\n",
            "Iteration 42, loss = 0.45649658\n",
            "Iteration 43, loss = 0.45626825\n",
            "Iteration 44, loss = 0.45603397\n",
            "Iteration 45, loss = 0.45575574\n",
            "Iteration 46, loss = 0.45558272\n",
            "Iteration 47, loss = 0.45527107\n",
            "Iteration 48, loss = 0.45504260\n",
            "Iteration 49, loss = 0.45483802\n",
            "Iteration 50, loss = 0.45462681\n",
            "Iteration 51, loss = 0.45435318\n",
            "Iteration 52, loss = 0.45426120\n",
            "Iteration 53, loss = 0.45397297\n",
            "Iteration 54, loss = 0.45385823\n",
            "Iteration 55, loss = 0.45360621\n",
            "Iteration 56, loss = 0.45344137\n",
            "Iteration 57, loss = 0.45328869\n",
            "Iteration 58, loss = 0.45316193\n",
            "Iteration 59, loss = 0.45300042\n",
            "Iteration 60, loss = 0.45280490\n",
            "Iteration 61, loss = 0.45260887\n",
            "Iteration 62, loss = 0.45256898\n",
            "Iteration 63, loss = 0.45244203\n",
            "Iteration 64, loss = 0.45218668\n",
            "Iteration 65, loss = 0.45213744\n",
            "Iteration 66, loss = 0.45195952\n",
            "Iteration 67, loss = 0.45162354\n",
            "Iteration 68, loss = 0.45161210\n",
            "Iteration 69, loss = 0.45141457\n",
            "Iteration 70, loss = 0.45120258\n",
            "Iteration 71, loss = 0.45123914\n",
            "Iteration 72, loss = 0.45108697\n",
            "Iteration 73, loss = 0.45082264\n",
            "Iteration 74, loss = 0.45078526\n",
            "Iteration 75, loss = 0.45080495\n",
            "Iteration 76, loss = 0.45055261\n",
            "Iteration 77, loss = 0.45047454\n",
            "Iteration 78, loss = 0.45032766\n",
            "Iteration 79, loss = 0.45014520\n",
            "Iteration 80, loss = 0.45009720\n",
            "Iteration 81, loss = 0.44993578\n",
            "Iteration 82, loss = 0.44984160\n",
            "Iteration 83, loss = 0.44960417\n",
            "Iteration 84, loss = 0.44977847\n",
            "Iteration 85, loss = 0.44956860\n",
            "Iteration 86, loss = 0.44949658\n",
            "Iteration 87, loss = 0.44928357\n",
            "Iteration 88, loss = 0.44922937\n",
            "Iteration 89, loss = 0.44916611\n",
            "Iteration 90, loss = 0.44917325\n",
            "Iteration 91, loss = 0.44904141\n",
            "Iteration 92, loss = 0.44905997\n",
            "Iteration 93, loss = 0.44877962\n",
            "Iteration 94, loss = 0.44879510\n",
            "Iteration 95, loss = 0.44884312\n",
            "Iteration 96, loss = 0.44862730\n",
            "Iteration 97, loss = 0.44858052\n",
            "Iteration 98, loss = 0.44852503\n",
            "Iteration 99, loss = 0.44845551\n",
            "Iteration 100, loss = 0.44851647\n",
            "Iteration 101, loss = 0.44840187\n",
            "Iteration 102, loss = 0.44823131\n",
            "Iteration 103, loss = 0.44825709\n",
            "Iteration 104, loss = 0.44822765\n",
            "Iteration 105, loss = 0.44822058\n",
            "Iteration 106, loss = 0.44821940\n",
            "Iteration 107, loss = 0.44805976\n",
            "Iteration 108, loss = 0.44801201\n",
            "Iteration 109, loss = 0.44789262\n",
            "Iteration 110, loss = 0.44792471\n",
            "Iteration 111, loss = 0.44768001\n",
            "Iteration 112, loss = 0.44790954\n",
            "Iteration 113, loss = 0.44779419\n",
            "Iteration 114, loss = 0.44785153\n",
            "Iteration 115, loss = 0.44771017\n",
            "Iteration 116, loss = 0.44757500\n",
            "Iteration 117, loss = 0.44750394\n",
            "Iteration 118, loss = 0.44741845\n",
            "Iteration 119, loss = 0.44742744\n",
            "Iteration 120, loss = 0.44751286\n",
            "Iteration 121, loss = 0.44749649\n",
            "Iteration 122, loss = 0.44743326\n",
            "Iteration 123, loss = 0.44733638\n",
            "Iteration 124, loss = 0.44721380\n",
            "Iteration 125, loss = 0.44724799\n",
            "Iteration 126, loss = 0.44730034\n",
            "Iteration 127, loss = 0.44721681\n",
            "Iteration 128, loss = 0.44707593\n",
            "Iteration 129, loss = 0.44702952\n",
            "Iteration 130, loss = 0.44698908\n",
            "Iteration 131, loss = 0.44700058\n",
            "Iteration 132, loss = 0.44704936\n",
            "Iteration 133, loss = 0.44679854\n",
            "Iteration 134, loss = 0.44696343\n",
            "Iteration 135, loss = 0.44693838\n",
            "Iteration 136, loss = 0.44688360\n",
            "Iteration 137, loss = 0.44679444\n",
            "Iteration 138, loss = 0.44672906\n",
            "Iteration 139, loss = 0.44682739\n",
            "Iteration 140, loss = 0.44674552\n",
            "Iteration 141, loss = 0.44673818\n",
            "Iteration 142, loss = 0.44668049\n",
            "Iteration 143, loss = 0.44671824\n",
            "Iteration 144, loss = 0.44652270\n",
            "Iteration 145, loss = 0.44655350\n",
            "Iteration 146, loss = 0.44651999\n",
            "Iteration 147, loss = 0.44647709\n",
            "Iteration 148, loss = 0.44641351\n",
            "Iteration 149, loss = 0.44647724\n",
            "Iteration 150, loss = 0.44644340\n",
            "Iteration 151, loss = 0.44632169\n",
            "Iteration 152, loss = 0.44652579\n",
            "Iteration 153, loss = 0.44634777\n",
            "Iteration 154, loss = 0.44623387\n",
            "Iteration 155, loss = 0.44630277\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 156, loss = 0.44591571\n",
            "Iteration 157, loss = 0.44591333\n",
            "Iteration 158, loss = 0.44588874\n",
            "Iteration 159, loss = 0.44587866\n",
            "Iteration 160, loss = 0.44590262\n",
            "Iteration 161, loss = 0.44589830\n",
            "Iteration 162, loss = 0.44584459\n",
            "Iteration 163, loss = 0.44588523\n",
            "Iteration 164, loss = 0.44586962\n",
            "Iteration 165, loss = 0.44586927\n",
            "Iteration 166, loss = 0.44586527\n",
            "Iteration 167, loss = 0.44582956\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 168, loss = 0.44576383\n",
            "Iteration 169, loss = 0.44576164\n",
            "Iteration 170, loss = 0.44575210\n",
            "Iteration 171, loss = 0.44575638\n",
            "Iteration 172, loss = 0.44576229\n",
            "Iteration 173, loss = 0.44575286\n",
            "Iteration 174, loss = 0.44575598\n",
            "Iteration 175, loss = 0.44575604\n",
            "Iteration 176, loss = 0.44574708\n",
            "Iteration 177, loss = 0.44574734\n",
            "Iteration 178, loss = 0.44574611\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 179, loss = 0.44572669\n",
            "Iteration 180, loss = 0.44572861\n",
            "Iteration 181, loss = 0.44572906\n",
            "Iteration 182, loss = 0.44572721\n",
            "Iteration 183, loss = 0.44572775\n",
            "Iteration 184, loss = 0.44572544\n",
            "Iteration 185, loss = 0.44572749\n",
            "Iteration 186, loss = 0.44572627\n",
            "Iteration 187, loss = 0.44572596\n",
            "Iteration 188, loss = 0.44572441\n",
            "Iteration 189, loss = 0.44572560\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 190, loss = 0.44572158\n",
            "Iteration 191, loss = 0.44572106\n",
            "Iteration 192, loss = 0.44572109\n",
            "Iteration 193, loss = 0.44572115\n",
            "Iteration 194, loss = 0.44572088\n",
            "Iteration 195, loss = 0.44572096\n",
            "Iteration 196, loss = 0.44572138\n",
            "Iteration 197, loss = 0.44572066\n",
            "Iteration 198, loss = 0.44572119\n",
            "Iteration 199, loss = 0.44572091\n",
            "Iteration 200, loss = 0.44572100\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 1, loss = 0.68887131\n",
            "Iteration 2, loss = 0.65808639\n",
            "Iteration 3, loss = 0.60707206\n",
            "Iteration 4, loss = 0.54660288\n",
            "Iteration 5, loss = 0.51151051\n",
            "Iteration 6, loss = 0.49556977\n",
            "Iteration 7, loss = 0.48755706\n",
            "Iteration 8, loss = 0.48262730\n",
            "Iteration 9, loss = 0.47929408\n",
            "Iteration 10, loss = 0.47680391\n",
            "Iteration 11, loss = 0.47491479\n",
            "Iteration 12, loss = 0.47331803\n",
            "Iteration 13, loss = 0.47182906\n",
            "Iteration 14, loss = 0.47062217\n",
            "Iteration 15, loss = 0.46949133\n",
            "Iteration 16, loss = 0.46852263\n",
            "Iteration 17, loss = 0.46750656\n",
            "Iteration 18, loss = 0.46671987\n",
            "Iteration 19, loss = 0.46597424\n",
            "Iteration 20, loss = 0.46514396\n",
            "Iteration 21, loss = 0.46449600\n",
            "Iteration 22, loss = 0.46383728\n",
            "Iteration 23, loss = 0.46292852\n",
            "Iteration 24, loss = 0.46246132\n",
            "Iteration 25, loss = 0.46188007\n",
            "Iteration 26, loss = 0.46118303\n",
            "Iteration 27, loss = 0.46075467\n",
            "Iteration 28, loss = 0.46035161\n",
            "Iteration 29, loss = 0.45997380\n",
            "Iteration 30, loss = 0.45951384\n",
            "Iteration 31, loss = 0.45921594\n",
            "Iteration 32, loss = 0.45873982\n",
            "Iteration 33, loss = 0.45849612\n",
            "Iteration 34, loss = 0.45810773\n",
            "Iteration 35, loss = 0.45769965\n",
            "Iteration 36, loss = 0.45748630\n",
            "Iteration 37, loss = 0.45709476\n",
            "Iteration 38, loss = 0.45678014\n",
            "Iteration 39, loss = 0.45645916\n",
            "Iteration 40, loss = 0.45620317\n",
            "Iteration 41, loss = 0.45599321\n",
            "Iteration 42, loss = 0.45577201\n",
            "Iteration 43, loss = 0.45550876\n",
            "Iteration 44, loss = 0.45532656\n",
            "Iteration 45, loss = 0.45497215\n",
            "Iteration 46, loss = 0.45482518\n",
            "Iteration 47, loss = 0.45450570\n",
            "Iteration 48, loss = 0.45433456\n",
            "Iteration 49, loss = 0.45410494\n",
            "Iteration 50, loss = 0.45387352\n",
            "Iteration 51, loss = 0.45366419\n",
            "Iteration 52, loss = 0.45344326\n",
            "Iteration 53, loss = 0.45329638\n",
            "Iteration 54, loss = 0.45305678\n",
            "Iteration 55, loss = 0.45286300\n",
            "Iteration 56, loss = 0.45277206\n",
            "Iteration 57, loss = 0.45263249\n",
            "Iteration 58, loss = 0.45240003\n",
            "Iteration 59, loss = 0.45225279\n",
            "Iteration 60, loss = 0.45198900\n",
            "Iteration 61, loss = 0.45196589\n",
            "Iteration 62, loss = 0.45173815\n",
            "Iteration 63, loss = 0.45167469\n",
            "Iteration 64, loss = 0.45145490\n",
            "Iteration 65, loss = 0.45131296\n",
            "Iteration 66, loss = 0.45118068\n",
            "Iteration 67, loss = 0.45099778\n",
            "Iteration 68, loss = 0.45085889\n",
            "Iteration 69, loss = 0.45083188\n",
            "Iteration 70, loss = 0.45066787\n",
            "Iteration 71, loss = 0.45040916\n",
            "Iteration 72, loss = 0.45051347\n",
            "Iteration 73, loss = 0.45033800\n",
            "Iteration 74, loss = 0.45019251\n",
            "Iteration 75, loss = 0.45004092\n",
            "Iteration 76, loss = 0.44990947\n",
            "Iteration 77, loss = 0.44982942\n",
            "Iteration 78, loss = 0.44971791\n",
            "Iteration 79, loss = 0.44959579\n",
            "Iteration 80, loss = 0.44952583\n",
            "Iteration 81, loss = 0.44942536\n",
            "Iteration 82, loss = 0.44926274\n",
            "Iteration 83, loss = 0.44915381\n",
            "Iteration 84, loss = 0.44904328\n",
            "Iteration 85, loss = 0.44896835\n",
            "Iteration 86, loss = 0.44886457\n",
            "Iteration 87, loss = 0.44884559\n",
            "Iteration 88, loss = 0.44879407\n",
            "Iteration 89, loss = 0.44874583\n",
            "Iteration 90, loss = 0.44863210\n",
            "Iteration 91, loss = 0.44840950\n",
            "Iteration 92, loss = 0.44839308\n",
            "Iteration 93, loss = 0.44824091\n",
            "Iteration 94, loss = 0.44823928\n",
            "Iteration 95, loss = 0.44807546\n",
            "Iteration 96, loss = 0.44782805\n",
            "Iteration 97, loss = 0.44807796\n",
            "Iteration 98, loss = 0.44785219\n",
            "Iteration 99, loss = 0.44797927\n",
            "Iteration 100, loss = 0.44779885\n",
            "Iteration 101, loss = 0.44771158\n",
            "Iteration 102, loss = 0.44774885\n",
            "Iteration 103, loss = 0.44763450\n",
            "Iteration 104, loss = 0.44761603\n",
            "Iteration 105, loss = 0.44746109\n",
            "Iteration 106, loss = 0.44768224\n",
            "Iteration 107, loss = 0.44728512\n",
            "Iteration 108, loss = 0.44730221\n",
            "Iteration 109, loss = 0.44721597\n",
            "Iteration 110, loss = 0.44716548\n",
            "Iteration 111, loss = 0.44699888\n",
            "Iteration 112, loss = 0.44705235\n",
            "Iteration 113, loss = 0.44713347\n",
            "Iteration 114, loss = 0.44718408\n",
            "Iteration 115, loss = 0.44706679\n",
            "Iteration 116, loss = 0.44691511\n",
            "Iteration 117, loss = 0.44681465\n",
            "Iteration 118, loss = 0.44696365\n",
            "Iteration 119, loss = 0.44683437\n",
            "Iteration 120, loss = 0.44671111\n",
            "Iteration 121, loss = 0.44676167\n",
            "Iteration 122, loss = 0.44658375\n",
            "Iteration 123, loss = 0.44672605\n",
            "Iteration 124, loss = 0.44663512\n",
            "Iteration 125, loss = 0.44645203\n",
            "Iteration 126, loss = 0.44652538\n",
            "Iteration 127, loss = 0.44645439\n",
            "Iteration 128, loss = 0.44647440\n",
            "Iteration 129, loss = 0.44634428\n",
            "Iteration 130, loss = 0.44650727\n",
            "Iteration 131, loss = 0.44639290\n",
            "Iteration 132, loss = 0.44626438\n",
            "Iteration 133, loss = 0.44626433\n",
            "Iteration 134, loss = 0.44634732\n",
            "Iteration 135, loss = 0.44620565\n",
            "Iteration 136, loss = 0.44626508\n",
            "Iteration 137, loss = 0.44624426\n",
            "Iteration 138, loss = 0.44608964\n",
            "Iteration 139, loss = 0.44605158\n",
            "Iteration 140, loss = 0.44613713\n",
            "Iteration 141, loss = 0.44597304\n",
            "Iteration 142, loss = 0.44611088\n",
            "Iteration 143, loss = 0.44592144\n",
            "Iteration 144, loss = 0.44583878\n",
            "Iteration 145, loss = 0.44592990\n",
            "Iteration 146, loss = 0.44577592\n",
            "Iteration 147, loss = 0.44576843\n",
            "Iteration 148, loss = 0.44575193\n",
            "Iteration 149, loss = 0.44577484\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 150, loss = 0.44537830\n",
            "Iteration 151, loss = 0.44546255\n",
            "Iteration 152, loss = 0.44536214\n",
            "Iteration 153, loss = 0.44534986\n",
            "Iteration 154, loss = 0.44537900\n",
            "Iteration 155, loss = 0.44534840\n",
            "Iteration 156, loss = 0.44534632\n",
            "Iteration 157, loss = 0.44533208\n",
            "Iteration 158, loss = 0.44532942\n",
            "Iteration 159, loss = 0.44533019\n",
            "Iteration 160, loss = 0.44539034\n",
            "Iteration 161, loss = 0.44533179\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 162, loss = 0.44525976\n",
            "Iteration 163, loss = 0.44523412\n",
            "Iteration 164, loss = 0.44523274\n",
            "Iteration 165, loss = 0.44523098\n",
            "Iteration 166, loss = 0.44523532\n",
            "Iteration 167, loss = 0.44523237\n",
            "Iteration 168, loss = 0.44522581\n",
            "Iteration 169, loss = 0.44523067\n",
            "Iteration 170, loss = 0.44521927\n",
            "Iteration 171, loss = 0.44521954\n",
            "Iteration 172, loss = 0.44522250\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 173, loss = 0.44520287\n",
            "Iteration 174, loss = 0.44520236\n",
            "Iteration 175, loss = 0.44520266\n",
            "Iteration 176, loss = 0.44520196\n",
            "Iteration 177, loss = 0.44520159\n",
            "Iteration 178, loss = 0.44520047\n",
            "Iteration 179, loss = 0.44520130\n",
            "Iteration 180, loss = 0.44519893\n",
            "Iteration 181, loss = 0.44520041\n",
            "Iteration 182, loss = 0.44520061\n",
            "Iteration 183, loss = 0.44519982\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 184, loss = 0.44519521\n",
            "Iteration 185, loss = 0.44519532\n",
            "Iteration 186, loss = 0.44519489\n",
            "Iteration 187, loss = 0.44519509\n",
            "Iteration 188, loss = 0.44519528\n",
            "Iteration 189, loss = 0.44519484\n",
            "Iteration 190, loss = 0.44519459\n",
            "Iteration 191, loss = 0.44519477\n",
            "Iteration 192, loss = 0.44519492\n",
            "Iteration 193, loss = 0.44519442\n",
            "Iteration 194, loss = 0.44519490\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 195, loss = 0.44519382\n",
            "Iteration 196, loss = 0.44519378\n",
            "Iteration 197, loss = 0.44519372\n",
            "Iteration 198, loss = 0.44519381\n",
            "Iteration 199, loss = 0.44519378\n",
            "Iteration 200, loss = 0.44519370\n",
            "Iteration 1, loss = 0.68873268\n",
            "Iteration 2, loss = 0.65701133\n",
            "Iteration 3, loss = 0.60054739\n",
            "Iteration 4, loss = 0.53918649\n",
            "Iteration 5, loss = 0.50830516\n",
            "Iteration 6, loss = 0.49474455\n",
            "Iteration 7, loss = 0.48775794\n",
            "Iteration 8, loss = 0.48339677\n",
            "Iteration 9, loss = 0.48030207\n",
            "Iteration 10, loss = 0.47815010\n",
            "Iteration 11, loss = 0.47623093\n",
            "Iteration 12, loss = 0.47465337\n",
            "Iteration 13, loss = 0.47326334\n",
            "Iteration 14, loss = 0.47223556\n",
            "Iteration 15, loss = 0.47101351\n",
            "Iteration 16, loss = 0.46984936\n",
            "Iteration 17, loss = 0.46904063\n",
            "Iteration 18, loss = 0.46816569\n",
            "Iteration 19, loss = 0.46721567\n",
            "Iteration 20, loss = 0.46650411\n",
            "Iteration 21, loss = 0.46548981\n",
            "Iteration 22, loss = 0.46514730\n",
            "Iteration 23, loss = 0.46432901\n",
            "Iteration 24, loss = 0.46375854\n",
            "Iteration 25, loss = 0.46314331\n",
            "Iteration 26, loss = 0.46259277\n",
            "Iteration 27, loss = 0.46209759\n",
            "Iteration 28, loss = 0.46167104\n",
            "Iteration 29, loss = 0.46126774\n",
            "Iteration 30, loss = 0.46085732\n",
            "Iteration 31, loss = 0.46043620\n",
            "Iteration 32, loss = 0.46014213\n",
            "Iteration 33, loss = 0.45972087\n",
            "Iteration 34, loss = 0.45931917\n",
            "Iteration 35, loss = 0.45909108\n",
            "Iteration 36, loss = 0.45864572\n",
            "Iteration 37, loss = 0.45835451\n",
            "Iteration 38, loss = 0.45805466\n",
            "Iteration 39, loss = 0.45790247\n",
            "Iteration 40, loss = 0.45755687\n",
            "Iteration 41, loss = 0.45740458\n",
            "Iteration 42, loss = 0.45706368\n",
            "Iteration 43, loss = 0.45672815\n",
            "Iteration 44, loss = 0.45653775\n",
            "Iteration 45, loss = 0.45625499\n",
            "Iteration 46, loss = 0.45600348\n",
            "Iteration 47, loss = 0.45576768\n",
            "Iteration 48, loss = 0.45559595\n",
            "Iteration 49, loss = 0.45537109\n",
            "Iteration 50, loss = 0.45531951\n",
            "Iteration 51, loss = 0.45495341\n",
            "Iteration 52, loss = 0.45477853\n",
            "Iteration 53, loss = 0.45451872\n",
            "Iteration 54, loss = 0.45422045\n",
            "Iteration 55, loss = 0.45422239\n",
            "Iteration 56, loss = 0.45392715\n",
            "Iteration 57, loss = 0.45391312\n",
            "Iteration 58, loss = 0.45352756\n",
            "Iteration 59, loss = 0.45336989\n",
            "Iteration 60, loss = 0.45324941\n",
            "Iteration 61, loss = 0.45308327\n",
            "Iteration 62, loss = 0.45294920\n",
            "Iteration 63, loss = 0.45276600\n",
            "Iteration 64, loss = 0.45253775\n",
            "Iteration 65, loss = 0.45248883\n",
            "Iteration 66, loss = 0.45238898\n",
            "Iteration 67, loss = 0.45220863\n",
            "Iteration 68, loss = 0.45203979\n",
            "Iteration 69, loss = 0.45203581\n",
            "Iteration 70, loss = 0.45175371\n",
            "Iteration 71, loss = 0.45168055\n",
            "Iteration 72, loss = 0.45164022\n",
            "Iteration 73, loss = 0.45124802\n",
            "Iteration 74, loss = 0.45131862\n",
            "Iteration 75, loss = 0.45116473\n",
            "Iteration 76, loss = 0.45115507\n",
            "Iteration 77, loss = 0.45088450\n",
            "Iteration 78, loss = 0.45073179\n",
            "Iteration 79, loss = 0.45076682\n",
            "Iteration 80, loss = 0.45058789\n",
            "Iteration 81, loss = 0.45045083\n",
            "Iteration 82, loss = 0.45042084\n",
            "Iteration 83, loss = 0.45027881\n",
            "Iteration 84, loss = 0.45033856\n",
            "Iteration 85, loss = 0.45018248\n",
            "Iteration 86, loss = 0.45010627\n",
            "Iteration 87, loss = 0.44995584\n",
            "Iteration 88, loss = 0.45007583\n",
            "Iteration 89, loss = 0.44986871\n",
            "Iteration 90, loss = 0.44985423\n",
            "Iteration 91, loss = 0.44965003\n",
            "Iteration 92, loss = 0.44971075\n",
            "Iteration 93, loss = 0.44964767\n",
            "Iteration 94, loss = 0.44954787\n",
            "Iteration 95, loss = 0.44953174\n",
            "Iteration 96, loss = 0.44949094\n",
            "Iteration 97, loss = 0.44929872\n",
            "Iteration 98, loss = 0.44921156\n",
            "Iteration 99, loss = 0.44913770\n",
            "Iteration 100, loss = 0.44921930\n",
            "Iteration 101, loss = 0.44901412\n",
            "Iteration 102, loss = 0.44907991\n",
            "Iteration 103, loss = 0.44892976\n",
            "Iteration 104, loss = 0.44900316\n",
            "Iteration 105, loss = 0.44892295\n",
            "Iteration 106, loss = 0.44868662\n",
            "Iteration 107, loss = 0.44854735\n",
            "Iteration 108, loss = 0.44860811\n",
            "Iteration 109, loss = 0.44860708\n",
            "Iteration 110, loss = 0.44855777\n",
            "Iteration 111, loss = 0.44853589\n",
            "Iteration 112, loss = 0.44864329\n",
            "Iteration 113, loss = 0.44848184\n",
            "Iteration 114, loss = 0.44838107\n",
            "Iteration 115, loss = 0.44857528\n",
            "Iteration 116, loss = 0.44833756\n",
            "Iteration 117, loss = 0.44813447\n",
            "Iteration 118, loss = 0.44827411\n",
            "Iteration 119, loss = 0.44820092\n",
            "Iteration 120, loss = 0.44804055\n",
            "Iteration 121, loss = 0.44798907\n",
            "Iteration 122, loss = 0.44822542\n",
            "Iteration 123, loss = 0.44803155\n",
            "Iteration 124, loss = 0.44802842\n",
            "Iteration 125, loss = 0.44780054\n",
            "Iteration 126, loss = 0.44795634\n",
            "Iteration 127, loss = 0.44783071\n",
            "Iteration 128, loss = 0.44793670\n",
            "Iteration 129, loss = 0.44789812\n",
            "Iteration 130, loss = 0.44775747\n",
            "Iteration 131, loss = 0.44759100\n",
            "Iteration 132, loss = 0.44764220\n",
            "Iteration 133, loss = 0.44772708\n",
            "Iteration 134, loss = 0.44760237\n",
            "Iteration 135, loss = 0.44764519\n",
            "Iteration 136, loss = 0.44752590\n",
            "Iteration 137, loss = 0.44764198\n",
            "Iteration 138, loss = 0.44737737\n",
            "Iteration 139, loss = 0.44730891\n",
            "Iteration 140, loss = 0.44726178\n",
            "Iteration 141, loss = 0.44731189\n",
            "Iteration 142, loss = 0.44727100\n",
            "Iteration 143, loss = 0.44724448\n",
            "Iteration 144, loss = 0.44712997\n",
            "Iteration 145, loss = 0.44717994\n",
            "Iteration 146, loss = 0.44713259\n",
            "Iteration 147, loss = 0.44711915\n",
            "Iteration 148, loss = 0.44720191\n",
            "Iteration 149, loss = 0.44738120\n",
            "Iteration 150, loss = 0.44711603\n",
            "Iteration 151, loss = 0.44705229\n",
            "Iteration 152, loss = 0.44719442\n",
            "Iteration 153, loss = 0.44702644\n",
            "Iteration 154, loss = 0.44698099\n",
            "Iteration 155, loss = 0.44701534\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 156, loss = 0.44665424\n",
            "Iteration 157, loss = 0.44665769\n",
            "Iteration 158, loss = 0.44661551\n",
            "Iteration 159, loss = 0.44662498\n",
            "Iteration 160, loss = 0.44665285\n",
            "Iteration 161, loss = 0.44661986\n",
            "Iteration 162, loss = 0.44658626\n",
            "Iteration 163, loss = 0.44659482\n",
            "Iteration 164, loss = 0.44662685\n",
            "Iteration 165, loss = 0.44656803\n",
            "Iteration 166, loss = 0.44656203\n",
            "Iteration 167, loss = 0.44655226\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 168, loss = 0.44653111\n",
            "Iteration 169, loss = 0.44647514\n",
            "Iteration 170, loss = 0.44648093\n",
            "Iteration 171, loss = 0.44647054\n",
            "Iteration 172, loss = 0.44647461\n",
            "Iteration 173, loss = 0.44647369\n",
            "Iteration 174, loss = 0.44646195\n",
            "Iteration 175, loss = 0.44646465\n",
            "Iteration 176, loss = 0.44646388\n",
            "Iteration 177, loss = 0.44646325\n",
            "Iteration 178, loss = 0.44647072\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 179, loss = 0.44644304\n",
            "Iteration 180, loss = 0.44644287\n",
            "Iteration 181, loss = 0.44644241\n",
            "Iteration 182, loss = 0.44644328\n",
            "Iteration 183, loss = 0.44644240\n",
            "Iteration 184, loss = 0.44644267\n",
            "Iteration 185, loss = 0.44644200\n",
            "Iteration 186, loss = 0.44644159\n",
            "Iteration 187, loss = 0.44644123\n",
            "Iteration 188, loss = 0.44644071\n",
            "Iteration 189, loss = 0.44644118\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 190, loss = 0.44643665\n",
            "Iteration 191, loss = 0.44643640\n",
            "Iteration 192, loss = 0.44643657\n",
            "Iteration 193, loss = 0.44643610\n",
            "Iteration 194, loss = 0.44643633\n",
            "Iteration 195, loss = 0.44643618\n",
            "Iteration 196, loss = 0.44643620\n",
            "Iteration 197, loss = 0.44643598\n",
            "Iteration 198, loss = 0.44643621\n",
            "Iteration 199, loss = 0.44643612\n",
            "Iteration 200, loss = 0.44643595\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 1, loss = 0.68880221\n",
            "Iteration 2, loss = 0.65783438\n",
            "Iteration 3, loss = 0.60474341\n",
            "Iteration 4, loss = 0.54362935\n",
            "Iteration 5, loss = 0.51049651\n",
            "Iteration 6, loss = 0.49559406\n",
            "Iteration 7, loss = 0.48835279\n",
            "Iteration 8, loss = 0.48390678\n",
            "Iteration 9, loss = 0.48068373\n",
            "Iteration 10, loss = 0.47843936\n",
            "Iteration 11, loss = 0.47659518\n",
            "Iteration 12, loss = 0.47504741\n",
            "Iteration 13, loss = 0.47369477\n",
            "Iteration 14, loss = 0.47256214\n",
            "Iteration 15, loss = 0.47148358\n",
            "Iteration 16, loss = 0.47053138\n",
            "Iteration 17, loss = 0.46964763\n",
            "Iteration 18, loss = 0.46876307\n",
            "Iteration 19, loss = 0.46799662\n",
            "Iteration 20, loss = 0.46731566\n",
            "Iteration 21, loss = 0.46656236\n",
            "Iteration 22, loss = 0.46586647\n",
            "Iteration 23, loss = 0.46521425\n",
            "Iteration 24, loss = 0.46469801\n",
            "Iteration 25, loss = 0.46405981\n",
            "Iteration 26, loss = 0.46349966\n",
            "Iteration 27, loss = 0.46291196\n",
            "Iteration 28, loss = 0.46245642\n",
            "Iteration 29, loss = 0.46196655\n",
            "Iteration 30, loss = 0.46153211\n",
            "Iteration 31, loss = 0.46122414\n",
            "Iteration 32, loss = 0.46084366\n",
            "Iteration 33, loss = 0.46040787\n",
            "Iteration 34, loss = 0.45991313\n",
            "Iteration 35, loss = 0.45977365\n",
            "Iteration 36, loss = 0.45938663\n",
            "Iteration 37, loss = 0.45907497\n",
            "Iteration 38, loss = 0.45873938\n",
            "Iteration 39, loss = 0.45842547\n",
            "Iteration 40, loss = 0.45818036\n",
            "Iteration 41, loss = 0.45801559\n",
            "Iteration 42, loss = 0.45765815\n",
            "Iteration 43, loss = 0.45735197\n",
            "Iteration 44, loss = 0.45717603\n",
            "Iteration 45, loss = 0.45689372\n",
            "Iteration 46, loss = 0.45672772\n",
            "Iteration 47, loss = 0.45643656\n",
            "Iteration 48, loss = 0.45625350\n",
            "Iteration 49, loss = 0.45604522\n",
            "Iteration 50, loss = 0.45583662\n",
            "Iteration 51, loss = 0.45553637\n",
            "Iteration 52, loss = 0.45539756\n",
            "Iteration 53, loss = 0.45521802\n",
            "Iteration 54, loss = 0.45505335\n",
            "Iteration 55, loss = 0.45481340\n",
            "Iteration 56, loss = 0.45473875\n",
            "Iteration 57, loss = 0.45450340\n",
            "Iteration 58, loss = 0.45438460\n",
            "Iteration 59, loss = 0.45414814\n",
            "Iteration 60, loss = 0.45391206\n",
            "Iteration 61, loss = 0.45384948\n",
            "Iteration 62, loss = 0.45355343\n",
            "Iteration 63, loss = 0.45345486\n",
            "Iteration 64, loss = 0.45351528\n",
            "Iteration 65, loss = 0.45328396\n",
            "Iteration 66, loss = 0.45311361\n",
            "Iteration 67, loss = 0.45289057\n",
            "Iteration 68, loss = 0.45298725\n",
            "Iteration 69, loss = 0.45263205\n",
            "Iteration 70, loss = 0.45255276\n",
            "Iteration 71, loss = 0.45242752\n",
            "Iteration 72, loss = 0.45230281\n",
            "Iteration 73, loss = 0.45211655\n",
            "Iteration 74, loss = 0.45210546\n",
            "Iteration 75, loss = 0.45188028\n",
            "Iteration 76, loss = 0.45189012\n",
            "Iteration 77, loss = 0.45163810\n",
            "Iteration 78, loss = 0.45147453\n",
            "Iteration 79, loss = 0.45144982\n",
            "Iteration 80, loss = 0.45131279\n",
            "Iteration 81, loss = 0.45117542\n",
            "Iteration 82, loss = 0.45130432\n",
            "Iteration 83, loss = 0.45113726\n",
            "Iteration 84, loss = 0.45096423\n",
            "Iteration 85, loss = 0.45086864\n",
            "Iteration 86, loss = 0.45073494\n",
            "Iteration 87, loss = 0.45066992\n",
            "Iteration 88, loss = 0.45057838\n",
            "Iteration 89, loss = 0.45055398\n",
            "Iteration 90, loss = 0.45037212\n",
            "Iteration 91, loss = 0.45039604\n",
            "Iteration 92, loss = 0.45023470\n",
            "Iteration 93, loss = 0.45030521\n",
            "Iteration 94, loss = 0.45007529\n",
            "Iteration 95, loss = 0.45010034\n",
            "Iteration 96, loss = 0.45000185\n",
            "Iteration 97, loss = 0.44996670\n",
            "Iteration 98, loss = 0.44988146\n",
            "Iteration 99, loss = 0.44975404\n",
            "Iteration 100, loss = 0.44976746\n",
            "Iteration 101, loss = 0.44968564\n",
            "Iteration 102, loss = 0.44960333\n",
            "Iteration 103, loss = 0.44952138\n",
            "Iteration 104, loss = 0.44952575\n",
            "Iteration 105, loss = 0.44942591\n",
            "Iteration 106, loss = 0.44937613\n",
            "Iteration 107, loss = 0.44927121\n",
            "Iteration 108, loss = 0.44919344\n",
            "Iteration 109, loss = 0.44906697\n",
            "Iteration 110, loss = 0.44903652\n",
            "Iteration 111, loss = 0.44892722\n",
            "Iteration 112, loss = 0.44894441\n",
            "Iteration 113, loss = 0.44897637\n",
            "Iteration 114, loss = 0.44890593\n",
            "Iteration 115, loss = 0.44890295\n",
            "Iteration 116, loss = 0.44880923\n",
            "Iteration 117, loss = 0.44876586\n",
            "Iteration 118, loss = 0.44871122\n",
            "Iteration 119, loss = 0.44873038\n",
            "Iteration 120, loss = 0.44850313\n",
            "Iteration 121, loss = 0.44853118\n",
            "Iteration 122, loss = 0.44863147\n",
            "Iteration 123, loss = 0.44847119\n",
            "Iteration 124, loss = 0.44840097\n",
            "Iteration 125, loss = 0.44843939\n",
            "Iteration 126, loss = 0.44831997\n",
            "Iteration 127, loss = 0.44819317\n",
            "Iteration 128, loss = 0.44832515\n",
            "Iteration 129, loss = 0.44816885\n",
            "Iteration 130, loss = 0.44816414\n",
            "Iteration 131, loss = 0.44805338\n",
            "Iteration 132, loss = 0.44819901\n",
            "Iteration 133, loss = 0.44807311\n",
            "Iteration 134, loss = 0.44798577\n",
            "Iteration 135, loss = 0.44816250\n",
            "Iteration 136, loss = 0.44797733\n",
            "Iteration 137, loss = 0.44775727\n",
            "Iteration 138, loss = 0.44791699\n",
            "Iteration 139, loss = 0.44794089\n",
            "Iteration 140, loss = 0.44783462\n",
            "Iteration 141, loss = 0.44776527\n",
            "Iteration 142, loss = 0.44766295\n",
            "Iteration 143, loss = 0.44769315\n",
            "Iteration 144, loss = 0.44774174\n",
            "Iteration 145, loss = 0.44770331\n",
            "Iteration 146, loss = 0.44765741\n",
            "Iteration 147, loss = 0.44766219\n",
            "Iteration 148, loss = 0.44764896\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 149, loss = 0.44723214\n",
            "Iteration 150, loss = 0.44719052\n",
            "Iteration 151, loss = 0.44719406\n",
            "Iteration 152, loss = 0.44720670\n",
            "Iteration 153, loss = 0.44722566\n",
            "Iteration 154, loss = 0.44719875\n",
            "Iteration 155, loss = 0.44718079\n",
            "Iteration 156, loss = 0.44715354\n",
            "Iteration 157, loss = 0.44714446\n",
            "Iteration 158, loss = 0.44712521\n",
            "Iteration 159, loss = 0.44711152\n",
            "Iteration 160, loss = 0.44712095\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 161, loss = 0.44706902\n",
            "Iteration 162, loss = 0.44704169\n",
            "Iteration 163, loss = 0.44704758\n",
            "Iteration 164, loss = 0.44703604\n",
            "Iteration 165, loss = 0.44703419\n",
            "Iteration 166, loss = 0.44703243\n",
            "Iteration 167, loss = 0.44703944\n",
            "Iteration 168, loss = 0.44703312\n",
            "Iteration 169, loss = 0.44702736\n",
            "Iteration 170, loss = 0.44703332\n",
            "Iteration 171, loss = 0.44702527\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 172, loss = 0.44700806\n",
            "Iteration 173, loss = 0.44700828\n",
            "Iteration 174, loss = 0.44700677\n",
            "Iteration 175, loss = 0.44700669\n",
            "Iteration 176, loss = 0.44700639\n",
            "Iteration 177, loss = 0.44700669\n",
            "Iteration 178, loss = 0.44700980\n",
            "Iteration 179, loss = 0.44700438\n",
            "Iteration 180, loss = 0.44700855\n",
            "Iteration 181, loss = 0.44700579\n",
            "Iteration 182, loss = 0.44700626\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 183, loss = 0.44700162\n",
            "Iteration 184, loss = 0.44700118\n",
            "Iteration 185, loss = 0.44700116\n",
            "Iteration 186, loss = 0.44700128\n",
            "Iteration 187, loss = 0.44700119\n",
            "Iteration 188, loss = 0.44700115\n",
            "Iteration 189, loss = 0.44700096\n",
            "Iteration 190, loss = 0.44700107\n",
            "Iteration 191, loss = 0.44700098\n",
            "Iteration 192, loss = 0.44700053\n",
            "Iteration 193, loss = 0.44700058\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 194, loss = 0.44699998\n",
            "Iteration 195, loss = 0.44699989\n",
            "Iteration 196, loss = 0.44699986\n",
            "Iteration 197, loss = 0.44699986\n",
            "Iteration 198, loss = 0.44699987\n",
            "Iteration 199, loss = 0.44699983\n",
            "Iteration 200, loss = 0.44699983\n",
            "Iteration 1, loss = 0.68879050\n",
            "Iteration 2, loss = 0.65819793\n",
            "Iteration 3, loss = 0.60777539\n",
            "Iteration 4, loss = 0.54814483\n",
            "Iteration 5, loss = 0.51298676\n",
            "Iteration 6, loss = 0.49704982\n",
            "Iteration 7, loss = 0.48908135\n",
            "Iteration 8, loss = 0.48411854\n",
            "Iteration 9, loss = 0.48091168\n",
            "Iteration 10, loss = 0.47839867\n",
            "Iteration 11, loss = 0.47642793\n",
            "Iteration 12, loss = 0.47474697\n",
            "Iteration 13, loss = 0.47336346\n",
            "Iteration 14, loss = 0.47207422\n",
            "Iteration 15, loss = 0.47091523\n",
            "Iteration 16, loss = 0.46983469\n",
            "Iteration 17, loss = 0.46895588\n",
            "Iteration 18, loss = 0.46799250\n",
            "Iteration 19, loss = 0.46716046\n",
            "Iteration 20, loss = 0.46644546\n",
            "Iteration 21, loss = 0.46555455\n",
            "Iteration 22, loss = 0.46488812\n",
            "Iteration 23, loss = 0.46435079\n",
            "Iteration 24, loss = 0.46374366\n",
            "Iteration 25, loss = 0.46321119\n",
            "Iteration 26, loss = 0.46266766\n",
            "Iteration 27, loss = 0.46222489\n",
            "Iteration 28, loss = 0.46180654\n",
            "Iteration 29, loss = 0.46139629\n",
            "Iteration 30, loss = 0.46087448\n",
            "Iteration 31, loss = 0.46064142\n",
            "Iteration 32, loss = 0.46021280\n",
            "Iteration 33, loss = 0.45977383\n",
            "Iteration 34, loss = 0.45937223\n",
            "Iteration 35, loss = 0.45908899\n",
            "Iteration 36, loss = 0.45876035\n",
            "Iteration 37, loss = 0.45847304\n",
            "Iteration 38, loss = 0.45818650\n",
            "Iteration 39, loss = 0.45784508\n",
            "Iteration 40, loss = 0.45763046\n",
            "Iteration 41, loss = 0.45729087\n",
            "Iteration 42, loss = 0.45707105\n",
            "Iteration 43, loss = 0.45677372\n",
            "Iteration 44, loss = 0.45649337\n",
            "Iteration 45, loss = 0.45633429\n",
            "Iteration 46, loss = 0.45597038\n",
            "Iteration 47, loss = 0.45591045\n",
            "Iteration 48, loss = 0.45567735\n",
            "Iteration 49, loss = 0.45535587\n",
            "Iteration 50, loss = 0.45518355\n",
            "Iteration 51, loss = 0.45496950\n",
            "Iteration 52, loss = 0.45468709\n",
            "Iteration 53, loss = 0.45444008\n",
            "Iteration 54, loss = 0.45432069\n",
            "Iteration 55, loss = 0.45417150\n",
            "Iteration 56, loss = 0.45391731\n",
            "Iteration 57, loss = 0.45370115\n",
            "Iteration 58, loss = 0.45346602\n",
            "Iteration 59, loss = 0.45358203\n",
            "Iteration 60, loss = 0.45340327\n",
            "Iteration 61, loss = 0.45302221\n",
            "Iteration 62, loss = 0.45291111\n",
            "Iteration 63, loss = 0.45262924\n",
            "Iteration 64, loss = 0.45259479\n",
            "Iteration 65, loss = 0.45238015\n",
            "Iteration 66, loss = 0.45226653\n",
            "Iteration 67, loss = 0.45215362\n",
            "Iteration 68, loss = 0.45195820\n",
            "Iteration 69, loss = 0.45193053\n",
            "Iteration 70, loss = 0.45166244\n",
            "Iteration 71, loss = 0.45158945\n",
            "Iteration 72, loss = 0.45138432\n",
            "Iteration 73, loss = 0.45132808\n",
            "Iteration 74, loss = 0.45112893\n",
            "Iteration 75, loss = 0.45111671\n",
            "Iteration 76, loss = 0.45093679\n",
            "Iteration 77, loss = 0.45079383\n",
            "Iteration 78, loss = 0.45076086\n",
            "Iteration 79, loss = 0.45053327\n",
            "Iteration 80, loss = 0.45056984\n",
            "Iteration 81, loss = 0.45034267\n",
            "Iteration 82, loss = 0.45031887\n",
            "Iteration 83, loss = 0.45016555\n",
            "Iteration 84, loss = 0.45015039\n",
            "Iteration 85, loss = 0.45004385\n",
            "Iteration 86, loss = 0.44990039\n",
            "Iteration 87, loss = 0.44984039\n",
            "Iteration 88, loss = 0.44973602\n",
            "Iteration 89, loss = 0.44957328\n",
            "Iteration 90, loss = 0.44955510\n",
            "Iteration 91, loss = 0.44973178\n",
            "Iteration 92, loss = 0.44938995\n",
            "Iteration 93, loss = 0.44937234\n",
            "Iteration 94, loss = 0.44942368\n",
            "Iteration 95, loss = 0.44912507\n",
            "Iteration 96, loss = 0.44907426\n",
            "Iteration 97, loss = 0.44915208\n",
            "Iteration 98, loss = 0.44919684\n",
            "Iteration 99, loss = 0.44884383\n",
            "Iteration 100, loss = 0.44914251\n",
            "Iteration 101, loss = 0.44881851\n",
            "Iteration 102, loss = 0.44891706\n",
            "Iteration 103, loss = 0.44878054\n",
            "Iteration 104, loss = 0.44868712\n",
            "Iteration 105, loss = 0.44878635\n",
            "Iteration 106, loss = 0.44844014\n",
            "Iteration 107, loss = 0.44844932\n",
            "Iteration 108, loss = 0.44834262\n",
            "Iteration 109, loss = 0.44838037\n",
            "Iteration 110, loss = 0.44816564\n",
            "Iteration 111, loss = 0.44814368\n",
            "Iteration 112, loss = 0.44824793\n",
            "Iteration 113, loss = 0.44807585\n",
            "Iteration 114, loss = 0.44810009\n",
            "Iteration 115, loss = 0.44804638\n",
            "Iteration 116, loss = 0.44798602\n",
            "Iteration 117, loss = 0.44799033\n",
            "Iteration 118, loss = 0.44770280\n",
            "Iteration 119, loss = 0.44785291\n",
            "Iteration 120, loss = 0.44777239\n",
            "Iteration 121, loss = 0.44777505\n",
            "Iteration 122, loss = 0.44780245\n",
            "Iteration 123, loss = 0.44767907\n",
            "Iteration 124, loss = 0.44758070\n",
            "Iteration 125, loss = 0.44741591\n",
            "Iteration 126, loss = 0.44757107\n",
            "Iteration 127, loss = 0.44755490\n",
            "Iteration 128, loss = 0.44739352\n",
            "Iteration 129, loss = 0.44740037\n",
            "Iteration 130, loss = 0.44746593\n",
            "Iteration 131, loss = 0.44745574\n",
            "Iteration 132, loss = 0.44725682\n",
            "Iteration 133, loss = 0.44729061\n",
            "Iteration 134, loss = 0.44712907\n",
            "Iteration 135, loss = 0.44723116\n",
            "Iteration 136, loss = 0.44726009\n",
            "Iteration 137, loss = 0.44708628\n",
            "Iteration 138, loss = 0.44703768\n",
            "Iteration 139, loss = 0.44709657\n",
            "Iteration 140, loss = 0.44700688\n",
            "Iteration 141, loss = 0.44708577\n",
            "Iteration 142, loss = 0.44704196\n",
            "Iteration 143, loss = 0.44698026\n",
            "Iteration 144, loss = 0.44693490\n",
            "Iteration 145, loss = 0.44688528\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 146, loss = 0.44654245\n",
            "Iteration 147, loss = 0.44652306\n",
            "Iteration 148, loss = 0.44655506\n",
            "Iteration 149, loss = 0.44647672\n",
            "Iteration 150, loss = 0.44650535\n",
            "Iteration 151, loss = 0.44647701\n",
            "Iteration 152, loss = 0.44646570\n",
            "Iteration 153, loss = 0.44648297\n",
            "Iteration 154, loss = 0.44649220\n",
            "Iteration 155, loss = 0.44647873\n",
            "Iteration 156, loss = 0.44643908\n",
            "Iteration 157, loss = 0.44645657\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 158, loss = 0.44636297\n",
            "Iteration 159, loss = 0.44636044\n",
            "Iteration 160, loss = 0.44635255\n",
            "Iteration 161, loss = 0.44636145\n",
            "Iteration 162, loss = 0.44635157\n",
            "Iteration 163, loss = 0.44634933\n",
            "Iteration 164, loss = 0.44634780\n",
            "Iteration 165, loss = 0.44634545\n",
            "Iteration 166, loss = 0.44635495\n",
            "Iteration 167, loss = 0.44634909\n",
            "Iteration 168, loss = 0.44634738\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 169, loss = 0.44632744\n",
            "Iteration 170, loss = 0.44632621\n",
            "Iteration 171, loss = 0.44632404\n",
            "Iteration 172, loss = 0.44632606\n",
            "Iteration 173, loss = 0.44632680\n",
            "Iteration 174, loss = 0.44632363\n",
            "Iteration 175, loss = 0.44632316\n",
            "Iteration 176, loss = 0.44632479\n",
            "Iteration 177, loss = 0.44632362\n",
            "Iteration 178, loss = 0.44632256\n",
            "Iteration 179, loss = 0.44632320\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 180, loss = 0.44631846\n",
            "Iteration 181, loss = 0.44631826\n",
            "Iteration 182, loss = 0.44631840\n",
            "Iteration 183, loss = 0.44631810\n",
            "Iteration 184, loss = 0.44631823\n",
            "Iteration 185, loss = 0.44631825\n",
            "Iteration 186, loss = 0.44631782\n",
            "Iteration 187, loss = 0.44631797\n",
            "Iteration 188, loss = 0.44631769\n",
            "Iteration 189, loss = 0.44631799\n",
            "Iteration 190, loss = 0.44631828\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 191, loss = 0.44631706\n",
            "Iteration 192, loss = 0.44631699\n",
            "Iteration 193, loss = 0.44631688\n",
            "Iteration 194, loss = 0.44631688\n",
            "Iteration 195, loss = 0.44631689\n",
            "Iteration 196, loss = 0.44631685\n",
            "Iteration 197, loss = 0.44631686\n",
            "Iteration 198, loss = 0.44631688\n",
            "Iteration 199, loss = 0.44631679\n",
            "Iteration 200, loss = 0.44631680\n",
            "[-0.85132634 -0.88957434 -0.89450956 -0.8624306  -0.8636644  -0.86119679\n",
            " -0.90931524 -0.85256015 -0.87600247 -0.89327576]\n",
            "Iteration 1, loss = 0.70378210\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  4.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 2, loss = 0.69824572\n",
            "Iteration 3, loss = 0.69275587\n",
            "Iteration 4, loss = 0.68772631\n",
            "Iteration 5, loss = 0.68348064\n",
            "Iteration 6, loss = 0.67945942\n",
            "Iteration 7, loss = 0.67532180\n",
            "Iteration 8, loss = 0.67101083\n",
            "Iteration 9, loss = 0.66635045\n",
            "Iteration 10, loss = 0.66120602\n",
            "Iteration 11, loss = 0.65558460\n",
            "Iteration 12, loss = 0.64921312\n",
            "Iteration 13, loss = 0.64221801\n",
            "Iteration 14, loss = 0.63437610\n",
            "Iteration 15, loss = 0.62577193\n",
            "Iteration 16, loss = 0.61652151\n",
            "Iteration 17, loss = 0.60663302\n",
            "Iteration 18, loss = 0.59631667\n",
            "Iteration 19, loss = 0.58570345\n",
            "Iteration 20, loss = 0.57531539\n",
            "Iteration 21, loss = 0.56508518\n",
            "Iteration 22, loss = 0.55544467\n",
            "Iteration 23, loss = 0.54687363\n",
            "Iteration 24, loss = 0.53911591\n",
            "Iteration 25, loss = 0.53229229\n",
            "Iteration 26, loss = 0.52616585\n",
            "Iteration 27, loss = 0.52068263\n",
            "Iteration 28, loss = 0.51594331\n",
            "Iteration 29, loss = 0.51183217\n",
            "Iteration 30, loss = 0.50834367\n",
            "Iteration 31, loss = 0.50503169\n",
            "Iteration 32, loss = 0.50226651\n",
            "Iteration 33, loss = 0.49979823\n",
            "Iteration 34, loss = 0.49769233\n",
            "Iteration 35, loss = 0.49582817\n",
            "Iteration 36, loss = 0.49411360\n",
            "Iteration 37, loss = 0.49255313\n",
            "Iteration 38, loss = 0.49121679\n",
            "Iteration 39, loss = 0.49008961\n",
            "Iteration 40, loss = 0.48893666\n",
            "Iteration 41, loss = 0.48785318\n",
            "Iteration 42, loss = 0.48687658\n",
            "Iteration 43, loss = 0.48609681\n",
            "Iteration 44, loss = 0.48525903\n",
            "Iteration 45, loss = 0.48453131\n",
            "Iteration 46, loss = 0.48378439\n",
            "Iteration 47, loss = 0.48317843\n",
            "Iteration 48, loss = 0.48243529\n",
            "Iteration 49, loss = 0.48189812\n",
            "Iteration 50, loss = 0.48124666\n",
            "Iteration 51, loss = 0.48064881\n",
            "Iteration 52, loss = 0.48023218\n",
            "Iteration 53, loss = 0.47967259\n",
            "Iteration 54, loss = 0.47917732\n",
            "Iteration 55, loss = 0.47880357\n",
            "Iteration 56, loss = 0.47830439\n",
            "Iteration 57, loss = 0.47789520\n",
            "Iteration 58, loss = 0.47741514\n",
            "Iteration 59, loss = 0.47709059\n",
            "Iteration 60, loss = 0.47690099\n",
            "Iteration 61, loss = 0.47633976\n",
            "Iteration 62, loss = 0.47602732\n",
            "Iteration 63, loss = 0.47575383\n",
            "Iteration 64, loss = 0.47545371\n",
            "Iteration 65, loss = 0.47510984\n",
            "Iteration 66, loss = 0.47474971\n",
            "Iteration 67, loss = 0.47453221\n",
            "Iteration 68, loss = 0.47417443\n",
            "Iteration 69, loss = 0.47399952\n",
            "Iteration 70, loss = 0.47366902\n",
            "Iteration 71, loss = 0.47344192\n",
            "Iteration 72, loss = 0.47316670\n",
            "Iteration 73, loss = 0.47290861\n",
            "Iteration 74, loss = 0.47273601\n",
            "Iteration 75, loss = 0.47240142\n",
            "Iteration 76, loss = 0.47209865\n",
            "Iteration 77, loss = 0.47206940\n",
            "Iteration 78, loss = 0.47160919\n",
            "Iteration 79, loss = 0.47123520\n",
            "Iteration 80, loss = 0.47096190\n",
            "Iteration 81, loss = 0.47062718\n",
            "Iteration 82, loss = 0.47041082\n",
            "Iteration 83, loss = 0.47020569\n",
            "Iteration 84, loss = 0.46985596\n",
            "Iteration 85, loss = 0.46963797\n",
            "Iteration 86, loss = 0.46946713\n",
            "Iteration 87, loss = 0.46922223\n",
            "Iteration 88, loss = 0.46887238\n",
            "Iteration 89, loss = 0.46866449\n",
            "Iteration 90, loss = 0.46844808\n",
            "Iteration 91, loss = 0.46826278\n",
            "Iteration 92, loss = 0.46809855\n",
            "Iteration 93, loss = 0.46778558\n",
            "Iteration 94, loss = 0.46764052\n",
            "Iteration 95, loss = 0.46748688\n",
            "Iteration 96, loss = 0.46722560\n",
            "Iteration 97, loss = 0.46706850\n",
            "Iteration 98, loss = 0.46692972\n",
            "Iteration 99, loss = 0.46675777\n",
            "Iteration 100, loss = 0.46645101\n",
            "Iteration 101, loss = 0.46626820\n",
            "Iteration 102, loss = 0.46613404\n",
            "Iteration 103, loss = 0.46591769\n",
            "Iteration 104, loss = 0.46581894\n",
            "Iteration 105, loss = 0.46555108\n",
            "Iteration 106, loss = 0.46538406\n",
            "Iteration 107, loss = 0.46521089\n",
            "Iteration 108, loss = 0.46525360\n",
            "Iteration 109, loss = 0.46511319\n",
            "Iteration 110, loss = 0.46470016\n",
            "Iteration 111, loss = 0.46458117\n",
            "Iteration 112, loss = 0.46443856\n",
            "Iteration 113, loss = 0.46425023\n",
            "Iteration 114, loss = 0.46403867\n",
            "Iteration 115, loss = 0.46408195\n",
            "Iteration 116, loss = 0.46380829\n",
            "Iteration 117, loss = 0.46384302\n",
            "Iteration 118, loss = 0.46357315\n",
            "Iteration 119, loss = 0.46351895\n",
            "Iteration 120, loss = 0.46318443\n",
            "Iteration 121, loss = 0.46298946\n",
            "Iteration 122, loss = 0.46285084\n",
            "Iteration 123, loss = 0.46276563\n",
            "Iteration 124, loss = 0.46260423\n",
            "Iteration 125, loss = 0.46244179\n",
            "Iteration 126, loss = 0.46230476\n",
            "Iteration 127, loss = 0.46211937\n",
            "Iteration 128, loss = 0.46213839\n",
            "Iteration 129, loss = 0.46195062\n",
            "Iteration 130, loss = 0.46165101\n",
            "Iteration 131, loss = 0.46150582\n",
            "Iteration 132, loss = 0.46141288\n",
            "Iteration 133, loss = 0.46118097\n",
            "Iteration 134, loss = 0.46114508\n",
            "Iteration 135, loss = 0.46096637\n",
            "Iteration 136, loss = 0.46081191\n",
            "Iteration 137, loss = 0.46064433\n",
            "Iteration 138, loss = 0.46063670\n",
            "Iteration 139, loss = 0.46037891\n",
            "Iteration 140, loss = 0.46034699\n",
            "Iteration 141, loss = 0.46012740\n",
            "Iteration 142, loss = 0.46007846\n",
            "Iteration 143, loss = 0.45985119\n",
            "Iteration 144, loss = 0.45979209\n",
            "Iteration 145, loss = 0.45986109\n",
            "Iteration 146, loss = 0.45953627\n",
            "Iteration 147, loss = 0.45956731\n",
            "Iteration 148, loss = 0.45924479\n",
            "Iteration 149, loss = 0.45915966\n",
            "Iteration 150, loss = 0.45899687\n",
            "Iteration 151, loss = 0.45898837\n",
            "Iteration 152, loss = 0.45883034\n",
            "Iteration 153, loss = 0.45874205\n",
            "Iteration 154, loss = 0.45858665\n",
            "Iteration 155, loss = 0.45849174\n",
            "Iteration 156, loss = 0.45846531\n",
            "Iteration 157, loss = 0.45827698\n",
            "Iteration 158, loss = 0.45813887\n",
            "Iteration 159, loss = 0.45809919\n",
            "Iteration 160, loss = 0.45790229\n",
            "Iteration 161, loss = 0.45785160\n",
            "Iteration 162, loss = 0.45762467\n",
            "Iteration 163, loss = 0.45763955\n",
            "Iteration 164, loss = 0.45766873\n",
            "Iteration 165, loss = 0.45738722\n",
            "Iteration 166, loss = 0.45729069\n",
            "Iteration 167, loss = 0.45722410\n",
            "Iteration 168, loss = 0.45716586\n",
            "Iteration 169, loss = 0.45701336\n",
            "Iteration 170, loss = 0.45693343\n",
            "Iteration 171, loss = 0.45676547\n",
            "Iteration 172, loss = 0.45668995\n",
            "Iteration 173, loss = 0.45658828\n",
            "Iteration 174, loss = 0.45644678\n",
            "Iteration 175, loss = 0.45640259\n",
            "Iteration 176, loss = 0.45639053\n",
            "Iteration 177, loss = 0.45621930\n",
            "Iteration 178, loss = 0.45612545\n",
            "Iteration 179, loss = 0.45603072\n",
            "Iteration 180, loss = 0.45607554\n",
            "Iteration 181, loss = 0.45593846\n",
            "Iteration 182, loss = 0.45581952\n",
            "Iteration 183, loss = 0.45590138\n",
            "Iteration 184, loss = 0.45557570\n",
            "Iteration 185, loss = 0.45570144\n",
            "Iteration 186, loss = 0.45558493\n",
            "Iteration 187, loss = 0.45550665\n",
            "Iteration 188, loss = 0.45533175\n",
            "Iteration 189, loss = 0.45528688\n",
            "Iteration 190, loss = 0.45517393\n",
            "Iteration 191, loss = 0.45522327\n",
            "Iteration 192, loss = 0.45505225\n",
            "Iteration 193, loss = 0.45498402\n",
            "Iteration 194, loss = 0.45491205\n",
            "Iteration 195, loss = 0.45487627\n",
            "Iteration 196, loss = 0.45476018\n",
            "Iteration 197, loss = 0.45469438\n",
            "Iteration 198, loss = 0.45462880\n",
            "Iteration 199, loss = 0.45476869\n",
            "Iteration 200, loss = 0.45464465\n",
            "Iteration 1, loss = 0.70591289\n",
            "Iteration 2, loss = 0.69946982\n",
            "Iteration 3, loss = 0.69334710\n",
            "Iteration 4, loss = 0.68816186\n",
            "Iteration 5, loss = 0.68378735\n",
            "Iteration 6, loss = 0.67977806\n",
            "Iteration 7, loss = 0.67581146\n",
            "Iteration 8, loss = 0.67174697\n",
            "Iteration 9, loss = 0.66734855\n",
            "Iteration 10, loss = 0.66263351\n",
            "Iteration 11, loss = 0.65737902\n",
            "Iteration 12, loss = 0.65160669\n",
            "Iteration 13, loss = 0.64512408\n",
            "Iteration 14, loss = 0.63803640\n",
            "Iteration 15, loss = 0.63045945\n",
            "Iteration 16, loss = 0.62227255\n",
            "Iteration 17, loss = 0.61390631\n",
            "Iteration 18, loss = 0.60526241\n",
            "Iteration 19, loss = 0.59674471\n",
            "Iteration 20, loss = 0.58828723\n",
            "Iteration 21, loss = 0.57992653\n",
            "Iteration 22, loss = 0.57131648\n",
            "Iteration 23, loss = 0.56247583\n",
            "Iteration 24, loss = 0.55395774\n",
            "Iteration 25, loss = 0.54590494\n",
            "Iteration 26, loss = 0.53833115\n",
            "Iteration 27, loss = 0.53164960\n",
            "Iteration 28, loss = 0.52556347\n",
            "Iteration 29, loss = 0.52002221\n",
            "Iteration 30, loss = 0.51510222\n",
            "Iteration 31, loss = 0.51063775\n",
            "Iteration 32, loss = 0.50664478\n",
            "Iteration 33, loss = 0.50305386\n",
            "Iteration 34, loss = 0.49990807\n",
            "Iteration 35, loss = 0.49690009\n",
            "Iteration 36, loss = 0.49442031\n",
            "Iteration 37, loss = 0.49184431\n",
            "Iteration 38, loss = 0.48980810\n",
            "Iteration 39, loss = 0.48794369\n",
            "Iteration 40, loss = 0.48622405\n",
            "Iteration 41, loss = 0.48456441\n",
            "Iteration 42, loss = 0.48306539\n",
            "Iteration 43, loss = 0.48166144\n",
            "Iteration 44, loss = 0.48036929\n",
            "Iteration 45, loss = 0.47924334\n",
            "Iteration 46, loss = 0.47812416\n",
            "Iteration 47, loss = 0.47713004\n",
            "Iteration 48, loss = 0.47627259\n",
            "Iteration 49, loss = 0.47551071\n",
            "Iteration 50, loss = 0.47463522\n",
            "Iteration 51, loss = 0.47380846\n",
            "Iteration 52, loss = 0.47315303\n",
            "Iteration 53, loss = 0.47248491\n",
            "Iteration 54, loss = 0.47171736\n",
            "Iteration 55, loss = 0.47112521\n",
            "Iteration 56, loss = 0.47054761\n",
            "Iteration 57, loss = 0.46980205\n",
            "Iteration 58, loss = 0.46939710\n",
            "Iteration 59, loss = 0.46874265\n",
            "Iteration 60, loss = 0.46826085\n",
            "Iteration 61, loss = 0.46820124\n",
            "Iteration 62, loss = 0.46726624\n",
            "Iteration 63, loss = 0.46687775\n",
            "Iteration 64, loss = 0.46638737\n",
            "Iteration 65, loss = 0.46594037\n",
            "Iteration 66, loss = 0.46557410\n",
            "Iteration 67, loss = 0.46520927\n",
            "Iteration 68, loss = 0.46493064\n",
            "Iteration 69, loss = 0.46447083\n",
            "Iteration 70, loss = 0.46414777\n",
            "Iteration 71, loss = 0.46366859\n",
            "Iteration 72, loss = 0.46334427\n",
            "Iteration 73, loss = 0.46293678\n",
            "Iteration 74, loss = 0.46257023\n",
            "Iteration 75, loss = 0.46222286\n",
            "Iteration 76, loss = 0.46197396\n",
            "Iteration 77, loss = 0.46157100\n",
            "Iteration 78, loss = 0.46149704\n",
            "Iteration 79, loss = 0.46099516\n",
            "Iteration 80, loss = 0.46059459\n",
            "Iteration 81, loss = 0.46027174\n",
            "Iteration 82, loss = 0.45997247\n",
            "Iteration 83, loss = 0.45970668\n",
            "Iteration 84, loss = 0.45947696\n",
            "Iteration 85, loss = 0.45924051\n",
            "Iteration 86, loss = 0.45874603\n",
            "Iteration 87, loss = 0.45855017\n",
            "Iteration 88, loss = 0.45813705\n",
            "Iteration 89, loss = 0.45787102\n",
            "Iteration 90, loss = 0.45756476\n",
            "Iteration 91, loss = 0.45739819\n",
            "Iteration 92, loss = 0.45710179\n",
            "Iteration 93, loss = 0.45685471\n",
            "Iteration 94, loss = 0.45659239\n",
            "Iteration 95, loss = 0.45643381\n",
            "Iteration 96, loss = 0.45611351\n",
            "Iteration 97, loss = 0.45586154\n",
            "Iteration 98, loss = 0.45587677\n",
            "Iteration 99, loss = 0.45570901\n",
            "Iteration 100, loss = 0.45516515\n",
            "Iteration 101, loss = 0.45490494\n",
            "Iteration 102, loss = 0.45471517\n",
            "Iteration 103, loss = 0.45445934\n",
            "Iteration 104, loss = 0.45434971\n",
            "Iteration 105, loss = 0.45441096\n",
            "Iteration 106, loss = 0.45385713\n",
            "Iteration 107, loss = 0.45368617\n",
            "Iteration 108, loss = 0.45344644\n",
            "Iteration 109, loss = 0.45325286\n",
            "Iteration 110, loss = 0.45307738\n",
            "Iteration 111, loss = 0.45294232\n",
            "Iteration 112, loss = 0.45267809\n",
            "Iteration 113, loss = 0.45245532\n",
            "Iteration 114, loss = 0.45226667\n",
            "Iteration 115, loss = 0.45207991\n",
            "Iteration 116, loss = 0.45195693\n",
            "Iteration 117, loss = 0.45178704\n",
            "Iteration 118, loss = 0.45163983\n",
            "Iteration 119, loss = 0.45138061\n",
            "Iteration 120, loss = 0.45122672\n",
            "Iteration 121, loss = 0.45114011\n",
            "Iteration 122, loss = 0.45107426\n",
            "Iteration 123, loss = 0.45090556\n",
            "Iteration 124, loss = 0.45054268\n",
            "Iteration 125, loss = 0.45041583\n",
            "Iteration 126, loss = 0.45040285\n",
            "Iteration 127, loss = 0.45012597\n",
            "Iteration 128, loss = 0.44999469\n",
            "Iteration 129, loss = 0.45007097\n",
            "Iteration 130, loss = 0.44985556\n",
            "Iteration 131, loss = 0.44951529\n",
            "Iteration 132, loss = 0.44940152\n",
            "Iteration 133, loss = 0.44925414\n",
            "Iteration 134, loss = 0.44908830\n",
            "Iteration 135, loss = 0.44900957\n",
            "Iteration 136, loss = 0.44887516\n",
            "Iteration 137, loss = 0.44902020\n",
            "Iteration 138, loss = 0.44861924\n",
            "Iteration 139, loss = 0.44854851\n",
            "Iteration 140, loss = 0.44820467\n",
            "Iteration 141, loss = 0.44832576\n",
            "Iteration 142, loss = 0.44819940\n",
            "Iteration 143, loss = 0.44798401\n",
            "Iteration 144, loss = 0.44783762\n",
            "Iteration 145, loss = 0.44769604\n",
            "Iteration 146, loss = 0.44758270\n",
            "Iteration 147, loss = 0.44765864\n",
            "Iteration 148, loss = 0.44728235\n",
            "Iteration 149, loss = 0.44718721\n",
            "Iteration 150, loss = 0.44707213\n",
            "Iteration 151, loss = 0.44688199\n",
            "Iteration 152, loss = 0.44680078\n",
            "Iteration 153, loss = 0.44680585\n",
            "Iteration 154, loss = 0.44671032\n",
            "Iteration 155, loss = 0.44664672\n",
            "Iteration 156, loss = 0.44641474\n",
            "Iteration 157, loss = 0.44629526\n",
            "Iteration 158, loss = 0.44625420\n",
            "Iteration 159, loss = 0.44600352\n",
            "Iteration 160, loss = 0.44595120\n",
            "Iteration 161, loss = 0.44580895\n",
            "Iteration 162, loss = 0.44573510\n",
            "Iteration 163, loss = 0.44557000\n",
            "Iteration 164, loss = 0.44558498\n",
            "Iteration 165, loss = 0.44534720\n",
            "Iteration 166, loss = 0.44525773\n",
            "Iteration 167, loss = 0.44518792\n",
            "Iteration 168, loss = 0.44522765\n",
            "Iteration 169, loss = 0.44487596\n",
            "Iteration 170, loss = 0.44488285\n",
            "Iteration 171, loss = 0.44481628\n",
            "Iteration 172, loss = 0.44468327\n",
            "Iteration 173, loss = 0.44448161\n",
            "Iteration 174, loss = 0.44435734\n",
            "Iteration 175, loss = 0.44425984\n",
            "Iteration 176, loss = 0.44431608\n",
            "Iteration 177, loss = 0.44429973\n",
            "Iteration 178, loss = 0.44411965\n",
            "Iteration 179, loss = 0.44385956\n",
            "Iteration 180, loss = 0.44386346\n",
            "Iteration 181, loss = 0.44374233\n",
            "Iteration 182, loss = 0.44364741\n",
            "Iteration 183, loss = 0.44371662\n",
            "Iteration 184, loss = 0.44351621\n",
            "Iteration 185, loss = 0.44364078\n",
            "Iteration 186, loss = 0.44325739\n",
            "Iteration 187, loss = 0.44316852\n",
            "Iteration 188, loss = 0.44315326\n",
            "Iteration 189, loss = 0.44334282\n",
            "Iteration 190, loss = 0.44304913\n",
            "Iteration 191, loss = 0.44311469\n",
            "Iteration 192, loss = 0.44283156\n",
            "Iteration 193, loss = 0.44266812\n",
            "Iteration 194, loss = 0.44262797\n",
            "Iteration 195, loss = 0.44270534\n",
            "Iteration 196, loss = 0.44272088\n",
            "Iteration 197, loss = 0.44247972\n",
            "Iteration 198, loss = 0.44225948\n",
            "Iteration 199, loss = 0.44224806\n",
            "Iteration 200, loss = 0.44214769\n",
            "Iteration 1, loss = 0.70234959\n",
            "Iteration 2, loss = 0.69232942\n",
            "Iteration 3, loss = 0.68408662\n",
            "Iteration 4, loss = 0.67694872\n",
            "Iteration 5, loss = 0.66966404\n",
            "Iteration 6, loss = 0.66123995\n",
            "Iteration 7, loss = 0.65129158\n",
            "Iteration 8, loss = 0.63949374\n",
            "Iteration 9, loss = 0.62579172\n",
            "Iteration 10, loss = 0.61036568\n",
            "Iteration 11, loss = 0.59417958\n",
            "Iteration 12, loss = 0.57685417\n",
            "Iteration 13, loss = 0.56059548\n",
            "Iteration 14, loss = 0.54590255\n",
            "Iteration 15, loss = 0.53386991\n",
            "Iteration 16, loss = 0.52390093\n",
            "Iteration 17, loss = 0.51587526\n",
            "Iteration 18, loss = 0.50945987\n",
            "Iteration 19, loss = 0.50408482\n",
            "Iteration 20, loss = 0.49985653\n",
            "Iteration 21, loss = 0.49630062\n",
            "Iteration 22, loss = 0.49329694\n",
            "Iteration 23, loss = 0.49092603\n",
            "Iteration 24, loss = 0.48878829\n",
            "Iteration 25, loss = 0.48697027\n",
            "Iteration 26, loss = 0.48530247\n",
            "Iteration 27, loss = 0.48384619\n",
            "Iteration 28, loss = 0.48254493\n",
            "Iteration 29, loss = 0.48141765\n",
            "Iteration 30, loss = 0.48039413\n",
            "Iteration 31, loss = 0.47951214\n",
            "Iteration 32, loss = 0.47863101\n",
            "Iteration 33, loss = 0.47771790\n",
            "Iteration 34, loss = 0.47707200\n",
            "Iteration 35, loss = 0.47631206\n",
            "Iteration 36, loss = 0.47569789\n",
            "Iteration 37, loss = 0.47517924\n",
            "Iteration 38, loss = 0.47451888\n",
            "Iteration 39, loss = 0.47394791\n",
            "Iteration 40, loss = 0.47340691\n",
            "Iteration 41, loss = 0.47293437\n",
            "Iteration 42, loss = 0.47237915\n",
            "Iteration 43, loss = 0.47191867\n",
            "Iteration 44, loss = 0.47138740\n",
            "Iteration 45, loss = 0.47103032\n",
            "Iteration 46, loss = 0.47051551\n",
            "Iteration 47, loss = 0.47010285\n",
            "Iteration 48, loss = 0.46977903\n",
            "Iteration 49, loss = 0.46912647\n",
            "Iteration 50, loss = 0.46888283\n",
            "Iteration 51, loss = 0.46834545\n",
            "Iteration 52, loss = 0.46794425\n",
            "Iteration 53, loss = 0.46752186\n",
            "Iteration 54, loss = 0.46714678\n",
            "Iteration 55, loss = 0.46696998\n",
            "Iteration 56, loss = 0.46652967\n",
            "Iteration 57, loss = 0.46626891\n",
            "Iteration 58, loss = 0.46610003\n",
            "Iteration 59, loss = 0.46539303\n",
            "Iteration 60, loss = 0.46521279\n",
            "Iteration 61, loss = 0.46480948\n",
            "Iteration 62, loss = 0.46444695\n",
            "Iteration 63, loss = 0.46426065\n",
            "Iteration 64, loss = 0.46387372\n",
            "Iteration 65, loss = 0.46370413\n",
            "Iteration 66, loss = 0.46332969\n",
            "Iteration 67, loss = 0.46297050\n",
            "Iteration 68, loss = 0.46281233\n",
            "Iteration 69, loss = 0.46242430\n",
            "Iteration 70, loss = 0.46229322\n",
            "Iteration 71, loss = 0.46199829\n",
            "Iteration 72, loss = 0.46191782\n",
            "Iteration 73, loss = 0.46146009\n",
            "Iteration 74, loss = 0.46124489\n",
            "Iteration 75, loss = 0.46093506\n",
            "Iteration 76, loss = 0.46078200\n",
            "Iteration 77, loss = 0.46040451\n",
            "Iteration 78, loss = 0.46015408\n",
            "Iteration 79, loss = 0.45999090\n",
            "Iteration 80, loss = 0.45970017\n",
            "Iteration 81, loss = 0.45945339\n",
            "Iteration 82, loss = 0.45924630\n",
            "Iteration 83, loss = 0.45902728\n",
            "Iteration 84, loss = 0.45876296\n",
            "Iteration 85, loss = 0.45848780\n",
            "Iteration 86, loss = 0.45830630\n",
            "Iteration 87, loss = 0.45806314\n",
            "Iteration 88, loss = 0.45787696\n",
            "Iteration 89, loss = 0.45750662\n",
            "Iteration 90, loss = 0.45741625\n",
            "Iteration 91, loss = 0.45718701\n",
            "Iteration 92, loss = 0.45699021\n",
            "Iteration 93, loss = 0.45681580\n",
            "Iteration 94, loss = 0.45664596\n",
            "Iteration 95, loss = 0.45647973\n",
            "Iteration 96, loss = 0.45619700\n",
            "Iteration 97, loss = 0.45587831\n",
            "Iteration 98, loss = 0.45575356\n",
            "Iteration 99, loss = 0.45554492\n",
            "Iteration 100, loss = 0.45540052\n",
            "Iteration 101, loss = 0.45514736\n",
            "Iteration 102, loss = 0.45494666\n",
            "Iteration 103, loss = 0.45491784\n",
            "Iteration 104, loss = 0.45462375\n",
            "Iteration 105, loss = 0.45457677\n",
            "Iteration 106, loss = 0.45454615\n",
            "Iteration 107, loss = 0.45431344\n",
            "Iteration 108, loss = 0.45413054\n",
            "Iteration 109, loss = 0.45403630\n",
            "Iteration 110, loss = 0.45390885\n",
            "Iteration 111, loss = 0.45369230\n",
            "Iteration 112, loss = 0.45358445\n",
            "Iteration 113, loss = 0.45347817\n",
            "Iteration 114, loss = 0.45337285\n",
            "Iteration 115, loss = 0.45326117\n",
            "Iteration 116, loss = 0.45318239\n",
            "Iteration 117, loss = 0.45296280\n",
            "Iteration 118, loss = 0.45283677\n",
            "Iteration 119, loss = 0.45291014\n",
            "Iteration 120, loss = 0.45273663\n",
            "Iteration 121, loss = 0.45268996\n",
            "Iteration 122, loss = 0.45280814\n",
            "Iteration 123, loss = 0.45239428\n",
            "Iteration 124, loss = 0.45245865\n",
            "Iteration 125, loss = 0.45235455\n",
            "Iteration 126, loss = 0.45212040\n",
            "Iteration 127, loss = 0.45209458\n",
            "Iteration 128, loss = 0.45192119\n",
            "Iteration 129, loss = 0.45182050\n",
            "Iteration 130, loss = 0.45172827\n",
            "Iteration 131, loss = 0.45162537\n",
            "Iteration 132, loss = 0.45155600\n",
            "Iteration 133, loss = 0.45149999\n",
            "Iteration 134, loss = 0.45133009\n",
            "Iteration 135, loss = 0.45137805\n",
            "Iteration 136, loss = 0.45120114\n",
            "Iteration 137, loss = 0.45125378\n",
            "Iteration 138, loss = 0.45100182\n",
            "Iteration 139, loss = 0.45097883\n",
            "Iteration 140, loss = 0.45091336\n",
            "Iteration 141, loss = 0.45082414\n",
            "Iteration 142, loss = 0.45070928\n",
            "Iteration 143, loss = 0.45061393\n",
            "Iteration 144, loss = 0.45060873\n",
            "Iteration 145, loss = 0.45060820\n",
            "Iteration 146, loss = 0.45051182\n",
            "Iteration 147, loss = 0.45035698\n",
            "Iteration 148, loss = 0.45027083\n",
            "Iteration 149, loss = 0.45040050\n",
            "Iteration 150, loss = 0.45028183\n",
            "Iteration 151, loss = 0.45022153\n",
            "Iteration 152, loss = 0.45001444\n",
            "Iteration 153, loss = 0.44995633\n",
            "Iteration 154, loss = 0.45017844\n",
            "Iteration 155, loss = 0.45001552\n",
            "Iteration 156, loss = 0.44976204\n",
            "Iteration 157, loss = 0.44994786\n",
            "Iteration 158, loss = 0.44964115\n",
            "Iteration 159, loss = 0.44960957\n",
            "Iteration 160, loss = 0.44983026\n",
            "Iteration 161, loss = 0.44955282\n",
            "Iteration 162, loss = 0.44947719\n",
            "Iteration 163, loss = 0.44940570\n",
            "Iteration 164, loss = 0.44938995\n",
            "Iteration 165, loss = 0.44931189\n",
            "Iteration 166, loss = 0.44935043\n",
            "Iteration 167, loss = 0.44908194\n",
            "Iteration 168, loss = 0.44929494\n",
            "Iteration 169, loss = 0.44932567\n",
            "Iteration 170, loss = 0.44905393\n",
            "Iteration 171, loss = 0.44912884\n",
            "Iteration 172, loss = 0.44907742\n",
            "Iteration 173, loss = 0.44897880\n",
            "Iteration 174, loss = 0.44884190\n",
            "Iteration 175, loss = 0.44872472\n",
            "Iteration 176, loss = 0.44903017\n",
            "Iteration 177, loss = 0.44875375\n",
            "Iteration 178, loss = 0.44859977\n",
            "Iteration 179, loss = 0.44862690\n",
            "Iteration 180, loss = 0.44846921\n",
            "Iteration 181, loss = 0.44845181\n",
            "Iteration 182, loss = 0.44856923\n",
            "Iteration 183, loss = 0.44847195\n",
            "Iteration 184, loss = 0.44860560\n",
            "Iteration 185, loss = 0.44828886\n",
            "Iteration 186, loss = 0.44834765\n",
            "Iteration 187, loss = 0.44823597\n",
            "Iteration 188, loss = 0.44841014\n",
            "Iteration 189, loss = 0.44816929\n",
            "Iteration 190, loss = 0.44802617\n",
            "Iteration 191, loss = 0.44806143\n",
            "Iteration 192, loss = 0.44803495\n",
            "Iteration 193, loss = 0.44797396\n",
            "Iteration 194, loss = 0.44791422\n",
            "Iteration 195, loss = 0.44789291\n",
            "Iteration 196, loss = 0.44784640\n",
            "Iteration 197, loss = 0.44771715\n",
            "Iteration 198, loss = 0.44770498\n",
            "Iteration 199, loss = 0.44773920\n",
            "Iteration 200, loss = 0.44756997\n",
            "Iteration 1, loss = 0.70261921\n",
            "Iteration 2, loss = 0.69236806\n",
            "Iteration 3, loss = 0.68391752\n",
            "Iteration 4, loss = 0.67651006\n",
            "Iteration 5, loss = 0.66889073\n",
            "Iteration 6, loss = 0.66015652\n",
            "Iteration 7, loss = 0.64967665\n",
            "Iteration 8, loss = 0.63733525\n",
            "Iteration 9, loss = 0.62297059\n",
            "Iteration 10, loss = 0.60700393\n",
            "Iteration 11, loss = 0.59027822\n",
            "Iteration 12, loss = 0.57256321\n",
            "Iteration 13, loss = 0.55577205\n",
            "Iteration 14, loss = 0.54057758\n",
            "Iteration 15, loss = 0.52786274\n",
            "Iteration 16, loss = 0.51743236\n",
            "Iteration 17, loss = 0.50901677\n",
            "Iteration 18, loss = 0.50220859\n",
            "Iteration 19, loss = 0.49658661\n",
            "Iteration 20, loss = 0.49212171\n",
            "Iteration 21, loss = 0.48843244\n",
            "Iteration 22, loss = 0.48536310\n",
            "Iteration 23, loss = 0.48284448\n",
            "Iteration 24, loss = 0.48057722\n",
            "Iteration 25, loss = 0.47872059\n",
            "Iteration 26, loss = 0.47715183\n",
            "Iteration 27, loss = 0.47565290\n",
            "Iteration 28, loss = 0.47426535\n",
            "Iteration 29, loss = 0.47318259\n",
            "Iteration 30, loss = 0.47203443\n",
            "Iteration 31, loss = 0.47111081\n",
            "Iteration 32, loss = 0.47014578\n",
            "Iteration 33, loss = 0.46927013\n",
            "Iteration 34, loss = 0.46861710\n",
            "Iteration 35, loss = 0.46783431\n",
            "Iteration 36, loss = 0.46705256\n",
            "Iteration 37, loss = 0.46639444\n",
            "Iteration 38, loss = 0.46582977\n",
            "Iteration 39, loss = 0.46509850\n",
            "Iteration 40, loss = 0.46452507\n",
            "Iteration 41, loss = 0.46406036\n",
            "Iteration 42, loss = 0.46333831\n",
            "Iteration 43, loss = 0.46283519\n",
            "Iteration 44, loss = 0.46226071\n",
            "Iteration 45, loss = 0.46181924\n",
            "Iteration 46, loss = 0.46126978\n",
            "Iteration 47, loss = 0.46073277\n",
            "Iteration 48, loss = 0.46024845\n",
            "Iteration 49, loss = 0.45977827\n",
            "Iteration 50, loss = 0.45931387\n",
            "Iteration 51, loss = 0.45884980\n",
            "Iteration 52, loss = 0.45842915\n",
            "Iteration 53, loss = 0.45803695\n",
            "Iteration 54, loss = 0.45765741\n",
            "Iteration 55, loss = 0.45723796\n",
            "Iteration 56, loss = 0.45675693\n",
            "Iteration 57, loss = 0.45630010\n",
            "Iteration 58, loss = 0.45593011\n",
            "Iteration 59, loss = 0.45554574\n",
            "Iteration 60, loss = 0.45526259\n",
            "Iteration 61, loss = 0.45474894\n",
            "Iteration 62, loss = 0.45433424\n",
            "Iteration 63, loss = 0.45395791\n",
            "Iteration 64, loss = 0.45357643\n",
            "Iteration 65, loss = 0.45322605\n",
            "Iteration 66, loss = 0.45282059\n",
            "Iteration 67, loss = 0.45247457\n",
            "Iteration 68, loss = 0.45208142\n",
            "Iteration 69, loss = 0.45177058\n",
            "Iteration 70, loss = 0.45160778\n",
            "Iteration 71, loss = 0.45111201\n",
            "Iteration 72, loss = 0.45098364\n",
            "Iteration 73, loss = 0.45066323\n",
            "Iteration 74, loss = 0.45032198\n",
            "Iteration 75, loss = 0.44999623\n",
            "Iteration 76, loss = 0.44976195\n",
            "Iteration 77, loss = 0.44958343\n",
            "Iteration 78, loss = 0.44930032\n",
            "Iteration 79, loss = 0.44914749\n",
            "Iteration 80, loss = 0.44871929\n",
            "Iteration 81, loss = 0.44866175\n",
            "Iteration 82, loss = 0.44858176\n",
            "Iteration 83, loss = 0.44821428\n",
            "Iteration 84, loss = 0.44814169\n",
            "Iteration 85, loss = 0.44775244\n",
            "Iteration 86, loss = 0.44765459\n",
            "Iteration 87, loss = 0.44739342\n",
            "Iteration 88, loss = 0.44734297\n",
            "Iteration 89, loss = 0.44704976\n",
            "Iteration 90, loss = 0.44697429\n",
            "Iteration 91, loss = 0.44674225\n",
            "Iteration 92, loss = 0.44669439\n",
            "Iteration 93, loss = 0.44646565\n",
            "Iteration 94, loss = 0.44633160\n",
            "Iteration 95, loss = 0.44614417\n",
            "Iteration 96, loss = 0.44595092\n",
            "Iteration 97, loss = 0.44578681\n",
            "Iteration 98, loss = 0.44579300\n",
            "Iteration 99, loss = 0.44556157\n",
            "Iteration 100, loss = 0.44554335\n",
            "Iteration 101, loss = 0.44536705\n",
            "Iteration 102, loss = 0.44522270\n",
            "Iteration 103, loss = 0.44520851\n",
            "Iteration 104, loss = 0.44500034\n",
            "Iteration 105, loss = 0.44483314\n",
            "Iteration 106, loss = 0.44482426\n",
            "Iteration 107, loss = 0.44458263\n",
            "Iteration 108, loss = 0.44458164\n",
            "Iteration 109, loss = 0.44437697\n",
            "Iteration 110, loss = 0.44458859\n",
            "Iteration 111, loss = 0.44425157\n",
            "Iteration 112, loss = 0.44427253\n",
            "Iteration 113, loss = 0.44408138\n",
            "Iteration 114, loss = 0.44416182\n",
            "Iteration 115, loss = 0.44377383\n",
            "Iteration 116, loss = 0.44375373\n",
            "Iteration 117, loss = 0.44367556\n",
            "Iteration 118, loss = 0.44356813\n",
            "Iteration 119, loss = 0.44348220\n",
            "Iteration 120, loss = 0.44351305\n",
            "Iteration 121, loss = 0.44339830\n",
            "Iteration 122, loss = 0.44323555\n",
            "Iteration 123, loss = 0.44314108\n",
            "Iteration 124, loss = 0.44313355\n",
            "Iteration 125, loss = 0.44317357\n",
            "Iteration 126, loss = 0.44313015\n",
            "Iteration 127, loss = 0.44289093\n",
            "Iteration 128, loss = 0.44271226\n",
            "Iteration 129, loss = 0.44272337\n",
            "Iteration 130, loss = 0.44268259\n",
            "Iteration 131, loss = 0.44269724\n",
            "Iteration 132, loss = 0.44255014\n",
            "Iteration 133, loss = 0.44243229\n",
            "Iteration 134, loss = 0.44233685\n",
            "Iteration 135, loss = 0.44242782\n",
            "Iteration 136, loss = 0.44225398\n",
            "Iteration 137, loss = 0.44216424\n",
            "Iteration 138, loss = 0.44205958\n",
            "Iteration 139, loss = 0.44204663\n",
            "Iteration 140, loss = 0.44212485\n",
            "Iteration 141, loss = 0.44193015\n",
            "Iteration 142, loss = 0.44179864\n",
            "Iteration 143, loss = 0.44186372\n",
            "Iteration 144, loss = 0.44175021\n",
            "Iteration 145, loss = 0.44167298\n",
            "Iteration 146, loss = 0.44183604\n",
            "Iteration 147, loss = 0.44151215\n",
            "Iteration 148, loss = 0.44158383\n",
            "Iteration 149, loss = 0.44156750\n",
            "Iteration 150, loss = 0.44136940\n",
            "Iteration 151, loss = 0.44153612\n",
            "Iteration 152, loss = 0.44120129\n",
            "Iteration 153, loss = 0.44117587\n",
            "Iteration 154, loss = 0.44124065\n",
            "Iteration 155, loss = 0.44102754\n",
            "Iteration 156, loss = 0.44108709\n",
            "Iteration 157, loss = 0.44129828\n",
            "Iteration 158, loss = 0.44099219\n",
            "Iteration 159, loss = 0.44088012\n",
            "Iteration 160, loss = 0.44084416\n",
            "Iteration 161, loss = 0.44067831\n",
            "Iteration 162, loss = 0.44069887\n",
            "Iteration 163, loss = 0.44060184\n",
            "Iteration 164, loss = 0.44062628\n",
            "Iteration 165, loss = 0.44070076\n",
            "Iteration 166, loss = 0.44056233\n",
            "Iteration 167, loss = 0.44033118\n",
            "Iteration 168, loss = 0.44031236\n",
            "Iteration 169, loss = 0.44045406\n",
            "Iteration 170, loss = 0.44042047\n",
            "Iteration 171, loss = 0.44033799\n",
            "Iteration 172, loss = 0.44025059\n",
            "Iteration 173, loss = 0.44017490\n",
            "Iteration 174, loss = 0.44006786\n",
            "Iteration 175, loss = 0.44007039\n",
            "Iteration 176, loss = 0.43994519\n",
            "Iteration 177, loss = 0.44002676\n",
            "Iteration 178, loss = 0.43980879\n",
            "Iteration 179, loss = 0.43978595\n",
            "Iteration 180, loss = 0.43973983\n",
            "Iteration 181, loss = 0.43975380\n",
            "Iteration 182, loss = 0.44004680\n",
            "Iteration 183, loss = 0.43986437\n",
            "Iteration 184, loss = 0.43955340\n",
            "Iteration 185, loss = 0.43969466\n",
            "Iteration 186, loss = 0.43964404\n",
            "Iteration 187, loss = 0.43967774\n",
            "Iteration 188, loss = 0.43938750\n",
            "Iteration 189, loss = 0.43941434\n",
            "Iteration 190, loss = 0.43924721\n",
            "Iteration 191, loss = 0.43929556\n",
            "Iteration 192, loss = 0.43930853\n",
            "Iteration 193, loss = 0.43916384\n",
            "Iteration 194, loss = 0.43916042\n",
            "Iteration 195, loss = 0.43896598\n",
            "Iteration 196, loss = 0.43894558\n",
            "Iteration 197, loss = 0.43897260\n",
            "Iteration 198, loss = 0.43898288\n",
            "Iteration 199, loss = 0.43889157\n",
            "Iteration 200, loss = 0.43881948\n",
            "Iteration 1, loss = 0.70257100\n",
            "Iteration 2, loss = 0.69263128\n",
            "Iteration 3, loss = 0.68445965\n",
            "Iteration 4, loss = 0.67737948\n",
            "Iteration 5, loss = 0.67011472\n",
            "Iteration 6, loss = 0.66176881\n",
            "Iteration 7, loss = 0.65182783\n",
            "Iteration 8, loss = 0.64001593\n",
            "Iteration 9, loss = 0.62602039\n",
            "Iteration 10, loss = 0.61018550\n",
            "Iteration 11, loss = 0.59302342\n",
            "Iteration 12, loss = 0.57510756\n",
            "Iteration 13, loss = 0.55863210\n",
            "Iteration 14, loss = 0.54426587\n",
            "Iteration 15, loss = 0.53248519\n",
            "Iteration 16, loss = 0.52278366\n",
            "Iteration 17, loss = 0.51493417\n",
            "Iteration 18, loss = 0.50871778\n",
            "Iteration 19, loss = 0.50369200\n",
            "Iteration 20, loss = 0.49956563\n",
            "Iteration 21, loss = 0.49618855\n",
            "Iteration 22, loss = 0.49337595\n",
            "Iteration 23, loss = 0.49096171\n",
            "Iteration 24, loss = 0.48891611\n",
            "Iteration 25, loss = 0.48717963\n",
            "Iteration 26, loss = 0.48561750\n",
            "Iteration 27, loss = 0.48426936\n",
            "Iteration 28, loss = 0.48301628\n",
            "Iteration 29, loss = 0.48208383\n",
            "Iteration 30, loss = 0.48098494\n",
            "Iteration 31, loss = 0.48011230\n",
            "Iteration 32, loss = 0.47919467\n",
            "Iteration 33, loss = 0.47835352\n",
            "Iteration 34, loss = 0.47765258\n",
            "Iteration 35, loss = 0.47693680\n",
            "Iteration 36, loss = 0.47627925\n",
            "Iteration 37, loss = 0.47571774\n",
            "Iteration 38, loss = 0.47514791\n",
            "Iteration 39, loss = 0.47443240\n",
            "Iteration 40, loss = 0.47394635\n",
            "Iteration 41, loss = 0.47357422\n",
            "Iteration 42, loss = 0.47289576\n",
            "Iteration 43, loss = 0.47250018\n",
            "Iteration 44, loss = 0.47188099\n",
            "Iteration 45, loss = 0.47145738\n",
            "Iteration 46, loss = 0.47089531\n",
            "Iteration 47, loss = 0.47049985\n",
            "Iteration 48, loss = 0.47007926\n",
            "Iteration 49, loss = 0.46956589\n",
            "Iteration 50, loss = 0.46919924\n",
            "Iteration 51, loss = 0.46888816\n",
            "Iteration 52, loss = 0.46841223\n",
            "Iteration 53, loss = 0.46810187\n",
            "Iteration 54, loss = 0.46774620\n",
            "Iteration 55, loss = 0.46735488\n",
            "Iteration 56, loss = 0.46692847\n",
            "Iteration 57, loss = 0.46648531\n",
            "Iteration 58, loss = 0.46613763\n",
            "Iteration 59, loss = 0.46578507\n",
            "Iteration 60, loss = 0.46568218\n",
            "Iteration 61, loss = 0.46517241\n",
            "Iteration 62, loss = 0.46483716\n",
            "Iteration 63, loss = 0.46451218\n",
            "Iteration 64, loss = 0.46424241\n",
            "Iteration 65, loss = 0.46394274\n",
            "Iteration 66, loss = 0.46371245\n",
            "Iteration 67, loss = 0.46337289\n",
            "Iteration 68, loss = 0.46304415\n",
            "Iteration 69, loss = 0.46277938\n",
            "Iteration 70, loss = 0.46261012\n",
            "Iteration 71, loss = 0.46222904\n",
            "Iteration 72, loss = 0.46196027\n",
            "Iteration 73, loss = 0.46171913\n",
            "Iteration 74, loss = 0.46149364\n",
            "Iteration 75, loss = 0.46114596\n",
            "Iteration 76, loss = 0.46091853\n",
            "Iteration 77, loss = 0.46083204\n",
            "Iteration 78, loss = 0.46043562\n",
            "Iteration 79, loss = 0.46014318\n",
            "Iteration 80, loss = 0.45983010\n",
            "Iteration 81, loss = 0.45964297\n",
            "Iteration 82, loss = 0.45975748\n",
            "Iteration 83, loss = 0.45915836\n",
            "Iteration 84, loss = 0.45900471\n",
            "Iteration 85, loss = 0.45863354\n",
            "Iteration 86, loss = 0.45840510\n",
            "Iteration 87, loss = 0.45814043\n",
            "Iteration 88, loss = 0.45797802\n",
            "Iteration 89, loss = 0.45772179\n",
            "Iteration 90, loss = 0.45747939\n",
            "Iteration 91, loss = 0.45732222\n",
            "Iteration 92, loss = 0.45717237\n",
            "Iteration 93, loss = 0.45687083\n",
            "Iteration 94, loss = 0.45670499\n",
            "Iteration 95, loss = 0.45639612\n",
            "Iteration 96, loss = 0.45622473\n",
            "Iteration 97, loss = 0.45597022\n",
            "Iteration 98, loss = 0.45591574\n",
            "Iteration 99, loss = 0.45559443\n",
            "Iteration 100, loss = 0.45547218\n",
            "Iteration 101, loss = 0.45524368\n",
            "Iteration 102, loss = 0.45501705\n",
            "Iteration 103, loss = 0.45499844\n",
            "Iteration 104, loss = 0.45479559\n",
            "Iteration 105, loss = 0.45459435\n",
            "Iteration 106, loss = 0.45444234\n",
            "Iteration 107, loss = 0.45428720\n",
            "Iteration 108, loss = 0.45420521\n",
            "Iteration 109, loss = 0.45407687\n",
            "Iteration 110, loss = 0.45403092\n",
            "Iteration 111, loss = 0.45387214\n",
            "Iteration 112, loss = 0.45381350\n",
            "Iteration 113, loss = 0.45349448\n",
            "Iteration 114, loss = 0.45363077\n",
            "Iteration 115, loss = 0.45331868\n",
            "Iteration 116, loss = 0.45310324\n",
            "Iteration 117, loss = 0.45303760\n",
            "Iteration 118, loss = 0.45291630\n",
            "Iteration 119, loss = 0.45280280\n",
            "Iteration 120, loss = 0.45283161\n",
            "Iteration 121, loss = 0.45258040\n",
            "Iteration 122, loss = 0.45250408\n",
            "Iteration 123, loss = 0.45232012\n",
            "Iteration 124, loss = 0.45247317\n",
            "Iteration 125, loss = 0.45219323\n",
            "Iteration 126, loss = 0.45224588\n",
            "Iteration 127, loss = 0.45208774\n",
            "Iteration 128, loss = 0.45179864\n",
            "Iteration 129, loss = 0.45187204\n",
            "Iteration 130, loss = 0.45180752\n",
            "Iteration 131, loss = 0.45179741\n",
            "Iteration 132, loss = 0.45166679\n",
            "Iteration 133, loss = 0.45143761\n",
            "Iteration 134, loss = 0.45130723\n",
            "Iteration 135, loss = 0.45134043\n",
            "Iteration 136, loss = 0.45128666\n",
            "Iteration 137, loss = 0.45114168\n",
            "Iteration 138, loss = 0.45098007\n",
            "Iteration 139, loss = 0.45102769\n",
            "Iteration 140, loss = 0.45102449\n",
            "Iteration 141, loss = 0.45093113\n",
            "Iteration 142, loss = 0.45078682\n",
            "Iteration 143, loss = 0.45079353\n",
            "Iteration 144, loss = 0.45063736\n",
            "Iteration 145, loss = 0.45054487\n",
            "Iteration 146, loss = 0.45071462\n",
            "Iteration 147, loss = 0.45041942\n",
            "Iteration 148, loss = 0.45058913\n",
            "Iteration 149, loss = 0.45027829\n",
            "Iteration 150, loss = 0.45019651\n",
            "Iteration 151, loss = 0.45022714\n",
            "Iteration 152, loss = 0.45005307\n",
            "Iteration 153, loss = 0.45002444\n",
            "Iteration 154, loss = 0.44995819\n",
            "Iteration 155, loss = 0.44982946\n",
            "Iteration 156, loss = 0.44985717\n",
            "Iteration 157, loss = 0.44989054\n",
            "Iteration 158, loss = 0.44960633\n",
            "Iteration 159, loss = 0.44961001\n",
            "Iteration 160, loss = 0.44952657\n",
            "Iteration 161, loss = 0.44944258\n",
            "Iteration 162, loss = 0.44954616\n",
            "Iteration 163, loss = 0.44932280\n",
            "Iteration 164, loss = 0.44934844\n",
            "Iteration 165, loss = 0.44919503\n",
            "Iteration 166, loss = 0.44924744\n",
            "Iteration 167, loss = 0.44900654\n",
            "Iteration 168, loss = 0.44898221\n",
            "Iteration 169, loss = 0.44911075\n",
            "Iteration 170, loss = 0.44902896\n",
            "Iteration 171, loss = 0.44890012\n",
            "Iteration 172, loss = 0.44885499\n",
            "Iteration 173, loss = 0.44864413\n",
            "Iteration 174, loss = 0.44870812\n",
            "Iteration 175, loss = 0.44862444\n",
            "Iteration 176, loss = 0.44846444\n",
            "Iteration 177, loss = 0.44845794\n",
            "Iteration 178, loss = 0.44832210\n",
            "Iteration 179, loss = 0.44828487\n",
            "Iteration 180, loss = 0.44826383\n",
            "Iteration 181, loss = 0.44825057\n",
            "Iteration 182, loss = 0.44846454\n",
            "Iteration 183, loss = 0.44827867\n",
            "Iteration 184, loss = 0.44802518\n",
            "Iteration 185, loss = 0.44816806\n",
            "Iteration 186, loss = 0.44811615\n",
            "Iteration 187, loss = 0.44805566\n",
            "Iteration 188, loss = 0.44789510\n",
            "Iteration 189, loss = 0.44791336\n",
            "Iteration 190, loss = 0.44769532\n",
            "Iteration 191, loss = 0.44766029\n",
            "Iteration 192, loss = 0.44779668\n",
            "Iteration 193, loss = 0.44769771\n",
            "Iteration 194, loss = 0.44762000\n",
            "Iteration 195, loss = 0.44747133\n",
            "Iteration 196, loss = 0.44741072\n",
            "Iteration 197, loss = 0.44747505\n",
            "Iteration 198, loss = 0.44742708\n",
            "Iteration 199, loss = 0.44741967\n",
            "Iteration 200, loss = 0.44737096\n",
            "Iteration 1, loss = 0.70230279\n",
            "Iteration 2, loss = 0.69209636\n",
            "Iteration 3, loss = 0.68385412\n",
            "Iteration 4, loss = 0.67662840\n",
            "Iteration 5, loss = 0.66917407\n",
            "Iteration 6, loss = 0.66047370\n",
            "Iteration 7, loss = 0.65014279\n",
            "Iteration 8, loss = 0.63786726\n",
            "Iteration 9, loss = 0.62355870\n",
            "Iteration 10, loss = 0.60757164\n",
            "Iteration 11, loss = 0.59036351\n",
            "Iteration 12, loss = 0.57232343\n",
            "Iteration 13, loss = 0.55560900\n",
            "Iteration 14, loss = 0.54116198\n",
            "Iteration 15, loss = 0.52934897\n",
            "Iteration 16, loss = 0.51961322\n",
            "Iteration 17, loss = 0.51200184\n",
            "Iteration 18, loss = 0.50575702\n",
            "Iteration 19, loss = 0.50074603\n",
            "Iteration 20, loss = 0.49669730\n",
            "Iteration 21, loss = 0.49333621\n",
            "Iteration 22, loss = 0.49055967\n",
            "Iteration 23, loss = 0.48821492\n",
            "Iteration 24, loss = 0.48616029\n",
            "Iteration 25, loss = 0.48447622\n",
            "Iteration 26, loss = 0.48295805\n",
            "Iteration 27, loss = 0.48149061\n",
            "Iteration 28, loss = 0.48036503\n",
            "Iteration 29, loss = 0.47933599\n",
            "Iteration 30, loss = 0.47827186\n",
            "Iteration 31, loss = 0.47744213\n",
            "Iteration 32, loss = 0.47662440\n",
            "Iteration 33, loss = 0.47583623\n",
            "Iteration 34, loss = 0.47516837\n",
            "Iteration 35, loss = 0.47445701\n",
            "Iteration 36, loss = 0.47378354\n",
            "Iteration 37, loss = 0.47321809\n",
            "Iteration 38, loss = 0.47267349\n",
            "Iteration 39, loss = 0.47211599\n",
            "Iteration 40, loss = 0.47158844\n",
            "Iteration 41, loss = 0.47113199\n",
            "Iteration 42, loss = 0.47067800\n",
            "Iteration 43, loss = 0.47018521\n",
            "Iteration 44, loss = 0.46959223\n",
            "Iteration 45, loss = 0.46913113\n",
            "Iteration 46, loss = 0.46869174\n",
            "Iteration 47, loss = 0.46829259\n",
            "Iteration 48, loss = 0.46779686\n",
            "Iteration 49, loss = 0.46740113\n",
            "Iteration 50, loss = 0.46694747\n",
            "Iteration 51, loss = 0.46647530\n",
            "Iteration 52, loss = 0.46610824\n",
            "Iteration 53, loss = 0.46576014\n",
            "Iteration 54, loss = 0.46537634\n",
            "Iteration 55, loss = 0.46488477\n",
            "Iteration 56, loss = 0.46455076\n",
            "Iteration 57, loss = 0.46418756\n",
            "Iteration 58, loss = 0.46390987\n",
            "Iteration 59, loss = 0.46348415\n",
            "Iteration 60, loss = 0.46333579\n",
            "Iteration 61, loss = 0.46298295\n",
            "Iteration 62, loss = 0.46248982\n",
            "Iteration 63, loss = 0.46230335\n",
            "Iteration 64, loss = 0.46178178\n",
            "Iteration 65, loss = 0.46153346\n",
            "Iteration 66, loss = 0.46114232\n",
            "Iteration 67, loss = 0.46104216\n",
            "Iteration 68, loss = 0.46044654\n",
            "Iteration 69, loss = 0.46019537\n",
            "Iteration 70, loss = 0.45992697\n",
            "Iteration 71, loss = 0.45964239\n",
            "Iteration 72, loss = 0.45936257\n",
            "Iteration 73, loss = 0.45891139\n",
            "Iteration 74, loss = 0.45871272\n",
            "Iteration 75, loss = 0.45831823\n",
            "Iteration 76, loss = 0.45807344\n",
            "Iteration 77, loss = 0.45789022\n",
            "Iteration 78, loss = 0.45756792\n",
            "Iteration 79, loss = 0.45730609\n",
            "Iteration 80, loss = 0.45716951\n",
            "Iteration 81, loss = 0.45686046\n",
            "Iteration 82, loss = 0.45659251\n",
            "Iteration 83, loss = 0.45633168\n",
            "Iteration 84, loss = 0.45612367\n",
            "Iteration 85, loss = 0.45579643\n",
            "Iteration 86, loss = 0.45561888\n",
            "Iteration 87, loss = 0.45540541\n",
            "Iteration 88, loss = 0.45516321\n",
            "Iteration 89, loss = 0.45485047\n",
            "Iteration 90, loss = 0.45468382\n",
            "Iteration 91, loss = 0.45452818\n",
            "Iteration 92, loss = 0.45437539\n",
            "Iteration 93, loss = 0.45410622\n",
            "Iteration 94, loss = 0.45396930\n",
            "Iteration 95, loss = 0.45374547\n",
            "Iteration 96, loss = 0.45365618\n",
            "Iteration 97, loss = 0.45336421\n",
            "Iteration 98, loss = 0.45345517\n",
            "Iteration 99, loss = 0.45320069\n",
            "Iteration 100, loss = 0.45291221\n",
            "Iteration 101, loss = 0.45281145\n",
            "Iteration 102, loss = 0.45252272\n",
            "Iteration 103, loss = 0.45241240\n",
            "Iteration 104, loss = 0.45233251\n",
            "Iteration 105, loss = 0.45212218\n",
            "Iteration 106, loss = 0.45193268\n",
            "Iteration 107, loss = 0.45194607\n",
            "Iteration 108, loss = 0.45173550\n",
            "Iteration 109, loss = 0.45165201\n",
            "Iteration 110, loss = 0.45135092\n",
            "Iteration 111, loss = 0.45125748\n",
            "Iteration 112, loss = 0.45117172\n",
            "Iteration 113, loss = 0.45114563\n",
            "Iteration 114, loss = 0.45098037\n",
            "Iteration 115, loss = 0.45088380\n",
            "Iteration 116, loss = 0.45072579\n",
            "Iteration 117, loss = 0.45054396\n",
            "Iteration 118, loss = 0.45045398\n",
            "Iteration 119, loss = 0.45052998\n",
            "Iteration 120, loss = 0.45028014\n",
            "Iteration 121, loss = 0.45019850\n",
            "Iteration 122, loss = 0.45007605\n",
            "Iteration 123, loss = 0.45005179\n",
            "Iteration 124, loss = 0.44995427\n",
            "Iteration 125, loss = 0.45000040\n",
            "Iteration 126, loss = 0.44976077\n",
            "Iteration 127, loss = 0.44968417\n",
            "Iteration 128, loss = 0.44944316\n",
            "Iteration 129, loss = 0.44939323\n",
            "Iteration 130, loss = 0.44938233\n",
            "Iteration 131, loss = 0.44954238\n",
            "Iteration 132, loss = 0.44912185\n",
            "Iteration 133, loss = 0.44916641\n",
            "Iteration 134, loss = 0.44909677\n",
            "Iteration 135, loss = 0.44891893\n",
            "Iteration 136, loss = 0.44908202\n",
            "Iteration 137, loss = 0.44879488\n",
            "Iteration 138, loss = 0.44885042\n",
            "Iteration 139, loss = 0.44866276\n",
            "Iteration 140, loss = 0.44851848\n",
            "Iteration 141, loss = 0.44860134\n",
            "Iteration 142, loss = 0.44843398\n",
            "Iteration 143, loss = 0.44830765\n",
            "Iteration 144, loss = 0.44826004\n",
            "Iteration 145, loss = 0.44816126\n",
            "Iteration 146, loss = 0.44832016\n",
            "Iteration 147, loss = 0.44794962\n",
            "Iteration 148, loss = 0.44800826\n",
            "Iteration 149, loss = 0.44817898\n",
            "Iteration 150, loss = 0.44795854\n",
            "Iteration 151, loss = 0.44786786\n",
            "Iteration 152, loss = 0.44770079\n",
            "Iteration 153, loss = 0.44757365\n",
            "Iteration 154, loss = 0.44749765\n",
            "Iteration 155, loss = 0.44751529\n",
            "Iteration 156, loss = 0.44732510\n",
            "Iteration 157, loss = 0.44735208\n",
            "Iteration 158, loss = 0.44740216\n",
            "Iteration 159, loss = 0.44729796\n",
            "Iteration 160, loss = 0.44721526\n",
            "Iteration 161, loss = 0.44704152\n",
            "Iteration 162, loss = 0.44715228\n",
            "Iteration 163, loss = 0.44689568\n",
            "Iteration 164, loss = 0.44702405\n",
            "Iteration 165, loss = 0.44690896\n",
            "Iteration 166, loss = 0.44675517\n",
            "Iteration 167, loss = 0.44667440\n",
            "Iteration 168, loss = 0.44665214\n",
            "Iteration 169, loss = 0.44658856\n",
            "Iteration 170, loss = 0.44658682\n",
            "Iteration 171, loss = 0.44664053\n",
            "Iteration 172, loss = 0.44635918\n",
            "Iteration 173, loss = 0.44629702\n",
            "Iteration 174, loss = 0.44626007\n",
            "Iteration 175, loss = 0.44641532\n",
            "Iteration 176, loss = 0.44625037\n",
            "Iteration 177, loss = 0.44621545\n",
            "Iteration 178, loss = 0.44626022\n",
            "Iteration 179, loss = 0.44597323\n",
            "Iteration 180, loss = 0.44608727\n",
            "Iteration 181, loss = 0.44612190\n",
            "Iteration 182, loss = 0.44578221\n",
            "Iteration 183, loss = 0.44587561\n",
            "Iteration 184, loss = 0.44584546\n",
            "Iteration 185, loss = 0.44573407\n",
            "Iteration 186, loss = 0.44571356\n",
            "Iteration 187, loss = 0.44572429\n",
            "Iteration 188, loss = 0.44572564\n",
            "Iteration 189, loss = 0.44558038\n",
            "Iteration 190, loss = 0.44551796\n",
            "Iteration 191, loss = 0.44545124\n",
            "Iteration 192, loss = 0.44535955\n",
            "Iteration 193, loss = 0.44519535\n",
            "Iteration 194, loss = 0.44535655\n",
            "Iteration 195, loss = 0.44521183\n",
            "Iteration 196, loss = 0.44515968\n",
            "Iteration 197, loss = 0.44507415\n",
            "Iteration 198, loss = 0.44517381\n",
            "Iteration 199, loss = 0.44508438\n",
            "Iteration 200, loss = 0.44513522\n",
            "Iteration 1, loss = 0.70249092\n",
            "Iteration 2, loss = 0.69229768\n",
            "Iteration 3, loss = 0.68395167\n",
            "Iteration 4, loss = 0.67667346\n",
            "Iteration 5, loss = 0.66917989\n",
            "Iteration 6, loss = 0.66051828\n",
            "Iteration 7, loss = 0.65021934\n",
            "Iteration 8, loss = 0.63802844\n",
            "Iteration 9, loss = 0.62380292\n",
            "Iteration 10, loss = 0.60780031\n",
            "Iteration 11, loss = 0.59090155\n",
            "Iteration 12, loss = 0.57310710\n",
            "Iteration 13, loss = 0.55646173\n",
            "Iteration 14, loss = 0.54185714\n",
            "Iteration 15, loss = 0.52981086\n",
            "Iteration 16, loss = 0.51985495\n",
            "Iteration 17, loss = 0.51189976\n",
            "Iteration 18, loss = 0.50544040\n",
            "Iteration 19, loss = 0.50032677\n",
            "Iteration 20, loss = 0.49621324\n",
            "Iteration 21, loss = 0.49263966\n",
            "Iteration 22, loss = 0.48977447\n",
            "Iteration 23, loss = 0.48739532\n",
            "Iteration 24, loss = 0.48531757\n",
            "Iteration 25, loss = 0.48354471\n",
            "Iteration 26, loss = 0.48204050\n",
            "Iteration 27, loss = 0.48051179\n",
            "Iteration 28, loss = 0.47934757\n",
            "Iteration 29, loss = 0.47826545\n",
            "Iteration 30, loss = 0.47722955\n",
            "Iteration 31, loss = 0.47636692\n",
            "Iteration 32, loss = 0.47555322\n",
            "Iteration 33, loss = 0.47468980\n",
            "Iteration 34, loss = 0.47401056\n",
            "Iteration 35, loss = 0.47328809\n",
            "Iteration 36, loss = 0.47268717\n",
            "Iteration 37, loss = 0.47211152\n",
            "Iteration 38, loss = 0.47159707\n",
            "Iteration 39, loss = 0.47115395\n",
            "Iteration 40, loss = 0.47062115\n",
            "Iteration 41, loss = 0.47014608\n",
            "Iteration 42, loss = 0.46973167\n",
            "Iteration 43, loss = 0.46932536\n",
            "Iteration 44, loss = 0.46876957\n",
            "Iteration 45, loss = 0.46831108\n",
            "Iteration 46, loss = 0.46793222\n",
            "Iteration 47, loss = 0.46757248\n",
            "Iteration 48, loss = 0.46711053\n",
            "Iteration 49, loss = 0.46681784\n",
            "Iteration 50, loss = 0.46632250\n",
            "Iteration 51, loss = 0.46588078\n",
            "Iteration 52, loss = 0.46558101\n",
            "Iteration 53, loss = 0.46521079\n",
            "Iteration 54, loss = 0.46476653\n",
            "Iteration 55, loss = 0.46440655\n",
            "Iteration 56, loss = 0.46403505\n",
            "Iteration 57, loss = 0.46374641\n",
            "Iteration 58, loss = 0.46351760\n",
            "Iteration 59, loss = 0.46311177\n",
            "Iteration 60, loss = 0.46297020\n",
            "Iteration 61, loss = 0.46265937\n",
            "Iteration 62, loss = 0.46219354\n",
            "Iteration 63, loss = 0.46195025\n",
            "Iteration 64, loss = 0.46153979\n",
            "Iteration 65, loss = 0.46132680\n",
            "Iteration 66, loss = 0.46096325\n",
            "Iteration 67, loss = 0.46078548\n",
            "Iteration 68, loss = 0.46030904\n",
            "Iteration 69, loss = 0.46013029\n",
            "Iteration 70, loss = 0.45982038\n",
            "Iteration 71, loss = 0.45950535\n",
            "Iteration 72, loss = 0.45937875\n",
            "Iteration 73, loss = 0.45891554\n",
            "Iteration 74, loss = 0.45868157\n",
            "Iteration 75, loss = 0.45840908\n",
            "Iteration 76, loss = 0.45808396\n",
            "Iteration 77, loss = 0.45784336\n",
            "Iteration 78, loss = 0.45767823\n",
            "Iteration 79, loss = 0.45728635\n",
            "Iteration 80, loss = 0.45708229\n",
            "Iteration 81, loss = 0.45675332\n",
            "Iteration 82, loss = 0.45656519\n",
            "Iteration 83, loss = 0.45625227\n",
            "Iteration 84, loss = 0.45612584\n",
            "Iteration 85, loss = 0.45575178\n",
            "Iteration 86, loss = 0.45562943\n",
            "Iteration 87, loss = 0.45542744\n",
            "Iteration 88, loss = 0.45519786\n",
            "Iteration 89, loss = 0.45489021\n",
            "Iteration 90, loss = 0.45474518\n",
            "Iteration 91, loss = 0.45459175\n",
            "Iteration 92, loss = 0.45444511\n",
            "Iteration 93, loss = 0.45421677\n",
            "Iteration 94, loss = 0.45409978\n",
            "Iteration 95, loss = 0.45386724\n",
            "Iteration 96, loss = 0.45395679\n",
            "Iteration 97, loss = 0.45350575\n",
            "Iteration 98, loss = 0.45342420\n",
            "Iteration 99, loss = 0.45326976\n",
            "Iteration 100, loss = 0.45314562\n",
            "Iteration 101, loss = 0.45304973\n",
            "Iteration 102, loss = 0.45272331\n",
            "Iteration 103, loss = 0.45260793\n",
            "Iteration 104, loss = 0.45252639\n",
            "Iteration 105, loss = 0.45239454\n",
            "Iteration 106, loss = 0.45214372\n",
            "Iteration 107, loss = 0.45208849\n",
            "Iteration 108, loss = 0.45193247\n",
            "Iteration 109, loss = 0.45184715\n",
            "Iteration 110, loss = 0.45170192\n",
            "Iteration 111, loss = 0.45155652\n",
            "Iteration 112, loss = 0.45150297\n",
            "Iteration 113, loss = 0.45147155\n",
            "Iteration 114, loss = 0.45130087\n",
            "Iteration 115, loss = 0.45131193\n",
            "Iteration 116, loss = 0.45110752\n",
            "Iteration 117, loss = 0.45098826\n",
            "Iteration 118, loss = 0.45094690\n",
            "Iteration 119, loss = 0.45088114\n",
            "Iteration 120, loss = 0.45066224\n",
            "Iteration 121, loss = 0.45066254\n",
            "Iteration 122, loss = 0.45050771\n",
            "Iteration 123, loss = 0.45051757\n",
            "Iteration 124, loss = 0.45046792\n",
            "Iteration 125, loss = 0.45041844\n",
            "Iteration 126, loss = 0.45024908\n",
            "Iteration 127, loss = 0.45015007\n",
            "Iteration 128, loss = 0.44993230\n",
            "Iteration 129, loss = 0.44985784\n",
            "Iteration 130, loss = 0.44993012\n",
            "Iteration 131, loss = 0.44991795\n",
            "Iteration 132, loss = 0.44961745\n",
            "Iteration 133, loss = 0.44961934\n",
            "Iteration 134, loss = 0.44966806\n",
            "Iteration 135, loss = 0.44941910\n",
            "Iteration 136, loss = 0.44957324\n",
            "Iteration 137, loss = 0.44925645\n",
            "Iteration 138, loss = 0.44930294\n",
            "Iteration 139, loss = 0.44915909\n",
            "Iteration 140, loss = 0.44910505\n",
            "Iteration 141, loss = 0.44910697\n",
            "Iteration 142, loss = 0.44896947\n",
            "Iteration 143, loss = 0.44882341\n",
            "Iteration 144, loss = 0.44885283\n",
            "Iteration 145, loss = 0.44867243\n",
            "Iteration 146, loss = 0.44867146\n",
            "Iteration 147, loss = 0.44849239\n",
            "Iteration 148, loss = 0.44861009\n",
            "Iteration 149, loss = 0.44873293\n",
            "Iteration 150, loss = 0.44840051\n",
            "Iteration 151, loss = 0.44835922\n",
            "Iteration 152, loss = 0.44829624\n",
            "Iteration 153, loss = 0.44816965\n",
            "Iteration 154, loss = 0.44811006\n",
            "Iteration 155, loss = 0.44810983\n",
            "Iteration 156, loss = 0.44792765\n",
            "Iteration 157, loss = 0.44801941\n",
            "Iteration 158, loss = 0.44791468\n",
            "Iteration 159, loss = 0.44782979\n",
            "Iteration 160, loss = 0.44785272\n",
            "Iteration 161, loss = 0.44764692\n",
            "Iteration 162, loss = 0.44775485\n",
            "Iteration 163, loss = 0.44758264\n",
            "Iteration 164, loss = 0.44766413\n",
            "Iteration 165, loss = 0.44759469\n",
            "Iteration 166, loss = 0.44739273\n",
            "Iteration 167, loss = 0.44736220\n",
            "Iteration 168, loss = 0.44725125\n",
            "Iteration 169, loss = 0.44725479\n",
            "Iteration 170, loss = 0.44713714\n",
            "Iteration 171, loss = 0.44730813\n",
            "Iteration 172, loss = 0.44697934\n",
            "Iteration 173, loss = 0.44705910\n",
            "Iteration 174, loss = 0.44698241\n",
            "Iteration 175, loss = 0.44701530\n",
            "Iteration 176, loss = 0.44686217\n",
            "Iteration 177, loss = 0.44679574\n",
            "Iteration 178, loss = 0.44704309\n",
            "Iteration 179, loss = 0.44672805\n",
            "Iteration 180, loss = 0.44664545\n",
            "Iteration 181, loss = 0.44670272\n",
            "Iteration 182, loss = 0.44644118\n",
            "Iteration 183, loss = 0.44660658\n",
            "Iteration 184, loss = 0.44639349\n",
            "Iteration 185, loss = 0.44632531\n",
            "Iteration 186, loss = 0.44640415\n",
            "Iteration 187, loss = 0.44636977\n",
            "Iteration 188, loss = 0.44631886\n",
            "Iteration 189, loss = 0.44628784\n",
            "Iteration 190, loss = 0.44619052\n",
            "Iteration 191, loss = 0.44619325\n",
            "Iteration 192, loss = 0.44607052\n",
            "Iteration 193, loss = 0.44590812\n",
            "Iteration 194, loss = 0.44602563\n",
            "Iteration 195, loss = 0.44591836\n",
            "Iteration 196, loss = 0.44593657\n",
            "Iteration 197, loss = 0.44574082\n",
            "Iteration 198, loss = 0.44578115\n",
            "Iteration 199, loss = 0.44589586\n",
            "Iteration 200, loss = 0.44593715\n",
            "Iteration 1, loss = 0.70264997\n",
            "Iteration 2, loss = 0.69233630\n",
            "Iteration 3, loss = 0.68397845\n",
            "Iteration 4, loss = 0.67673097\n",
            "Iteration 5, loss = 0.66927065\n",
            "Iteration 6, loss = 0.66075806\n",
            "Iteration 7, loss = 0.65064512\n",
            "Iteration 8, loss = 0.63867852\n",
            "Iteration 9, loss = 0.62505648\n",
            "Iteration 10, loss = 0.60994819\n",
            "Iteration 11, loss = 0.59424490\n",
            "Iteration 12, loss = 0.57807272\n",
            "Iteration 13, loss = 0.56192605\n",
            "Iteration 14, loss = 0.54710431\n",
            "Iteration 15, loss = 0.53442830\n",
            "Iteration 16, loss = 0.52381623\n",
            "Iteration 17, loss = 0.51529121\n",
            "Iteration 18, loss = 0.50836223\n",
            "Iteration 19, loss = 0.50276992\n",
            "Iteration 20, loss = 0.49813803\n",
            "Iteration 21, loss = 0.49426192\n",
            "Iteration 22, loss = 0.49116205\n",
            "Iteration 23, loss = 0.48852077\n",
            "Iteration 24, loss = 0.48625689\n",
            "Iteration 25, loss = 0.48416115\n",
            "Iteration 26, loss = 0.48254011\n",
            "Iteration 27, loss = 0.48084291\n",
            "Iteration 28, loss = 0.47954766\n",
            "Iteration 29, loss = 0.47832640\n",
            "Iteration 30, loss = 0.47718550\n",
            "Iteration 31, loss = 0.47624525\n",
            "Iteration 32, loss = 0.47532808\n",
            "Iteration 33, loss = 0.47443811\n",
            "Iteration 34, loss = 0.47361714\n",
            "Iteration 35, loss = 0.47277241\n",
            "Iteration 36, loss = 0.47210462\n",
            "Iteration 37, loss = 0.47143751\n",
            "Iteration 38, loss = 0.47078783\n",
            "Iteration 39, loss = 0.47021391\n",
            "Iteration 40, loss = 0.46958429\n",
            "Iteration 41, loss = 0.46899386\n",
            "Iteration 42, loss = 0.46844321\n",
            "Iteration 43, loss = 0.46794849\n",
            "Iteration 44, loss = 0.46734351\n",
            "Iteration 45, loss = 0.46679833\n",
            "Iteration 46, loss = 0.46633483\n",
            "Iteration 47, loss = 0.46587426\n",
            "Iteration 48, loss = 0.46537073\n",
            "Iteration 49, loss = 0.46493936\n",
            "Iteration 50, loss = 0.46443078\n",
            "Iteration 51, loss = 0.46394975\n",
            "Iteration 52, loss = 0.46356534\n",
            "Iteration 53, loss = 0.46329900\n",
            "Iteration 54, loss = 0.46267338\n",
            "Iteration 55, loss = 0.46226384\n",
            "Iteration 56, loss = 0.46185835\n",
            "Iteration 57, loss = 0.46151190\n",
            "Iteration 58, loss = 0.46120554\n",
            "Iteration 59, loss = 0.46082601\n",
            "Iteration 60, loss = 0.46061671\n",
            "Iteration 61, loss = 0.46016410\n",
            "Iteration 62, loss = 0.45967919\n",
            "Iteration 63, loss = 0.45936185\n",
            "Iteration 64, loss = 0.45899719\n",
            "Iteration 65, loss = 0.45866015\n",
            "Iteration 66, loss = 0.45835071\n",
            "Iteration 67, loss = 0.45800895\n",
            "Iteration 68, loss = 0.45767098\n",
            "Iteration 69, loss = 0.45743202\n",
            "Iteration 70, loss = 0.45703277\n",
            "Iteration 71, loss = 0.45670279\n",
            "Iteration 72, loss = 0.45655800\n",
            "Iteration 73, loss = 0.45605235\n",
            "Iteration 74, loss = 0.45572173\n",
            "Iteration 75, loss = 0.45548408\n",
            "Iteration 76, loss = 0.45514092\n",
            "Iteration 77, loss = 0.45496945\n",
            "Iteration 78, loss = 0.45470083\n",
            "Iteration 79, loss = 0.45431186\n",
            "Iteration 80, loss = 0.45399761\n",
            "Iteration 81, loss = 0.45369860\n",
            "Iteration 82, loss = 0.45346374\n",
            "Iteration 83, loss = 0.45311590\n",
            "Iteration 84, loss = 0.45296360\n",
            "Iteration 85, loss = 0.45257236\n",
            "Iteration 86, loss = 0.45253578\n",
            "Iteration 87, loss = 0.45220641\n",
            "Iteration 88, loss = 0.45195557\n",
            "Iteration 89, loss = 0.45162752\n",
            "Iteration 90, loss = 0.45141698\n",
            "Iteration 91, loss = 0.45127251\n",
            "Iteration 92, loss = 0.45109491\n",
            "Iteration 93, loss = 0.45083069\n",
            "Iteration 94, loss = 0.45069571\n",
            "Iteration 95, loss = 0.45050200\n",
            "Iteration 96, loss = 0.45052182\n",
            "Iteration 97, loss = 0.45009065\n",
            "Iteration 98, loss = 0.45005918\n",
            "Iteration 99, loss = 0.44980237\n",
            "Iteration 100, loss = 0.44980075\n",
            "Iteration 101, loss = 0.44963587\n",
            "Iteration 102, loss = 0.44922876\n",
            "Iteration 103, loss = 0.44913543\n",
            "Iteration 104, loss = 0.44891766\n",
            "Iteration 105, loss = 0.44882873\n",
            "Iteration 106, loss = 0.44862108\n",
            "Iteration 107, loss = 0.44850989\n",
            "Iteration 108, loss = 0.44831377\n",
            "Iteration 109, loss = 0.44830734\n",
            "Iteration 110, loss = 0.44808174\n",
            "Iteration 111, loss = 0.44793315\n",
            "Iteration 112, loss = 0.44779771\n",
            "Iteration 113, loss = 0.44774620\n",
            "Iteration 114, loss = 0.44752104\n",
            "Iteration 115, loss = 0.44753472\n",
            "Iteration 116, loss = 0.44731587\n",
            "Iteration 117, loss = 0.44721032\n",
            "Iteration 118, loss = 0.44721866\n",
            "Iteration 119, loss = 0.44708676\n",
            "Iteration 120, loss = 0.44702032\n",
            "Iteration 121, loss = 0.44687736\n",
            "Iteration 122, loss = 0.44669794\n",
            "Iteration 123, loss = 0.44668794\n",
            "Iteration 124, loss = 0.44654983\n",
            "Iteration 125, loss = 0.44652503\n",
            "Iteration 126, loss = 0.44639898\n",
            "Iteration 127, loss = 0.44637953\n",
            "Iteration 128, loss = 0.44627364\n",
            "Iteration 129, loss = 0.44614201\n",
            "Iteration 130, loss = 0.44615893\n",
            "Iteration 131, loss = 0.44613926\n",
            "Iteration 132, loss = 0.44574935\n",
            "Iteration 133, loss = 0.44572253\n",
            "Iteration 134, loss = 0.44582139\n",
            "Iteration 135, loss = 0.44554385\n",
            "Iteration 136, loss = 0.44570544\n",
            "Iteration 137, loss = 0.44541283\n",
            "Iteration 138, loss = 0.44544979\n",
            "Iteration 139, loss = 0.44528601\n",
            "Iteration 140, loss = 0.44533318\n",
            "Iteration 141, loss = 0.44518958\n",
            "Iteration 142, loss = 0.44513703\n",
            "Iteration 143, loss = 0.44501405\n",
            "Iteration 144, loss = 0.44503803\n",
            "Iteration 145, loss = 0.44487405\n",
            "Iteration 146, loss = 0.44479330\n",
            "Iteration 147, loss = 0.44468171\n",
            "Iteration 148, loss = 0.44483192\n",
            "Iteration 149, loss = 0.44472046\n",
            "Iteration 150, loss = 0.44457854\n",
            "Iteration 151, loss = 0.44455508\n",
            "Iteration 152, loss = 0.44440265\n",
            "Iteration 153, loss = 0.44438235\n",
            "Iteration 154, loss = 0.44427327\n",
            "Iteration 155, loss = 0.44422231\n",
            "Iteration 156, loss = 0.44411086\n",
            "Iteration 157, loss = 0.44423074\n",
            "Iteration 158, loss = 0.44402849\n",
            "Iteration 159, loss = 0.44395656\n",
            "Iteration 160, loss = 0.44389967\n",
            "Iteration 161, loss = 0.44387871\n",
            "Iteration 162, loss = 0.44385058\n",
            "Iteration 163, loss = 0.44386464\n",
            "Iteration 164, loss = 0.44374975\n",
            "Iteration 165, loss = 0.44371500\n",
            "Iteration 166, loss = 0.44359566\n",
            "Iteration 167, loss = 0.44348367\n",
            "Iteration 168, loss = 0.44348306\n",
            "Iteration 169, loss = 0.44351803\n",
            "Iteration 170, loss = 0.44336691\n",
            "Iteration 171, loss = 0.44357261\n",
            "Iteration 172, loss = 0.44319129\n",
            "Iteration 173, loss = 0.44321658\n",
            "Iteration 174, loss = 0.44318088\n",
            "Iteration 175, loss = 0.44315793\n",
            "Iteration 176, loss = 0.44306137\n",
            "Iteration 177, loss = 0.44297575\n",
            "Iteration 178, loss = 0.44317531\n",
            "Iteration 179, loss = 0.44297122\n",
            "Iteration 180, loss = 0.44285415\n",
            "Iteration 181, loss = 0.44282533\n",
            "Iteration 182, loss = 0.44265935\n",
            "Iteration 183, loss = 0.44290938\n",
            "Iteration 184, loss = 0.44265843\n",
            "Iteration 185, loss = 0.44255497\n",
            "Iteration 186, loss = 0.44261891\n",
            "Iteration 187, loss = 0.44254656\n",
            "Iteration 188, loss = 0.44242115\n",
            "Iteration 189, loss = 0.44251861\n",
            "Iteration 190, loss = 0.44248060\n",
            "Iteration 191, loss = 0.44250964\n",
            "Iteration 192, loss = 0.44227728\n",
            "Iteration 193, loss = 0.44217933\n",
            "Iteration 194, loss = 0.44229896\n",
            "Iteration 195, loss = 0.44214120\n",
            "Iteration 196, loss = 0.44217610\n",
            "Iteration 197, loss = 0.44198125\n",
            "Iteration 198, loss = 0.44195143\n",
            "Iteration 199, loss = 0.44213899\n",
            "Iteration 200, loss = 0.44218313\n",
            "Iteration 1, loss = 0.70318808\n",
            "Iteration 2, loss = 0.69292251\n",
            "Iteration 3, loss = 0.68461648\n",
            "Iteration 4, loss = 0.67753485\n",
            "Iteration 5, loss = 0.67037820\n",
            "Iteration 6, loss = 0.66225351\n",
            "Iteration 7, loss = 0.65254136\n",
            "Iteration 8, loss = 0.64106397\n",
            "Iteration 9, loss = 0.62789109\n",
            "Iteration 10, loss = 0.61303587\n",
            "Iteration 11, loss = 0.59756717\n",
            "Iteration 12, loss = 0.58122205\n",
            "Iteration 13, loss = 0.56491127\n",
            "Iteration 14, loss = 0.55005414\n",
            "Iteration 15, loss = 0.53726492\n",
            "Iteration 16, loss = 0.52674041\n",
            "Iteration 17, loss = 0.51807733\n",
            "Iteration 18, loss = 0.51100462\n",
            "Iteration 19, loss = 0.50532315\n",
            "Iteration 20, loss = 0.50055846\n",
            "Iteration 21, loss = 0.49666884\n",
            "Iteration 22, loss = 0.49346675\n",
            "Iteration 23, loss = 0.49071981\n",
            "Iteration 24, loss = 0.48844843\n",
            "Iteration 25, loss = 0.48642901\n",
            "Iteration 26, loss = 0.48475629\n",
            "Iteration 27, loss = 0.48297414\n",
            "Iteration 28, loss = 0.48161549\n",
            "Iteration 29, loss = 0.48036910\n",
            "Iteration 30, loss = 0.47920728\n",
            "Iteration 31, loss = 0.47832415\n",
            "Iteration 32, loss = 0.47737879\n",
            "Iteration 33, loss = 0.47651416\n",
            "Iteration 34, loss = 0.47569077\n",
            "Iteration 35, loss = 0.47491571\n",
            "Iteration 36, loss = 0.47433281\n",
            "Iteration 37, loss = 0.47361711\n",
            "Iteration 38, loss = 0.47293071\n",
            "Iteration 39, loss = 0.47249737\n",
            "Iteration 40, loss = 0.47189459\n",
            "Iteration 41, loss = 0.47132560\n",
            "Iteration 42, loss = 0.47080006\n",
            "Iteration 43, loss = 0.47033433\n",
            "Iteration 44, loss = 0.46978050\n",
            "Iteration 45, loss = 0.46926569\n",
            "Iteration 46, loss = 0.46891315\n",
            "Iteration 47, loss = 0.46839515\n",
            "Iteration 48, loss = 0.46790116\n",
            "Iteration 49, loss = 0.46752297\n",
            "Iteration 50, loss = 0.46708633\n",
            "Iteration 51, loss = 0.46666337\n",
            "Iteration 52, loss = 0.46630401\n",
            "Iteration 53, loss = 0.46608713\n",
            "Iteration 54, loss = 0.46560533\n",
            "Iteration 55, loss = 0.46526210\n",
            "Iteration 56, loss = 0.46489590\n",
            "Iteration 57, loss = 0.46469167\n",
            "Iteration 58, loss = 0.46442527\n",
            "Iteration 59, loss = 0.46403349\n",
            "Iteration 60, loss = 0.46392990\n",
            "Iteration 61, loss = 0.46338641\n",
            "Iteration 62, loss = 0.46298446\n",
            "Iteration 63, loss = 0.46263662\n",
            "Iteration 64, loss = 0.46241059\n",
            "Iteration 65, loss = 0.46207793\n",
            "Iteration 66, loss = 0.46175033\n",
            "Iteration 67, loss = 0.46143898\n",
            "Iteration 68, loss = 0.46117873\n",
            "Iteration 69, loss = 0.46094240\n",
            "Iteration 70, loss = 0.46056258\n",
            "Iteration 71, loss = 0.46032495\n",
            "Iteration 72, loss = 0.46029313\n",
            "Iteration 73, loss = 0.45983664\n",
            "Iteration 74, loss = 0.45961121\n",
            "Iteration 75, loss = 0.45936970\n",
            "Iteration 76, loss = 0.45912336\n",
            "Iteration 77, loss = 0.45889505\n",
            "Iteration 78, loss = 0.45870637\n",
            "Iteration 79, loss = 0.45840666\n",
            "Iteration 80, loss = 0.45813831\n",
            "Iteration 81, loss = 0.45786382\n",
            "Iteration 82, loss = 0.45769873\n",
            "Iteration 83, loss = 0.45738612\n",
            "Iteration 84, loss = 0.45723088\n",
            "Iteration 85, loss = 0.45695233\n",
            "Iteration 86, loss = 0.45703764\n",
            "Iteration 87, loss = 0.45656292\n",
            "Iteration 88, loss = 0.45652118\n",
            "Iteration 89, loss = 0.45615218\n",
            "Iteration 90, loss = 0.45594117\n",
            "Iteration 91, loss = 0.45581313\n",
            "Iteration 92, loss = 0.45564364\n",
            "Iteration 93, loss = 0.45544639\n",
            "Iteration 94, loss = 0.45538179\n",
            "Iteration 95, loss = 0.45514625\n",
            "Iteration 96, loss = 0.45512100\n",
            "Iteration 97, loss = 0.45476074\n",
            "Iteration 98, loss = 0.45481617\n",
            "Iteration 99, loss = 0.45448673\n",
            "Iteration 100, loss = 0.45444737\n",
            "Iteration 101, loss = 0.45428614\n",
            "Iteration 102, loss = 0.45406511\n",
            "Iteration 103, loss = 0.45408051\n",
            "Iteration 104, loss = 0.45378838\n",
            "Iteration 105, loss = 0.45370932\n",
            "Iteration 106, loss = 0.45361433\n",
            "Iteration 107, loss = 0.45350951\n",
            "Iteration 108, loss = 0.45319749\n",
            "Iteration 109, loss = 0.45321042\n",
            "Iteration 110, loss = 0.45301723\n",
            "Iteration 111, loss = 0.45285591\n",
            "Iteration 112, loss = 0.45267564\n",
            "Iteration 113, loss = 0.45263433\n",
            "Iteration 114, loss = 0.45246672\n",
            "Iteration 115, loss = 0.45242205\n",
            "Iteration 116, loss = 0.45223327\n",
            "Iteration 117, loss = 0.45214783\n",
            "Iteration 118, loss = 0.45212908\n",
            "Iteration 119, loss = 0.45206494\n",
            "Iteration 120, loss = 0.45193672\n",
            "Iteration 121, loss = 0.45176669\n",
            "Iteration 122, loss = 0.45154706\n",
            "Iteration 123, loss = 0.45151770\n",
            "Iteration 124, loss = 0.45143712\n",
            "Iteration 125, loss = 0.45136879\n",
            "Iteration 126, loss = 0.45122227\n",
            "Iteration 127, loss = 0.45115035\n",
            "Iteration 128, loss = 0.45103244\n",
            "Iteration 129, loss = 0.45092839\n",
            "Iteration 130, loss = 0.45080689\n",
            "Iteration 131, loss = 0.45079960\n",
            "Iteration 132, loss = 0.45042122\n",
            "Iteration 133, loss = 0.45056308\n",
            "Iteration 134, loss = 0.45049829\n",
            "Iteration 135, loss = 0.45030420\n",
            "Iteration 136, loss = 0.45039311\n",
            "Iteration 137, loss = 0.45009309\n",
            "Iteration 138, loss = 0.45013290\n",
            "Iteration 139, loss = 0.44999994\n",
            "Iteration 140, loss = 0.45002450\n",
            "Iteration 141, loss = 0.44982423\n",
            "Iteration 142, loss = 0.44977308\n",
            "Iteration 143, loss = 0.44975364\n",
            "Iteration 144, loss = 0.44968239\n",
            "Iteration 145, loss = 0.44952106\n",
            "Iteration 146, loss = 0.44941934\n",
            "Iteration 147, loss = 0.44941379\n",
            "Iteration 148, loss = 0.44958542\n",
            "Iteration 149, loss = 0.44925207\n",
            "Iteration 150, loss = 0.44911733\n",
            "Iteration 151, loss = 0.44914588\n",
            "Iteration 152, loss = 0.44898203\n",
            "Iteration 153, loss = 0.44887849\n",
            "Iteration 154, loss = 0.44878175\n",
            "Iteration 155, loss = 0.44868329\n",
            "Iteration 156, loss = 0.44859392\n",
            "Iteration 157, loss = 0.44860295\n",
            "Iteration 158, loss = 0.44842758\n",
            "Iteration 159, loss = 0.44844189\n",
            "Iteration 160, loss = 0.44835018\n",
            "Iteration 161, loss = 0.44834747\n",
            "Iteration 162, loss = 0.44820652\n",
            "Iteration 163, loss = 0.44825698\n",
            "Iteration 164, loss = 0.44813507\n",
            "Iteration 165, loss = 0.44817926\n",
            "Iteration 166, loss = 0.44795110\n",
            "Iteration 167, loss = 0.44787205\n",
            "Iteration 168, loss = 0.44783044\n",
            "Iteration 169, loss = 0.44772898\n",
            "Iteration 170, loss = 0.44767267\n",
            "Iteration 171, loss = 0.44780530\n",
            "Iteration 172, loss = 0.44750827\n",
            "Iteration 173, loss = 0.44740753\n",
            "Iteration 174, loss = 0.44745099\n",
            "Iteration 175, loss = 0.44730787\n",
            "Iteration 176, loss = 0.44734736\n",
            "Iteration 177, loss = 0.44711413\n",
            "Iteration 178, loss = 0.44764596\n",
            "Iteration 179, loss = 0.44718530\n",
            "Iteration 180, loss = 0.44708233\n",
            "Iteration 181, loss = 0.44694591\n",
            "Iteration 182, loss = 0.44684179\n",
            "Iteration 183, loss = 0.44692966\n",
            "Iteration 184, loss = 0.44681065\n",
            "Iteration 185, loss = 0.44685542\n",
            "Iteration 186, loss = 0.44676475\n",
            "Iteration 187, loss = 0.44675480\n",
            "Iteration 188, loss = 0.44658514\n",
            "Iteration 189, loss = 0.44675567\n",
            "Iteration 190, loss = 0.44653455\n",
            "Iteration 191, loss = 0.44652904\n",
            "Iteration 192, loss = 0.44644067\n",
            "Iteration 193, loss = 0.44630801\n",
            "Iteration 194, loss = 0.44645095\n",
            "Iteration 195, loss = 0.44632809\n",
            "Iteration 196, loss = 0.44637167\n",
            "Iteration 197, loss = 0.44615091\n",
            "Iteration 198, loss = 0.44614544\n",
            "Iteration 199, loss = 0.44641007\n",
            "Iteration 200, loss = 0.44629552\n",
            "Iteration 1, loss = 0.70229615\n",
            "Iteration 2, loss = 0.69209873\n",
            "Iteration 3, loss = 0.68371232\n",
            "Iteration 4, loss = 0.67650670\n",
            "Iteration 5, loss = 0.66911327\n",
            "Iteration 6, loss = 0.66057616\n",
            "Iteration 7, loss = 0.65042568\n",
            "Iteration 8, loss = 0.63839134\n",
            "Iteration 9, loss = 0.62467574\n",
            "Iteration 10, loss = 0.60939305\n",
            "Iteration 11, loss = 0.59352287\n",
            "Iteration 12, loss = 0.57673111\n",
            "Iteration 13, loss = 0.56036259\n",
            "Iteration 14, loss = 0.54549922\n",
            "Iteration 15, loss = 0.53289015\n",
            "Iteration 16, loss = 0.52246560\n",
            "Iteration 17, loss = 0.51399956\n",
            "Iteration 18, loss = 0.50715930\n",
            "Iteration 19, loss = 0.50172216\n",
            "Iteration 20, loss = 0.49706883\n",
            "Iteration 21, loss = 0.49335447\n",
            "Iteration 22, loss = 0.49023772\n",
            "Iteration 23, loss = 0.48761538\n",
            "Iteration 24, loss = 0.48537245\n",
            "Iteration 25, loss = 0.48339481\n",
            "Iteration 26, loss = 0.48186657\n",
            "Iteration 27, loss = 0.48016572\n",
            "Iteration 28, loss = 0.47883200\n",
            "Iteration 29, loss = 0.47762220\n",
            "Iteration 30, loss = 0.47652908\n",
            "Iteration 31, loss = 0.47566773\n",
            "Iteration 32, loss = 0.47470309\n",
            "Iteration 33, loss = 0.47386480\n",
            "Iteration 34, loss = 0.47305865\n",
            "Iteration 35, loss = 0.47233392\n",
            "Iteration 36, loss = 0.47173583\n",
            "Iteration 37, loss = 0.47105590\n",
            "Iteration 38, loss = 0.47031336\n",
            "Iteration 39, loss = 0.46990478\n",
            "Iteration 40, loss = 0.46923230\n",
            "Iteration 41, loss = 0.46873436\n",
            "Iteration 42, loss = 0.46822289\n",
            "Iteration 43, loss = 0.46777780\n",
            "Iteration 44, loss = 0.46722418\n",
            "Iteration 45, loss = 0.46679175\n",
            "Iteration 46, loss = 0.46642816\n",
            "Iteration 47, loss = 0.46592100\n",
            "Iteration 48, loss = 0.46544196\n",
            "Iteration 49, loss = 0.46498399\n",
            "Iteration 50, loss = 0.46457501\n",
            "Iteration 51, loss = 0.46415239\n",
            "Iteration 52, loss = 0.46379066\n",
            "Iteration 53, loss = 0.46348781\n",
            "Iteration 54, loss = 0.46302658\n",
            "Iteration 55, loss = 0.46264557\n",
            "Iteration 56, loss = 0.46235748\n",
            "Iteration 57, loss = 0.46209264\n",
            "Iteration 58, loss = 0.46185009\n",
            "Iteration 59, loss = 0.46140296\n",
            "Iteration 60, loss = 0.46127492\n",
            "Iteration 61, loss = 0.46067559\n",
            "Iteration 62, loss = 0.46030558\n",
            "Iteration 63, loss = 0.45993961\n",
            "Iteration 64, loss = 0.45970864\n",
            "Iteration 65, loss = 0.45944181\n",
            "Iteration 66, loss = 0.45914401\n",
            "Iteration 67, loss = 0.45884539\n",
            "Iteration 68, loss = 0.45856466\n",
            "Iteration 69, loss = 0.45827155\n",
            "Iteration 70, loss = 0.45795559\n",
            "Iteration 71, loss = 0.45766872\n",
            "Iteration 72, loss = 0.45760192\n",
            "Iteration 73, loss = 0.45719803\n",
            "Iteration 74, loss = 0.45698205\n",
            "Iteration 75, loss = 0.45668752\n",
            "Iteration 76, loss = 0.45643973\n",
            "Iteration 77, loss = 0.45616569\n",
            "Iteration 78, loss = 0.45589894\n",
            "Iteration 79, loss = 0.45559547\n",
            "Iteration 80, loss = 0.45536702\n",
            "Iteration 81, loss = 0.45509436\n",
            "Iteration 82, loss = 0.45486547\n",
            "Iteration 83, loss = 0.45457248\n",
            "Iteration 84, loss = 0.45437759\n",
            "Iteration 85, loss = 0.45402293\n",
            "Iteration 86, loss = 0.45412550\n",
            "Iteration 87, loss = 0.45360463\n",
            "Iteration 88, loss = 0.45357920\n",
            "Iteration 89, loss = 0.45324758\n",
            "Iteration 90, loss = 0.45283676\n",
            "Iteration 91, loss = 0.45270321\n",
            "Iteration 92, loss = 0.45254238\n",
            "Iteration 93, loss = 0.45224996\n",
            "Iteration 94, loss = 0.45208306\n",
            "Iteration 95, loss = 0.45183768\n",
            "Iteration 96, loss = 0.45176566\n",
            "Iteration 97, loss = 0.45149565\n",
            "Iteration 98, loss = 0.45150982\n",
            "Iteration 99, loss = 0.45119408\n",
            "Iteration 100, loss = 0.45114791\n",
            "Iteration 101, loss = 0.45084328\n",
            "Iteration 102, loss = 0.45066903\n",
            "Iteration 103, loss = 0.45051650\n",
            "Iteration 104, loss = 0.45029061\n",
            "Iteration 105, loss = 0.45029530\n",
            "Iteration 106, loss = 0.45004220\n",
            "Iteration 107, loss = 0.44999660\n",
            "Iteration 108, loss = 0.44964237\n",
            "Iteration 109, loss = 0.44956799\n",
            "Iteration 110, loss = 0.44943903\n",
            "Iteration 111, loss = 0.44924075\n",
            "Iteration 112, loss = 0.44900906\n",
            "Iteration 113, loss = 0.44900375\n",
            "Iteration 114, loss = 0.44886791\n",
            "Iteration 115, loss = 0.44883265\n",
            "Iteration 116, loss = 0.44853159\n",
            "Iteration 117, loss = 0.44845880\n",
            "Iteration 118, loss = 0.44835849\n",
            "Iteration 119, loss = 0.44827649\n",
            "Iteration 120, loss = 0.44821989\n",
            "Iteration 121, loss = 0.44811043\n",
            "Iteration 122, loss = 0.44795814\n",
            "Iteration 123, loss = 0.44774755\n",
            "Iteration 124, loss = 0.44769210\n",
            "Iteration 125, loss = 0.44764139\n",
            "Iteration 126, loss = 0.44752047\n",
            "Iteration 127, loss = 0.44741256\n",
            "Iteration 128, loss = 0.44719597\n",
            "Iteration 129, loss = 0.44725862\n",
            "Iteration 130, loss = 0.44706340\n",
            "Iteration 131, loss = 0.44718158\n",
            "Iteration 132, loss = 0.44668051\n",
            "Iteration 133, loss = 0.44677105\n",
            "Iteration 134, loss = 0.44680367\n",
            "Iteration 135, loss = 0.44660147\n",
            "Iteration 136, loss = 0.44674196\n",
            "Iteration 137, loss = 0.44639336\n",
            "Iteration 138, loss = 0.44641906\n",
            "Iteration 139, loss = 0.44618946\n",
            "Iteration 140, loss = 0.44636364\n",
            "Iteration 141, loss = 0.44608626\n",
            "Iteration 142, loss = 0.44605870\n",
            "Iteration 143, loss = 0.44614705\n",
            "Iteration 144, loss = 0.44604423\n",
            "Iteration 145, loss = 0.44579887\n",
            "Iteration 146, loss = 0.44575162\n",
            "Iteration 147, loss = 0.44578572\n",
            "Iteration 148, loss = 0.44595090\n",
            "Iteration 149, loss = 0.44555893\n",
            "Iteration 150, loss = 0.44550489\n",
            "Iteration 151, loss = 0.44555818\n",
            "Iteration 152, loss = 0.44532710\n",
            "Iteration 153, loss = 0.44531920\n",
            "Iteration 154, loss = 0.44517666\n",
            "Iteration 155, loss = 0.44521308\n",
            "Iteration 156, loss = 0.44510291\n",
            "Iteration 157, loss = 0.44505082\n",
            "Iteration 158, loss = 0.44500818\n",
            "Iteration 159, loss = 0.44493655\n",
            "Iteration 160, loss = 0.44489801\n",
            "Iteration 161, loss = 0.44482293\n",
            "Iteration 162, loss = 0.44473278\n",
            "Iteration 163, loss = 0.44482582\n",
            "Iteration 164, loss = 0.44469492\n",
            "Iteration 165, loss = 0.44482691\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 166, loss = 0.44435884\n",
            "Iteration 167, loss = 0.44430342\n",
            "Iteration 168, loss = 0.44429054\n",
            "Iteration 169, loss = 0.44431206\n",
            "Iteration 170, loss = 0.44428025\n",
            "Iteration 171, loss = 0.44429750\n",
            "Iteration 172, loss = 0.44425662\n",
            "Iteration 173, loss = 0.44423155\n",
            "Iteration 174, loss = 0.44424521\n",
            "Iteration 175, loss = 0.44419864\n",
            "Iteration 176, loss = 0.44421043\n",
            "Iteration 177, loss = 0.44418320\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 178, loss = 0.44415745\n",
            "Iteration 179, loss = 0.44413667\n",
            "Iteration 180, loss = 0.44413417\n",
            "Iteration 181, loss = 0.44413102\n",
            "Iteration 182, loss = 0.44412820\n",
            "Iteration 183, loss = 0.44413628\n",
            "Iteration 184, loss = 0.44411981\n",
            "Iteration 185, loss = 0.44412337\n",
            "Iteration 186, loss = 0.44412279\n",
            "Iteration 187, loss = 0.44411975\n",
            "Iteration 188, loss = 0.44412544\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 189, loss = 0.44410601\n",
            "Iteration 190, loss = 0.44410389\n",
            "Iteration 191, loss = 0.44410409\n",
            "Iteration 192, loss = 0.44410322\n",
            "Iteration 193, loss = 0.44410266\n",
            "Iteration 194, loss = 0.44410349\n",
            "Iteration 195, loss = 0.44410314\n",
            "Iteration 196, loss = 0.44410389\n",
            "Iteration 197, loss = 0.44410115\n",
            "Iteration 198, loss = 0.44410028\n",
            "Iteration 199, loss = 0.44410186\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 200, loss = 0.44409818\n",
            "Iteration 1, loss = 0.70263607\n",
            "Iteration 2, loss = 0.69229219\n",
            "Iteration 3, loss = 0.68395286\n",
            "Iteration 4, loss = 0.67679248\n",
            "Iteration 5, loss = 0.66942635\n",
            "Iteration 6, loss = 0.66082965\n",
            "Iteration 7, loss = 0.65066443\n",
            "Iteration 8, loss = 0.63845699\n",
            "Iteration 9, loss = 0.62428526\n",
            "Iteration 10, loss = 0.60828552\n",
            "Iteration 11, loss = 0.59139313\n",
            "Iteration 12, loss = 0.57336149\n",
            "Iteration 13, loss = 0.55659817\n",
            "Iteration 14, loss = 0.54155105\n",
            "Iteration 15, loss = 0.52907088\n",
            "Iteration 16, loss = 0.51887997\n",
            "Iteration 17, loss = 0.51057893\n",
            "Iteration 18, loss = 0.50399698\n",
            "Iteration 19, loss = 0.49878808\n",
            "Iteration 20, loss = 0.49443978\n",
            "Iteration 21, loss = 0.49087137\n",
            "Iteration 22, loss = 0.48802121\n",
            "Iteration 23, loss = 0.48555041\n",
            "Iteration 24, loss = 0.48343039\n",
            "Iteration 25, loss = 0.48170061\n",
            "Iteration 26, loss = 0.48027338\n",
            "Iteration 27, loss = 0.47869315\n",
            "Iteration 28, loss = 0.47747929\n",
            "Iteration 29, loss = 0.47635386\n",
            "Iteration 30, loss = 0.47539984\n",
            "Iteration 31, loss = 0.47454953\n",
            "Iteration 32, loss = 0.47371762\n",
            "Iteration 33, loss = 0.47298432\n",
            "Iteration 34, loss = 0.47223194\n",
            "Iteration 35, loss = 0.47156683\n",
            "Iteration 36, loss = 0.47101125\n",
            "Iteration 37, loss = 0.47035529\n",
            "Iteration 38, loss = 0.46974340\n",
            "Iteration 39, loss = 0.46945753\n",
            "Iteration 40, loss = 0.46888957\n",
            "Iteration 41, loss = 0.46831667\n",
            "Iteration 42, loss = 0.46792164\n",
            "Iteration 43, loss = 0.46747245\n",
            "Iteration 44, loss = 0.46695466\n",
            "Iteration 45, loss = 0.46647554\n",
            "Iteration 46, loss = 0.46611113\n",
            "Iteration 47, loss = 0.46559823\n",
            "Iteration 48, loss = 0.46513919\n",
            "Iteration 49, loss = 0.46478594\n",
            "Iteration 50, loss = 0.46434958\n",
            "Iteration 51, loss = 0.46403239\n",
            "Iteration 52, loss = 0.46355836\n",
            "Iteration 53, loss = 0.46334210\n",
            "Iteration 54, loss = 0.46294771\n",
            "Iteration 55, loss = 0.46245489\n",
            "Iteration 56, loss = 0.46215794\n",
            "Iteration 57, loss = 0.46203872\n",
            "Iteration 58, loss = 0.46178280\n",
            "Iteration 59, loss = 0.46131335\n",
            "Iteration 60, loss = 0.46116553\n",
            "Iteration 61, loss = 0.46064254\n",
            "Iteration 62, loss = 0.46021467\n",
            "Iteration 63, loss = 0.45990459\n",
            "Iteration 64, loss = 0.45964443\n",
            "Iteration 65, loss = 0.45938354\n",
            "Iteration 66, loss = 0.45909820\n",
            "Iteration 67, loss = 0.45877637\n",
            "Iteration 68, loss = 0.45839847\n",
            "Iteration 69, loss = 0.45811105\n",
            "Iteration 70, loss = 0.45783766\n",
            "Iteration 71, loss = 0.45759375\n",
            "Iteration 72, loss = 0.45746001\n",
            "Iteration 73, loss = 0.45717143\n",
            "Iteration 74, loss = 0.45684488\n",
            "Iteration 75, loss = 0.45658260\n",
            "Iteration 76, loss = 0.45655546\n",
            "Iteration 77, loss = 0.45613102\n",
            "Iteration 78, loss = 0.45577013\n",
            "Iteration 79, loss = 0.45558434\n",
            "Iteration 80, loss = 0.45526744\n",
            "Iteration 81, loss = 0.45495456\n",
            "Iteration 82, loss = 0.45470650\n",
            "Iteration 83, loss = 0.45442392\n",
            "Iteration 84, loss = 0.45426852\n",
            "Iteration 85, loss = 0.45396001\n",
            "Iteration 86, loss = 0.45400987\n",
            "Iteration 87, loss = 0.45356468\n",
            "Iteration 88, loss = 0.45347971\n",
            "Iteration 89, loss = 0.45322108\n",
            "Iteration 90, loss = 0.45280896\n",
            "Iteration 91, loss = 0.45272681\n",
            "Iteration 92, loss = 0.45248870\n",
            "Iteration 93, loss = 0.45227431\n",
            "Iteration 94, loss = 0.45214928\n",
            "Iteration 95, loss = 0.45190308\n",
            "Iteration 96, loss = 0.45171793\n",
            "Iteration 97, loss = 0.45158199\n",
            "Iteration 98, loss = 0.45153227\n",
            "Iteration 99, loss = 0.45130719\n",
            "Iteration 100, loss = 0.45121697\n",
            "Iteration 101, loss = 0.45094191\n",
            "Iteration 102, loss = 0.45080275\n",
            "Iteration 103, loss = 0.45067522\n",
            "Iteration 104, loss = 0.45052884\n",
            "Iteration 105, loss = 0.45053765\n",
            "Iteration 106, loss = 0.45022175\n",
            "Iteration 107, loss = 0.45016515\n",
            "Iteration 108, loss = 0.44984439\n",
            "Iteration 109, loss = 0.44981635\n",
            "Iteration 110, loss = 0.44979655\n",
            "Iteration 111, loss = 0.44959760\n",
            "Iteration 112, loss = 0.44932002\n",
            "Iteration 113, loss = 0.44929004\n",
            "Iteration 114, loss = 0.44921842\n",
            "Iteration 115, loss = 0.44916429\n",
            "Iteration 116, loss = 0.44893873\n",
            "Iteration 117, loss = 0.44887797\n",
            "Iteration 118, loss = 0.44873792\n",
            "Iteration 119, loss = 0.44858627\n",
            "Iteration 120, loss = 0.44850354\n",
            "Iteration 121, loss = 0.44848651\n",
            "Iteration 122, loss = 0.44824115\n",
            "Iteration 123, loss = 0.44806221\n",
            "Iteration 124, loss = 0.44804043\n",
            "Iteration 125, loss = 0.44796507\n",
            "Iteration 126, loss = 0.44793856\n",
            "Iteration 127, loss = 0.44790169\n",
            "Iteration 128, loss = 0.44766845\n",
            "Iteration 129, loss = 0.44748904\n",
            "Iteration 130, loss = 0.44740487\n",
            "Iteration 131, loss = 0.44733161\n",
            "Iteration 132, loss = 0.44702817\n",
            "Iteration 133, loss = 0.44704301\n",
            "Iteration 134, loss = 0.44702618\n",
            "Iteration 135, loss = 0.44692326\n",
            "Iteration 136, loss = 0.44690323\n",
            "Iteration 137, loss = 0.44662909\n",
            "Iteration 138, loss = 0.44667365\n",
            "Iteration 139, loss = 0.44651810\n",
            "Iteration 140, loss = 0.44664141\n",
            "Iteration 141, loss = 0.44632527\n",
            "Iteration 142, loss = 0.44623469\n",
            "Iteration 143, loss = 0.44627326\n",
            "Iteration 144, loss = 0.44619020\n",
            "Iteration 145, loss = 0.44598038\n",
            "Iteration 146, loss = 0.44586727\n",
            "Iteration 147, loss = 0.44587046\n",
            "Iteration 148, loss = 0.44588896\n",
            "Iteration 149, loss = 0.44569677\n",
            "Iteration 150, loss = 0.44574173\n",
            "Iteration 151, loss = 0.44556363\n",
            "Iteration 152, loss = 0.44539424\n",
            "Iteration 153, loss = 0.44528586\n",
            "Iteration 154, loss = 0.44525355\n",
            "Iteration 155, loss = 0.44528934\n",
            "Iteration 156, loss = 0.44515654\n",
            "Iteration 157, loss = 0.44505227\n",
            "Iteration 158, loss = 0.44494990\n",
            "Iteration 159, loss = 0.44489987\n",
            "Iteration 160, loss = 0.44484082\n",
            "Iteration 161, loss = 0.44475420\n",
            "Iteration 162, loss = 0.44467177\n",
            "Iteration 163, loss = 0.44476683\n",
            "Iteration 164, loss = 0.44461292\n",
            "Iteration 165, loss = 0.44483667\n",
            "Iteration 166, loss = 0.44434358\n",
            "Iteration 167, loss = 0.44459721\n",
            "Iteration 168, loss = 0.44431940\n",
            "Iteration 169, loss = 0.44426930\n",
            "Iteration 170, loss = 0.44433503\n",
            "Iteration 171, loss = 0.44422229\n",
            "Iteration 172, loss = 0.44410560\n",
            "Iteration 173, loss = 0.44401490\n",
            "Iteration 174, loss = 0.44391011\n",
            "Iteration 175, loss = 0.44377001\n",
            "Iteration 176, loss = 0.44381506\n",
            "Iteration 177, loss = 0.44363337\n",
            "Iteration 178, loss = 0.44398349\n",
            "Iteration 179, loss = 0.44365700\n",
            "Iteration 180, loss = 0.44356397\n",
            "Iteration 181, loss = 0.44347097\n",
            "Iteration 182, loss = 0.44340368\n",
            "Iteration 183, loss = 0.44348492\n",
            "Iteration 184, loss = 0.44324454\n",
            "Iteration 185, loss = 0.44330016\n",
            "Iteration 186, loss = 0.44320724\n",
            "Iteration 187, loss = 0.44319008\n",
            "Iteration 188, loss = 0.44303815\n",
            "Iteration 189, loss = 0.44322013\n",
            "Iteration 190, loss = 0.44306866\n",
            "Iteration 191, loss = 0.44294902\n",
            "Iteration 192, loss = 0.44304660\n",
            "Iteration 193, loss = 0.44290197\n",
            "Iteration 194, loss = 0.44305476\n",
            "Iteration 195, loss = 0.44288558\n",
            "Iteration 196, loss = 0.44306543\n",
            "Iteration 197, loss = 0.44289534\n",
            "Iteration 198, loss = 0.44265058\n",
            "Iteration 199, loss = 0.44296684\n",
            "Iteration 200, loss = 0.44284848\n",
            "Iteration 1, loss = 0.70262800\n",
            "Iteration 2, loss = 0.69245208\n",
            "Iteration 3, loss = 0.68405776\n",
            "Iteration 4, loss = 0.67683923\n",
            "Iteration 5, loss = 0.66934385\n",
            "Iteration 6, loss = 0.66063902\n",
            "Iteration 7, loss = 0.65034602\n",
            "Iteration 8, loss = 0.63782156\n",
            "Iteration 9, loss = 0.62329829\n",
            "Iteration 10, loss = 0.60683567\n",
            "Iteration 11, loss = 0.58893980\n",
            "Iteration 12, loss = 0.57047367\n",
            "Iteration 13, loss = 0.55372523\n",
            "Iteration 14, loss = 0.53949242\n",
            "Iteration 15, loss = 0.52775376\n",
            "Iteration 16, loss = 0.51812299\n",
            "Iteration 17, loss = 0.51030484\n",
            "Iteration 18, loss = 0.50415988\n",
            "Iteration 19, loss = 0.49923289\n",
            "Iteration 20, loss = 0.49513762\n",
            "Iteration 21, loss = 0.49169294\n",
            "Iteration 22, loss = 0.48887324\n",
            "Iteration 23, loss = 0.48644500\n",
            "Iteration 24, loss = 0.48439151\n",
            "Iteration 25, loss = 0.48273451\n",
            "Iteration 26, loss = 0.48118139\n",
            "Iteration 27, loss = 0.47967867\n",
            "Iteration 28, loss = 0.47842542\n",
            "Iteration 29, loss = 0.47736363\n",
            "Iteration 30, loss = 0.47637207\n",
            "Iteration 31, loss = 0.47547326\n",
            "Iteration 32, loss = 0.47463175\n",
            "Iteration 33, loss = 0.47386910\n",
            "Iteration 34, loss = 0.47308346\n",
            "Iteration 35, loss = 0.47236713\n",
            "Iteration 36, loss = 0.47184033\n",
            "Iteration 37, loss = 0.47110555\n",
            "Iteration 38, loss = 0.47041435\n",
            "Iteration 39, loss = 0.46993530\n",
            "Iteration 40, loss = 0.46947245\n",
            "Iteration 41, loss = 0.46869820\n",
            "Iteration 42, loss = 0.46823961\n",
            "Iteration 43, loss = 0.46777903\n",
            "Iteration 44, loss = 0.46726112\n",
            "Iteration 45, loss = 0.46667374\n",
            "Iteration 46, loss = 0.46642605\n",
            "Iteration 47, loss = 0.46569913\n",
            "Iteration 48, loss = 0.46525048\n",
            "Iteration 49, loss = 0.46487009\n",
            "Iteration 50, loss = 0.46443842\n",
            "Iteration 51, loss = 0.46404494\n",
            "Iteration 52, loss = 0.46363553\n",
            "Iteration 53, loss = 0.46333275\n",
            "Iteration 54, loss = 0.46304577\n",
            "Iteration 55, loss = 0.46253592\n",
            "Iteration 56, loss = 0.46218916\n",
            "Iteration 57, loss = 0.46205700\n",
            "Iteration 58, loss = 0.46165489\n",
            "Iteration 59, loss = 0.46117966\n",
            "Iteration 60, loss = 0.46100649\n",
            "Iteration 61, loss = 0.46050432\n",
            "Iteration 62, loss = 0.46003564\n",
            "Iteration 63, loss = 0.45973091\n",
            "Iteration 64, loss = 0.45944687\n",
            "Iteration 65, loss = 0.45920661\n",
            "Iteration 66, loss = 0.45887523\n",
            "Iteration 67, loss = 0.45854633\n",
            "Iteration 68, loss = 0.45822537\n",
            "Iteration 69, loss = 0.45792220\n",
            "Iteration 70, loss = 0.45762859\n",
            "Iteration 71, loss = 0.45741190\n",
            "Iteration 72, loss = 0.45720644\n",
            "Iteration 73, loss = 0.45692756\n",
            "Iteration 74, loss = 0.45670056\n",
            "Iteration 75, loss = 0.45636131\n",
            "Iteration 76, loss = 0.45645540\n",
            "Iteration 77, loss = 0.45600576\n",
            "Iteration 78, loss = 0.45556414\n",
            "Iteration 79, loss = 0.45541449\n",
            "Iteration 80, loss = 0.45512166\n",
            "Iteration 81, loss = 0.45469957\n",
            "Iteration 82, loss = 0.45447908\n",
            "Iteration 83, loss = 0.45419227\n",
            "Iteration 84, loss = 0.45408910\n",
            "Iteration 85, loss = 0.45372715\n",
            "Iteration 86, loss = 0.45376557\n",
            "Iteration 87, loss = 0.45318092\n",
            "Iteration 88, loss = 0.45320917\n",
            "Iteration 89, loss = 0.45284068\n",
            "Iteration 90, loss = 0.45255757\n",
            "Iteration 91, loss = 0.45243148\n",
            "Iteration 92, loss = 0.45214069\n",
            "Iteration 93, loss = 0.45200595\n",
            "Iteration 94, loss = 0.45185531\n",
            "Iteration 95, loss = 0.45155741\n",
            "Iteration 96, loss = 0.45142099\n",
            "Iteration 97, loss = 0.45127405\n",
            "Iteration 98, loss = 0.45117064\n",
            "Iteration 99, loss = 0.45094570\n",
            "Iteration 100, loss = 0.45084299\n",
            "Iteration 101, loss = 0.45073789\n",
            "Iteration 102, loss = 0.45052734\n",
            "Iteration 103, loss = 0.45041856\n",
            "Iteration 104, loss = 0.45009100\n",
            "Iteration 105, loss = 0.45011907\n",
            "Iteration 106, loss = 0.44979229\n",
            "Iteration 107, loss = 0.44984905\n",
            "Iteration 108, loss = 0.44954052\n",
            "Iteration 109, loss = 0.44941083\n",
            "Iteration 110, loss = 0.44948898\n",
            "Iteration 111, loss = 0.44925398\n",
            "Iteration 112, loss = 0.44905305\n",
            "Iteration 113, loss = 0.44886937\n",
            "Iteration 114, loss = 0.44875747\n",
            "Iteration 115, loss = 0.44881962\n",
            "Iteration 116, loss = 0.44861297\n",
            "Iteration 117, loss = 0.44852376\n",
            "Iteration 118, loss = 0.44842560\n",
            "Iteration 119, loss = 0.44822886\n",
            "Iteration 120, loss = 0.44818699\n",
            "Iteration 121, loss = 0.44810338\n",
            "Iteration 122, loss = 0.44797400\n",
            "Iteration 123, loss = 0.44780518\n",
            "Iteration 124, loss = 0.44781498\n",
            "Iteration 125, loss = 0.44797285\n",
            "Iteration 126, loss = 0.44777307\n",
            "Iteration 127, loss = 0.44757752\n",
            "Iteration 128, loss = 0.44735507\n",
            "Iteration 129, loss = 0.44735436\n",
            "Iteration 130, loss = 0.44733389\n",
            "Iteration 131, loss = 0.44716031\n",
            "Iteration 132, loss = 0.44697986\n",
            "Iteration 133, loss = 0.44688894\n",
            "Iteration 134, loss = 0.44695640\n",
            "Iteration 135, loss = 0.44674264\n",
            "Iteration 136, loss = 0.44672504\n",
            "Iteration 137, loss = 0.44655894\n",
            "Iteration 138, loss = 0.44657717\n",
            "Iteration 139, loss = 0.44641858\n",
            "Iteration 140, loss = 0.44659625\n",
            "Iteration 141, loss = 0.44633127\n",
            "Iteration 142, loss = 0.44627172\n",
            "Iteration 143, loss = 0.44622823\n",
            "Iteration 144, loss = 0.44610886\n",
            "Iteration 145, loss = 0.44595848\n",
            "Iteration 146, loss = 0.44592811\n",
            "Iteration 147, loss = 0.44588300\n",
            "Iteration 148, loss = 0.44583445\n",
            "Iteration 149, loss = 0.44567758\n",
            "Iteration 150, loss = 0.44572256\n",
            "Iteration 151, loss = 0.44566076\n",
            "Iteration 152, loss = 0.44548062\n",
            "Iteration 153, loss = 0.44536687\n",
            "Iteration 154, loss = 0.44530303\n",
            "Iteration 155, loss = 0.44540780\n",
            "Iteration 156, loss = 0.44519688\n",
            "Iteration 157, loss = 0.44521254\n",
            "Iteration 158, loss = 0.44499961\n",
            "Iteration 159, loss = 0.44505108\n",
            "Iteration 160, loss = 0.44488591\n",
            "Iteration 161, loss = 0.44483269\n",
            "Iteration 162, loss = 0.44479717\n",
            "Iteration 163, loss = 0.44478004\n",
            "Iteration 164, loss = 0.44465771\n",
            "Iteration 165, loss = 0.44479233\n",
            "Iteration 166, loss = 0.44445556\n",
            "Iteration 167, loss = 0.44463366\n",
            "Iteration 168, loss = 0.44435332\n",
            "Iteration 169, loss = 0.44441339\n",
            "Iteration 170, loss = 0.44430465\n",
            "Iteration 171, loss = 0.44422364\n",
            "Iteration 172, loss = 0.44409416\n",
            "Iteration 173, loss = 0.44403331\n",
            "Iteration 174, loss = 0.44390422\n",
            "Iteration 175, loss = 0.44387514\n",
            "Iteration 176, loss = 0.44375215\n",
            "Iteration 177, loss = 0.44361039\n",
            "Iteration 178, loss = 0.44397604\n",
            "Iteration 179, loss = 0.44370359\n",
            "Iteration 180, loss = 0.44353944\n",
            "Iteration 181, loss = 0.44341811\n",
            "Iteration 182, loss = 0.44336392\n",
            "Iteration 183, loss = 0.44340980\n",
            "Iteration 184, loss = 0.44325826\n",
            "Iteration 185, loss = 0.44328010\n",
            "Iteration 186, loss = 0.44314444\n",
            "Iteration 187, loss = 0.44312835\n",
            "Iteration 188, loss = 0.44303516\n",
            "Iteration 189, loss = 0.44325374\n",
            "Iteration 190, loss = 0.44299480\n",
            "Iteration 191, loss = 0.44291566\n",
            "Iteration 192, loss = 0.44293595\n",
            "Iteration 193, loss = 0.44281351\n",
            "Iteration 194, loss = 0.44285060\n",
            "Iteration 195, loss = 0.44280021\n",
            "Iteration 196, loss = 0.44287758\n",
            "Iteration 197, loss = 0.44272340\n",
            "Iteration 198, loss = 0.44260613\n",
            "Iteration 199, loss = 0.44269208\n",
            "Iteration 200, loss = 0.44251273\n",
            "Confusion Matrix\n",
            "[[2839  771]\n",
            " [ 982 3512]]\n",
            "Attractive  -1.0   1.0\n",
            "row_0                 \n",
            "-1.0        2841   814\n",
            " 1.0         980  3469\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        -1.0       0.74      0.79      0.76      3610\n",
            "         1.0       0.82      0.78      0.80      4494\n",
            "\n",
            "    accuracy                           0.78      8104\n",
            "   macro avg       0.78      0.78      0.78      8104\n",
            "weighted avg       0.79      0.78      0.78      8104\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}